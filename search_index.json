[
["index.html", "Geocomputation with R Welcome Development How to contribute? Reproducibility", " Geocomputation with R Robin Lovelace, Jakub Nowosad, Jannes Muenchow 2018-04-21 Welcome Welcome to the online home of Geocomputation with R, a forthcoming book with CRC Press. Development Inspired by bookdown and other open source projects we are developing this book in the open. This approach encourages contributions, ensures reproducibility and provides access to the material as it evolves. The book’s development can be divided into four main phases: Basic methods Applied geocomputation Advanced methods Geocomputation in the wild Currently we are working on Part 3. New chapters will be added to this website as the project progresses, hosted at geocompr.robinlovelace.net and kept up-to-date thanks to Travis, which rebuilds the book each time its source code changes, and provides a visual indicator that reports the build status: The version of the book you are reading now was built on 2018-04-21 and was built on Travis. How to contribute? bookdown makes editing a book as easy as editing a wiki, provided you have a GitHub account (sign-up at github.com). Once logged-in to GitHub, clicking on the ‘edit me’ icon highlighted in the image below will take you to the source R Markdown where you can make changes: To raise an issue about the book’s content (e.g. code not running) or make a feature request, check-out the issue tracker. Reproducibility To reproduce the book, you need a recent version of R and up-to-date packages, which can be installed with the following command (which requires devtools): devtools::install_github(&quot;robinlovelace/geocompr&quot;) To build the book locally, clone or download the geocompr repo, load R in root directory (e.g. by opening geocompr.Rproj in RStudio) and run the following lines: bookdown::render_book(&quot;index.Rmd&quot;) # to build the book browseURL(&quot;_book/index.html&quot;) # to view it Further details can be found at github.com/Robinlovelace/geocompr. "],
["preface.html", "Preface Acknowledgements", " Preface This book is aimed at people who want to do spatial data analysis, visualization and modeling using open source software and reproducible workflows. It is based on R, a flexible language for ‘data science’ with powerful geospatial capabilities and a strong ecosystem of add-on packages dedicated to spatial data (see the ‘Spatial Task View’ at cran.r-project.org/web/views). R enables reproducibility through its command-line interace and ensures accessibility because it is freely available and works on most modern operating systems (including Linux, Windows and Mac). The book will therefore be of interest to a wide range of people worldwide, although we expect it to be especially useful for: People who have learned spatial analysis skills using a desktop Geographic Information System (GIS) such as QGIS, ArcMap, GRASS or SAGA, who want access to a powerful (geo)statistical and visualization programming language and the benefits of a command-line approach (Sherman 2008): With the advent of ‘modern’ GIS software, most people want to point and click their way through life. That’s good, but there is a tremendous amount of flexibility and power waiting for you with the command line. Graduate students and researchers from fields specializing in geographic data including Geography, Remote Sensing, Planning, GIS and Geographic Data Science Academics and post-graduate students working on projects in fields including Geology, Regional Science, Biology and Ecology, Agricultural Sciences (precision farming), Archaeology, Epidemiology, Transport Modeling, and broadly defined Data Science which require the power and flexibility of R for their research Applied researchers and analysts in public, private or third-sector organisations who need the reproducibility, speed and flexibility of a command-line language such as R in applications dealing with spatial data as diverse as Urban and Transport Planning, Logistics, Geo-marketing (store location analysis) and Emergency Planning The book is designed for intermediate-to-advanced R users interested in geocomputation and R beginners who have prior experience with geographic data. If you are new to both R and geographic data do not be discouraged: we provide links to further materials and describe the nature of spatial data from a beginner’s perspective in Chapter 2 and in links provided below. We aim to make R’s famously steep learning curve more mellow and less rollercoaster: the chapters increase in difficulty as the book progresses; each chapter starts relatively easy and covers the most important topics first to make the book as accessible as possible. Exercises can be found at the end of each chapter. Completing these encourages using R interactively to solve geospatial problems, ensuring you can operationalize the concepts and code in each chapter. Impatient readers are welcome to dive straight into the practical examples, starting in Chapter 2. However, we recommend reading about the wider context of Geocomputation with R in Chapter 1 first. If you are new to R we also recommend learning more about the language before attempting to run the code chunks provided in each chapter (unless you’re reading the book for an understanding of the concepts). Fortunately for R begginers R has supportive community that has developed a wealth of resources that can help. We particularly recommend three tutorials: R for Data Science (Grolemund and Wickham 2016) and Efficient R Programming (Gillespie and Lovelace 2016), especially Chapter 2 (on installing and setting-up R/RStudio) and Chapter 10 (on learning to learn), and An introduction to R (Venables, Smith, and Team 2017). A good interactive tutorial is DataCamp’s Introduction to R. Although R has a steep learning curve the command-line approach advocated in this book can quickly pay-off. As you’ll learn in subsequent chapters, R is an effective tool for tackling a wide range of geographic data challenges. We expect that, with practice, R will become the program of choice in your geospatial toolbox for many applications. Typing and executing commands at the command-line is, in many cases, faster than pointing-and-clicking around the graphical user interface (GUI) a desktop GIS. For some applications such as Spatial Statistics and modelling R may be the only realistic way to get the work done. As outlined in section 1.2 there are many reasons for using R for geocomputation: R is well-suited to the interactive use required in many geographic data analysis workflows compared with other languages. R excels in the rapidly growing fields of Data Science (which includes data carpentry, statistical learning techniques and data visualization) and Big Data (via efficient interfaces to databases and distributed computing systems). Furthermore R enables a reproducible workflow: sharing scripts underlying your analysis will allow others to build-on your work. To ensure reproducibility in this book we have made its source code available at github.com/Robinlovelace/geocompr. There you will find script files in the code/ folder that generate figures: when code generating a figure is not provided in the main text of the book the name of the script file that generated it is provided in the caption (see for example the caption for Figure 7.2). Other languages such as Python, Java and C++ can be used for geocomputation and there are excellent resources for learning geocomputation without R, as discussed in section 1.3. None of these provide the unique combination of package ecosystem, statistical capabilities, visualisation options, powerful IDEs offered by the R community. Furthermore, by teaching how to use one language (R) in depth, this book will equip you with the concepts and confidence needed to do geocomputation in other languages. Geocomputation with R will equip you with knowledge and skills to tackle a wide range of issues, including those with scientific, societal and environmental implications, manifested in geographic data. As described in section 1.1, geocomputation is not only about using computers to process geographic data: it is also about real-world impact. If you are interested in the wider context and motivations behind this book, read on: these are covered in Chapter 1. Acknowledgements We thank Patrick Schratz (University of Jena) for fruitful discussions on mlr and for providing code input (Chapter 13). This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License. References "],
["intro.html", "1 Introduction 1.1 What is geocomputation? 1.2 Why Geocomputation with R? 1.3 Software for geocomputation 1.4 R’s spatial ecosystem 1.5 The history of R-spatial 1.6 Exercises", " 1 Introduction This book is about harnessing the power of modern computers to do things with geographic data. It teaches a range of spatial skills, including: reading, writing and manipulating geographic data; making static and interactive maps; applying geocomputation to solve real-world problems; and modeling geographic phenomena. By demonstrating how various spatial operations can be linked, in reproducible ‘code chunks’ that intersperse the prose, the book also teaches a transparent and thus scientific workflow. Learning how to use the wealth of geospatial tools available from the R command line can be exciting but creating new ones can be truly liberating, by removing constraints on your creativity imposed by software. By the end of the book you should be able to create new tools for geocomputation in the form of shareable R scripts and functions. Over the last few decades free and open source software for geospatial data (‘FOSS4G’) has progressed at an astonishing rate (see foss4g.org). Thanks to FOSS4G and the wider open source movement geospatial analysis is no longer the preserve of those with expensive hardware and software: anyone can now download high performance spatial libraries on their computer. However, despite the growth of geospatial software that is open source, much of it is still not easy to script. Open source Geographic Information Systems (GIS) such as QGIS (see qgis.org) have greatly reduced the ‘barrier to entry’ but their emphasis on the Graphical User Interface (GUI) can discourage reproducibility. This book focuses on the Command Line Interface (CLI), enabling reproducible, and ‘computational’ workflows, something we will expand on in Chapter 13. Reproducibility is a major advantage of command-line interfaces, but what does it mean in practice? We define it as follows: A process is reproducible only if the same results can be generated by others using publicly accessible code. This may sound simple and easy to achieve (which it is if you carefully maintain your R code in script files) but has profound implications for teaching and the scientific process (Pebesma, Nüst, and Bivand 2012). A major aim of this book is to make geographic data analysis more accessible as part of a reproducible workflow. R is a flexible language that allows access to many spatial software libraries (see section 1.2). Before going into the details of the software, however, it is worth taking a step back and thinking about what we mean by geocomputation. 1.1 What is geocomputation? Geocomputation is a relatively young field with a ~30 year history, dating back to the first conference on the subject in 1996.1 What distinguishes geocomputation from the older quantitative geography, is its emphasis on “creative and experimental” GIS applications (Longley et al. 1998). Additionally, it is also about developing new, research-driven methods (Openshaw and Abrahart 2000): GeoComputation is about using the various different types of geodata and about developing relevant geo-tools within the overall context of a ‘scientific’ approach. This book aims to go beyond teaching methods and code: by the end of it you should be able use your geocomputational skills, to do “practical work that is beneficial or useful” (Openshaw and Abrahart 2000). Our approach differs from early adopters such as Stan Openshaw in one important way, however. At the turn of the 21st Century it was unrealistic to expect readers to be able to reproduce code examples, due to barriers preventing access to the necessary hardware, software and data. Fast-forward two decades and things have progressed rapidly. Anyone with access to a laptop with ~4GB RAM can realistically expect to be able to install and run software for geocompuation on publicly accessible datasets, which are more widely available than ever before (as we’ll see in Chapter 6).2 Unlike early works in the field all the work presented in this book is reproducible using code and example data supplied alongside the book, in R packages such as spData, the installation of which is covered in Chapter 2. Reproducible geographic research is related to Geographical Data Science (GDS). This recent concept essentially combines ‘data science’ with GIS and, like geocomputation, can be defined in comparison with GIS (see Table 1.1). The focus on reproducibility and a command-line interface in this book is aligned with GDS. Table 1.1: Differences in emphasis between the fields of Geographic Information Systems (GIS) and Geographic Data Science (GDS). Attribute GIS GDS Home disciplines Geography Geography, Computing, Statistics Software focus Graphical User Interface Code Reproducibility Minimal Maximal Geocomputation is young, but it builds on older fields. It can be seen as a part of Geography, which has a 2000+ year history (Talbert 2014); and an extension of Geographic Information Systems (GIS) (Neteler and Mitasova 2008), which emerged in the 1960s (Coppock and Rhind 1991). Geography has played an important role in explaining and influencing humanity’s relationship with the natural world long before the invention of the computer, however. Alexander von Humboldt’s travels to South America in the early 1800s illustrates this role: not only did the resulting observations lay the foundations for the traditions of physical and plant geography, they also paved the way towards policies to protect the natural world (Wulf 2015). This book aims to contribute to the ‘Geographic Tradition’ (Livingstone 1992) by harnessing the power of modern computers and open source software. The book’s links to older disciplines were reflected in suggested titles for the book: Geography with R and R for GIS. Each has advantages. The former conveys the message that it comprises much more than just spatial data: non-spatial attribute data are inevitably interwoven with geometry data, and Geography is about more than where something is on the map. The latter communicates that this is a book about using R as a GIS, to perform spatial operations on geographic data (Bivand, Pebesma, and Gómez-Rubio 2013). However, the term GIS conveys some connotations (see Table 1.1) which simply fail to communicate one of R’s greatest strengths: its console-based ability to seamlessly switch between geographic and non-geographic data processing, modeling and visualization tasks. By contrast, the term geocomputation implies reproducible and creative programming. Of course, (geocomputational) algorithms are powerful tools that can become highly complex. However, all algorithms are composed of smaller parts. By teaching you its foundations and underlying structure, we aim to empower you to create your own innovative solutions to geographic data problems. 1.2 Why Geocomputation with R? Early geographers used a variety of tools including rulers, compasses and sextants to advance knowledge about the world. However, until John Harrison invented the marine chronometer in the 18th century it had been impossible to determine the exact longitude at sea (‘the longitude problem’). Prior to his invention ships followed for centuries a line of constant latitude making each journey much longer, more expensive and often more dangerous. Nowadays this seems unimaginable with every smartphone having a GPS receiver at its disposal and a multitude of other sensors measuring the world in real-time (satellites, radar, autonomous cars, citizens, etc.). For instance, an autonomous car could create 100 GB or more per day (see e.g., this article). Equally, earth observation data (satellite imagery) has become so big that it is impossible to analyze the corresponding data with a single computer (see http://r-spatial.org/2016/11/29/openeo.html). Hence, we need computational power, software and related tools to handle and extract the most interesting patterns of this ever-increasing amount of (geo-)data. (Geo-)Databases help with data management, storing and querying such large amounts of data. Through interfaces, we can access subsets of these data for further analysis, information extraction and visualization. In this book we treat R as a ‘tool for the trade’ for the latter. R is a multi-platform, open source language and environment for statistical computing and graphics (https://www.r-project.org/). With a wide range of packages R also supports advanced geospatial statistics, modeling and visualization.3. At its core R is an object-oriented, functional programming language (Wickham 2014), and was specifically designed as an interactive interface to other software (Chambers 2016). The latter also includes many ‘bridges’ to a treasure trove of GIS software, ‘geolibraries’ and functions (see Chapter 10). It is thus ideal for quickly creating ‘geo-tools’, without needing to master lower level languages (compared to R) such as C, FORTRAN and Java (see section 1.3). This can feel like breaking free from the metaphorical ‘glass ceiling’ imposed by GUI-based proprietary geographic information systems (see Table 1.1 for a definition of GUI). What is more, advanced users might even extend R with the power of other languages (e.g., C++ through Rcpp or Python through reticulate; see also section 1.3). An example showing R’s flexibility with regard to geographic software development is its support for generating interactive maps thanks to leaflet (Cheng, Karambelkar, and Xie 2017). The packages tmap and mapview (Tennekes 2018a; Appelhans et al. 2018) are built on and extend leaflet. These packages help overcome the criticism that R has “limited interactive [plotting] facilities” (Bivand, Pebesma, and Gómez-Rubio 2013). The code below illustrates this by generating Figure 1.1. library(leaflet) popup = c(&quot;Robin&quot;, &quot;Jakub&quot;, &quot;Jannes&quot;) leaflet() %&gt;% addProviderTiles(&quot;NASAGIBS.ViirsEarthAtNight2012&quot;) %&gt;% addAwesomeMarkers(lng = c(-3, 23, 11), lat = c(52, 53, 49), popup = popup) Figure 1.1: Where the authors are from. The basemap is a tiled image of the Earth at Night provided by NASA. Interact with the online version at robinlovelace.net/geocompr, for example by zooming-in and clicking on the popups. It would have been difficult to produce Figure 1.1 using R a few years ago, let alone as an interactive map. This illustrates R’s flexibility and how, thanks to developments such as knitr and leaflet, it can be used as an interface to other software, a theme that will recur throughout this book. The use of R code, therefore, enables teaching geocomputation with reference to reproducible examples such as that provided in 1.1 rather than abstract concepts. 1.3 Software for geocomputation R is a powerful language for geocomputation but there are many other options for spatial data analysis. Awareness of these will help situate R in the wider geospatial ecosystem, and identify when a different tool may be more appropriate for a specific task. Over time R developers have added various R interfaces (or ‘bridges’ as we call them in Chapter 13) to other software. Therefore knowing what else is out there can also be useful from an R-spatial perspective because a) there may already be a bridge to something that adds the functionality you need and b) if there’s not already a bridge there may be one on the horizon. With this motivation in mind this section briefly introduces the languages C++, Java and Python for geocomputation, with reference to R. A natural choice for geocomputation would be C++ since major GIS packages (e.g., GDAL, QGIS, GRASS, SAGA, and even ArcGIS) often rely in great parts on it. This is because well-written C++ can be blazing fast, which makes it a good choice for performance-critical applications such as the processing of large spatial data. Usually, people find it harder to learn than Python or R. It is also likely that you have to invest a lot of time to code things that are readily available in R. Therefore, we would recommend to learn R, and subsequently to learn C++ through Rcpp if a need for performance optimization arises. Subsequently, you could even implement geoalgorithms you are missing from the most common desktop GIS with the help of Rcpp^[Though, in that case we would recommend to contribute the C++ code to one of the open-source GIS packages since this would make the geoalgorithm available to a wider audience. In turn, you could access the GIS software via one of the available R-GIS interfaces (Chapter 10). Java is another important (and versatile) language used in GIScience. For example, the open-source desktop GIS gvSig, OpenJump and uDig are written in Java. There are also many open source add-on libraries available for Java, including GeoTools and the Java Topology Suite.4 Furthermore, many server-based applications use Java including among others Geoserver/Geonode, deegree and 52°North WPS. Java’s object-oriented syntax is similar to that of C++ but its memory management, at least from a user’s perspective, is simpler and more robust. Java is rather unforgiving regarding class, object and variable declarations, which encourages well-designed programming structure, useful in large projects with thousands of lines of codes placed in numerous files. Following the write once, run anywhere principle, Java is platform-independent (which is unusual for a compiled language) and has excellent performance on large-scale systems. This makes Java a suitable language for complex architecture projects such RStudio, the Integrated Development Environment (IDE) in which this book was written! Java is less suitable for statistical modeling and visualization than Python or R. Although Java can be used for data science (Brzustowicz 2017), it has relatively few statistical libraries, especially compared with R. Furthermore Java is hard to use interactively. Interpreted languages (such as R and Python) are better suited for the type of interactive workflow used in many geographic workflows than compiled languages (such as Java and C++). Unlike Java (and most other languages) R has native support for data frames and matrices, making it especially well suited for (geographic) data analysis. Python is the final language for geocomputation that deserves attention in this section. Like R, Python has gained popularity due to the rapid growth of data science (Robinson). Both languages are object-oriented, and have lots of further things in common. Due to their similarities there is much on-line discussion framing the relative merits of each language as a competition, as exemplified by an infographic by DataCamp titled “DATA SCIENCE WARS: R vs Python”, which arguably generates more heat than light. In practice both languages have their strengths and to some extent which you use is less important than the domain of application and communication of results. Learning either will provide a head-start in learning the other. However, there are major advantages of R over Python for geocomputation which explains its prominence in this book. R has unparalleled support for statistics, including spatial statistics, with hundreds of packages (unmatched by Python) supporting thousands of statistical methods. The major advantage of Python is that it is a general-purpose programming language. It is well-suited to many applications, including desktop software, computer games, websites and data science. R, by contrast, was originally developed for statistics.5 This also explains Python’s larger user base compared with R’s. Python is often the only shared language between different (geocomputation) communities, explaining why it has become the ‘glue’ that holds many GIS programs together. Many geoalgorithms, including those in QGIS and ArcMap, can be accessed from the Python command line, making it well-suited as a starter language for command-line GIS.6 For spatial statistics and predictive modeling, however, R is second-to-none. This does not mean you must chose either R or Python: Python supports most common statistical techniques (though R tends to support new developments in spatial statistics earlier) and many concepts learned from Python can be applied to the R world. Like R Python also supports spatial data analysis and manipulation with packages such as osgeo, Shapely, NumPy and PyGeoProcessing (Garrard 2016). 1.4 R’s spatial ecosystem There are many ways to handle spatial data in R, with dozens of packages in the area.7 In this book we endeavor to teach the state-of-the-art in the field whilst ensuring that the methods are future-proof. Like many areas of software development, R’s spatial ecosystem is rapidly evolving. Because R is open source, these developments can easily build on previous work, by ‘standing on the shoulders of giants’, as Isaac Newton put it in 1675. This approach is advantageous because it encourages collaboration and avoids ‘reinventing the wheel’. The package sf (covered in Chapter 2), for example, builds on its predecessor sp. A surge in development time (and interest) in ‘R-Geo’ has followed the award of a grant by the R Consortium for the development of support for Simple Features, an open-source standard and model to store and access vector geometries. This resulted in the sf package (covered in 2.1.1). Multiple places reflect the immense interest in sf. This is especially true for the R-sig-Geo Archives, a long-standing open access email list containing much R-spatial wisdom accumulated over the years. Many posts on the list now discuss sf and related packages, suggesting that R’s spatial software developers are using the package and, therefore, it is here to stay. We even propose that the release of sf heralds a new era for spatial data analysis and geocomputation in R. This era8 clearly has the wind in its sails and is set to dominate future developments in R’s spatial ecosystem for years to come. So time invested in learning the ‘new ways’ of handling spatial data, and reading this book, is well spent! Figure 1.2: The popularity of spatial packages in R. The y-axis shows the average number of downloads, within a 30-day rolling window, of R’s top 5 spatial packages, defined as those with the highest number of downloads within the last 30 days. It is noteworthy that shifts in the wider R community, as exemplified by the data processing package dplyr (released in 2014) influenced shifts in R’s spatial ecosystem. Alongside other packages that have a shared style and emphasis on ‘tidy data’ (including e.g., ggplot2), dplyr was placed in the tidyverse ‘metapackage’ in late 2016. The tidyverse approach, with its focus on long-form data and fast intuitively named functions, has become immensely popular. This has led to a demand for ‘tidy spatial data’ which has been partly met by sf and other approaches such as tabularaster. An obvious feature of the tidyverse is the tendency for packages to work in harmony. Although an equivalent geoverse is presently missing, there is an on-going discussion of harmonizing R’s many spatial packages9 and a growing number of actively developed packages which are designed to work in harmony with sf (Table 1.2). Table 1.2: The top 5 most downloaded packages that depend on sf, in terms of average number of downloads per day over the previous month. As of 2018-04-14 there are 69 packages which import sf. package Downloads plotly 1789 raster 1559 spdep 944 spData 898 leaflet 731 1.5 The history of R-spatial There are many benefits of using recent spatial packages such as sf, but it also important to be aware of the history of R’s spatial capabilities: many functions, use-cases and teaching material are contained in older packages. These can still be useful today, provided you know where to look. R’s spatial capabilities originated in early spatial packages in the S language (Bivand and Gebhardt 2000). The 1990s saw the development of numerous S scripts and a handful of packages for spatial statistics. R packages arose from these and by 2000 there were R packages for various spatial methods “point pattern analysis, geostatistics, exploratory spatial data analysis and spatial econometrics”, according to an article presented at GeoComputation2000 (Bivand and Neteler 2000) Some of these, notably spatial, sgeostat and splancs are still available on CRAN (Rowlingson and Diggle 1993; Rowlingson and Diggle 2017; Venables and Ripley 2002; University and Gebhardt 2016). A subsequent article in R News (the predecessor of The R Journal) contained an overview of spatial statistical software in R at the time, much of which was based on previous code written for S/S-PLUS (Ripley 2001). This overview described packages for spatial smoothing and interpolation (e.g., akima, spatial, sgeostat and geoR) and point pattern analysis (splancs and spatstat; Akima and Gebhardt 2016; Rowlingson and Diggle 2017; Jr and Diggle 2016). While all these are still available on CRAN, spatstat stands out among them, as it remains dominant in the field of spatial point pattern analysis (Baddeley, Rubak, and Turner 2015). The following R News issue (Volume 1/3) put spatial packages in the spotlight again, with an introduction to splancs and a commentary on future prospects regarding spatial statistics (Bivand 2001). Additionally, the issue introduced two packages for testing spatial autocorrelation that eventually became part of spdep (Bivand 2017). Notably, the commentary mentions the need for standardization of spatial interfaces, efficient mechanisms for exchanging data with GIS, and handling of spatial metadata such as coordinate reference systems (CRS). maptools (written by Nicholas Lewin-Koh; Bivand and Lewin-Koh 2017) is another important package from this time. Initially maptools just contained a wrapper around shapelib and permitted the reading of ESRI Shapefiles into geometry nested lists. The corresponding and nowadays obsolete S3 class called “Map” stored this list alongside an attribute data frame. The work on the “Map” class representation was nevertheless important since it directly fed into sp prior to its publication on CRAN. In 2003, Bivand (2003) published an extended review of spatial packages. Around this time the development of R’s spatial capabilities increasingly supported interfaces to external libraries, especially to GDAL and PROJ.4. These interfaces facilitated geographic data I/O (meaning input output and covered in chapter 6) and CRS transformations, respectively. Bivand (2003) proposed a spatial data class system, including support for points, lines, polygons and grids based on GDAL’s support for a wide range of spatial data formats. All these ideas contributed to the packages rgdal and sp, which became the foundational packages for spatial data analysis with R (Bivand, Pebesma, and Gómez-Rubio 2013). rgdal provided GDAL bindings for R which greatly extended R’s spatial capabilities in terms of access to previously unavailable spatial data formats. Initially, Tim Keitt released it in 2003 with support for raster drivers. But soon, rgdal also enabled storing information about coordinate reference system (building on top of the PROJ.4 library), allowed map projections, datum transformations and OGR vector reading. Many of these additional capabilities were thanks to Barry Rowlingson who folded them into the rgdal codebase in March 2006.10 sp, released in 2005, overcame R’s inability to distinguish spatial and non-spatial objects (Pebesma and Bivand 2005). It grew from a workshop before, and a session at the 2003 DSC conference in Vienna, gathering input from most interested package developers. At the same time, sourceforge was chosen for development collaboration (migrated to R-Forge five years later) and the R-sig-geo mailing list was started. Prior to 2005, spatial coordinates were generally treated as any other number. This changed with sp as it provided generic classes and methods for spatial data. The sophisticated class system supported points, lines, polygons and grids, with and without attribute data. Making use of the S4 class system, sp stores each piece of ‘spatially specific’ information (such as bounding box, coordinate reference system, attribute table) in slots, which are accessible via the @ symbol. For instance, sp-classes store attribute data in the data slot as a data.frame. This enables non-spatial data operations to work alongside spatial operations (see section 2.1.2). Additionally, sp implemented generic methods for spatial data types for well-known functions such as summary() and plot() . In the following, sp classes rapidly became the go-to standard for spatial data in R, resulting in a proliferation of packages that depended on it from around 20 in 2008 and over 100 in 2013 (Bivand, Pebesma, and Gómez-Rubio 2013). Now more than 450 packages rely on sp, making it an important part of the R ecosystem. Prominent R packages using sp include: gstat, for spatial and spatio-temporal geostatistics; geosphere, for spherical trigonometry; and adehabitat used for the analysis of habitat selection by animals (Pebesma and Graeler 2018; Calenge 2006; Hijmans 2016). While rgdal and sp solved many spatial issues, R was still lacking geometry calculation abilities. Therefore, Colin Rundel started to develop a package that interfaces GEOS, an open-source geometry library, during a Google Summer of Coding project in 2010. The resulting rgeos package (first released in 2010; Bivand and Rundel 2017) brought geometry calculations to R by allowing functions and operators from the GEOS library to manipulate sp objects. Another limitation of sp was its limited support of raster data. The raster-package (first released in 2010; Hijmans 2017) overcame this by providing Raster* classes and functions for creating, reading and writing raster data. A key feature of raster is its ability to work with datasets that are too large to fit into the main memory (RAM), thereby overcoming one of R’s major limitations when working on large raster data.11 In parallel with or partly even preceding the development of spatial classes and methods came the support for R as an interface to dedicated GIS software. The GRASS package (Bivand 2000) and follow-on packages spgrass6 and rgrass7 (for GRASS GIS 6 and 7, respectively) were prominent examples in this direction (Bivand 2016b; Bivand 2016a). Other examples of bridges between R and GIS include RSAGA (Brenning, Bangs, and Becker 2018, first published in 2008), ArcGIS (A. Brenning 2012a, first published in 2008), and RQGIS (Muenchow, Schratz, and Brenning 2017, first published in 2016). Chapter 10 will give a thorough introduction to open-source R/GIS bridges. Map making was not a focus of R’s early spatial capabilities. But soon sp provided methods for advanced map making using both the base and lattice plotting system. Despite this, a demand for the layered grammar of graphics was growing especially after the release of ggplot2 in 2007. ggmap extended ggplot2’s spatial capabilities (Kahle and Wickham 2013). However, its main purpose was the easy access of several APIs to automatically download map tiles (among others, Google Maps and OpenStreetmap) and subsequent plotting of these as a basemap. Though ggmap facilitated map-making with ggplot2, one main limitation remained. To make spatial data work with the ggplot2 system, one needed to fortify spatial objects. Basically, this means, you need to combine the coordinates and attribute slots of a spatial class object into one data frame. While this works well in the case of points, it duplicates the same information over and over again in the case of polygons, since each coordinate (vertex) of a polygon receives the attribute data of the polygon. This is especially disadvantageous if you need to deal with tens of thousands of polygons. With the introduction of simple features to R this limitation disappears, and it seems likely that this will make ggplot2 the standard tool for the visualization of vector data. This might be different regarding the visualization of raster data. Raster visualization methods received a boost with the release of rasterVis (Lamigueiro 2014) which builds on top of the lattice system. More recently, new packages aim at easing the creation of complex, high-quality maps with minimal code. The tmap package (released in 2014) might serve as an archetype for this kind of development (Tennekes 2018a). It facilitates the user-friendly creation of thematic maps with an intuitive command-line interface (see also mapmisc) . tmap is a sophisticated yet user friendly mapping package which works in harmony with the leaflet package (released in 2015) for interactive map making (Cheng, Karambelkar, and Xie 2017). Similarly, the mapview package builds also on top of leaflet (Appelhans et al. 2018) for interactive mapping based on sp or sf objects. mapview allows the access of a wide range of background maps, scale bars and more. However, it is noteworthy that among all the recent developments described above, the support for simple features (sf; Pebesma 2018) has been without doubt the most important evolution in R’s spatial ecosystem. Naturally, this is the reason why we will describe sf in detail in Chapter 2. 1.6 Exercises Think about the terms ‘GIS’, ‘GDS’ and ‘Geocomputation’ described above. Which is your favorite, and why? Provide three reasons for using a scriptable language such as R for geocomputation instead of using an established GIS program such as QGIS. Name two advantages and two disadvantages of using mature packages compared with ‘cutting edge’ packages for spatial data (for example sp vs sf). References "],
["spatial-class.html", "2 Geographic data in R Prerequisites 2.1 Vector data 2.2 Raster data 2.3 Coordinate Reference Systems 2.4 Units 2.5 Exercises", " 2 Geographic data in R Prerequisites This is the first practical chapter of the book, and therefore it comes with some software requirements. We assume that you have an up-to-date version of R installed and that you are comfortable using software with a command-line interface such as the integrated development environment (IDE) RStudio.12 After you’ve checked you R installation and brushed-up on your R skills where appropriate, the next step is to install and load the packages used in this chapter. Packages are installed with install.packages(&quot;package_name&quot;). We will use the two packages that provide functions for handling spatial data, loaded with library(package_name) as follows: library(sf) # classes and functions for vector data library(raster) # classes and functions for raster data The chapter also relies on two data packages: spData and spDataLarge. Importantly, the spDataLarge package needs be installed with the following command: install.packages(&quot;spDataLarge&quot;, repos = &quot;https://nowosad.github.io/drat/&quot;, type = &quot;source&quot;). library(spData) # load geographic data library(spDataLarge) # load larger geographic data On Mac and Linux a few requirements must be met to install sf. These are described in the package’s README at github.com/r-spatial/sf. This chapter will provide brief explanations of the fundamental geographic data models: vector and raster. We will introduce the theory behind each data model and the disciplines in which they predominate, before demonstrating their implementation in R. The vector data model represents the world using points, lines and polygons. These have discrete, well-defined borders, meaning that vector datasets usually have a high level of precision (but not necessarily accuracy as we will see in 2.4). The raster data model divides the surface up into cells of constant size. Raster datasets are the basis of background images used in web-mapping and have been a vital source of geographic data since the origins of aerial photography and satellite-based remote sensing devices. Rasters aggregate spatially specific features to a given resolution, meaning that they are consistent over space and scalable (many worldwide raster datasets are available). Which to use? The answer likely depends on your domain of application: Vector data tends to dominate the social sciences because human settlements tend to have discrete borders. Raster often dominates in environmental sciences because of the reliance on remote sensing data. There is much overlap in some fields and raster and vector datasets can be used side-by-side: ecologists and demographers, for example, commonly use both vector and raster data. Whether your work involves more use of vector or raster datasets, it is worth understanding the underlying data model before using them, as discussed in subsequent chapters. This book uses sf and raster packages to work with vector data and raster datasets respectively. 2.1 Vector data Take care when using the word ‘vector’ as it can have two meanings in this book: geographic vector data and the vector class (note the monospace font) in R. The former is a data model, the latter is an R class just like data.frame and matrix. Still, there is a link between the two: the spatial coordinates which are at the heart of the geographic vector data model can be represented in R using vector objects. The geographic vector model is based on points located within a coordinate reference system (CRS). Points can represent self-standing features (e.g. the location of a bus stop) or they can be linked together to form more complex geometries such as lines and polygons. Most point geometries contain only two dimensions (3 dimensional CRSs contain an additional \\(z\\) value, typically representing height above sea level). In this system London, for example, can be represented by the coordinates c(-0.1, 51.5). This means that its location is -0.1 degrees east and 55.5 degrees north of the origin. The origin in this case is at 0 degrees longitude (the Prime Meridian) and 0 degree latitude (the Equator) in a geographic (‘lon/lat’) CRS (Figure 2.1, left panel). The same point could also be approximated in a projected CRS with ‘Easting/Northing’ values of c(530000, 180000) in the British National Grid (BNG), meaning that London is located 530 km East and 180 km North of the \\(origin\\) of the CRS. This can be verified visually: slightly more than 5 ‘boxes’ — square areas bounded by the grey grid lines 100 km in width — separate the point representing London from the origin (Figure 2.1, right panel). The location of BNG’s origin, in the sea beyond South West Peninsular, ensures that most locations in the UK have positive Easting and Northing values.13 There is more to CRSs, as described in sections 2.3 and 5.2 but, for the purposes of this section, it is sufficient to know that coordinates consist of two numbers representing distance from an origin, usually in \\(x\\) then \\(y\\) dimensions. Figure 2.1: Illustration of vector (point) data in which location of London (the red X) is represented with reference to an origin (the blue circle). The left plot represents a geographic CRS with an origin at 0° longitude and latitude. The right plot represents a projected CRS with an origin located in the sea west of the South West Peninsula. 2.1.1 An introduction to simple features Simple features is an open standard developed and endorsed by the Open Geospatial Consortium (OGC) to represent a wide range of geographic information. It is a hierarchical data model that simplifies geographic data by condensing a complex range of geographic forms into a single geometry class. Only 7 out of 17 possible types of simple feature are currently used in the vast majority of GIS operations (Figure 2.2). The R package sf (Pebesma 2018) fully supports all of these (including plotting methods etc.).14 Figure 2.2: The subset of the Simple Features class hierarchy supported by sf. sf can represent all common vector geometry types (raster data classes are not supported by sf): points, lines, polygons and their respective ‘multi’ versions (which group together features of the same type into a single feature). sf also supports geometry collections, which can contain multiple geometry types in a single object. Given the breadth of geographic data forms, it may come as a surprise that a class system to support all of them is provided in a single package, which can be installed from CRAN:15 sf incorporates the functionality of the three main packages of the sp paradigm (sp (Pebesma and Bivand 2018) for the class system, rgdal (Bivand, Keitt, and Rowlingson 2018) for reading and writing data, rgeos (Bivand and Rundel 2017) for spatial operations undertaken by GEOS) in a single, cohesive whole. This is well-documented in sf’s vignettes. As the first vignette explains, simple feature objects in R are stored in a data frame, with geographic data occupying a special column, a ‘list-column’. This column is usually named ‘geom’ or ‘geometry’. We will use the world dataset provided by the spData, loaded at the beginning of this chapter (see nowosad.github.io/spData for a list datasets loaded by the package). world is a spatial object containing spatial and attribute columns, the names of which are returned by the function names() (the last column contains the geographic information): names(world) #&gt; [1] &quot;iso_a2&quot; &quot;name_long&quot; &quot;continent&quot; &quot;region_un&quot; &quot;subregion&quot; #&gt; [6] &quot;type&quot; &quot;area_km2&quot; &quot;pop&quot; &quot;lifeExp&quot; &quot;gdpPercap&quot; #&gt; [11] &quot;geom&quot; It is the contents of this modest-looking geom column that gives sf objects their spatial powers, a ‘list-column’ that contains all the coordinates. The sf package provides a plot() method for visualizing geographic data: the follow command creates Figure 2.3. plot(world) Figure 2.3: A spatial plot of the world using the sf package, with a facet for each attribute. Note that instead of creating a single map, as most GIS programs would, the plot() command has created multiple maps, one for each variable in the world datasets. This behavior can be useful for exploring the spatial distribution of different variables and is discussed further in 2.1.3 below. Being able to treat spatial objects as regular data frames with spatial powers has many advantages, especially if you are already used to working with data frames. The commonly used summary() function, for example, provides a useful overview of the variables within the world object. summary(world[&quot;lifeExp&quot;]) #&gt; lifeExp geom #&gt; Min. :48.9 MULTIPOLYGON :177 #&gt; 1st Qu.:64.3 epsg:4326 : 0 #&gt; Median :72.8 +proj=long...: 0 #&gt; Mean :70.6 #&gt; 3rd Qu.:77.1 #&gt; Max. :83.6 #&gt; NA&#39;s :9 Although we have only selected one variable for the summary command, it also outputs a report on the geometry. This demonstrates the ‘sticky’ behavior of the geometry columns of sf objects, meaning the geometry is kept unless the user deliberately removes them, as we’ll see in section 3.2. The result provides a quick summary of both the non-spatial and spatial data contained in world: the mean average life expectancy is 71 years (ranging from less than 50 to more than 80 years with a median of 73 years) across all countries. The word MULTIPOLYGON in the summary output above refers to the geometry type of features (countries) in the world object. This representation is necessary for countries with islands such as Indonesia and Greece. Other geometry types are described in section 2.1.5. It is worth taking a deeper look at the basic behavior and contents of this simple feature object, which can usefully be thought of as a ’Spatial dataFrame). sf objects are easy to subset. The code below shows its first two rows and three columns. The output shows two major differences compared with a regular data.frame: the inclusion of additional geographic data (geometry type, dimension, bbox and CRS information - epsg (SRID), proj4string), and the presence of final geometry column: world[1:2, 1:3] #&gt; Simple feature collection with 2 features and 3 fields #&gt; geometry type: MULTIPOLYGON #&gt; dimension: XY #&gt; bbox: xmin: 11.6 ymin: -17.9 xmax: 75.2 ymax: 38.5 #&gt; epsg (SRID): 4326 #&gt; proj4string: +proj=longlat +datum=WGS84 +no_defs #&gt; iso_a2 name_long continent geom #&gt; 1 AF Afghanistan Asia MULTIPOLYGON (((61.2 35.7, ... #&gt; 2 AO Angola Africa MULTIPOLYGON (((16.3 -5.88,... All this may seem rather complex, especially for a class system that is supposed to be simple. However, there are good reasons for organizing things this way and using sf. 2.1.2 Why simple features? Simple features is a widely supported data model that underlies data structures in many GIS applications including QGIS and PostGIS. A major advantage of this is that using the data model ensures your work is cross-transferable to other set-ups, for example importing from and exporting to spatial databases. A more specific question from an R perspective is “why use the sf package when sp is already tried and tested”? There are many reasons (linked to the advantages of the simple features model) including: Fast reading and writing of data Enhanced plotting performance sf objects can be treated as data frames in most operations sf functions can be combined using %&gt;% operator and works well with the tidyverse collection of R packages sf function names are relatively consistent and intuitive (all begin with st_) Due to such advantages some spatial packages (including tmap, mapview and tidycensus) have added support for sf. However, it will take many years for most packages to transition and some will never switch. Fortunately these can still be used in a workflow based on sf objects, by converting them to the Spatial class used in sp: library(sp) world_sp = as(world, Class = &quot;Spatial&quot;) # sp functions ... Spatial objects can be converted back to sf in the same way or with st_as_sf(): world_sf = st_as_sf(world_sp, &quot;sf&quot;) 2.1.3 Basic map making You can quickly create basic maps in sf with the base plot() function. By default, sf creates a multi-panel plot (like sp’s spplot()), one sub-plot for each variable (see left-hand image in Figure 2.4). plot(world[3:4]) plot(world[&quot;pop&quot;]) Figure 2.4: Plotting with sf, with multiple variables (left) and a single variable (right). As with sp, you can add further layers to your maps using the add = TRUE-argument of the plot() function .16 To illustrate this, and prepare for content covered in chapters 3 and 4 on attribute and spatial data operations, we will subset and combine countries in the world object, which creates a single object representing Asia: asia = world[world$continent == &quot;Asia&quot;, ] asia = st_union(asia) We can now plot the Asian continent over a map of the world. Note that this only works when the initial plot has only one facet and when reset (which resets plot settings) is set to FALSE: plot(world[&quot;pop&quot;], reset = FALSE) plot(asia, add = TRUE, col = &quot;red&quot;) Figure 2.5: A plot of Asia added as a layer on top of countries worldwide. This can be very useful for quickly checking the geographic correspondence between two or more layers: the plot() function is fast to execute and requires few lines of code, but does not create interactive maps with a wide range of options. For more advanced map making we recommend using a dedicated visualization package such as tmap, ggplot2, mapview, or leaflet. 2.1.4 Base plot arguments sf simplifies spatial data objects compared with sp and provides a near-direct interface to GDAL and GEOS C++ functions. In theory this should make sf faster than sp/rgdal/rgeos. This section introduces sf classes in preparation for subsequent chapters which deal with vector data (in particular Chapters 4 and 5). As a final exercise, we will see one way of how to do a spatial overlay in sf. First, we convert the countries of the world into centroids, and then subset those in Asia. Finally, the summary command tells us how many centroids (countries) are part of Asia (43) and how many are not (134). world_centroids = st_centroid(world) sel_asia = st_intersects(world_centroids, asia, sparse = FALSE) #&gt; although coordinates are longitude/latitude, st_intersects assumes that they are planar summary(sel_asia) #&gt; V1 #&gt; Mode :logical #&gt; FALSE:134 #&gt; TRUE :43 Note: st_intersects() uses GEOS in the background for the spatial overlay operation (see also Chapter 4). Since sf’s plot() function builds on base plotting methods, you may also use its many optional arguments (see ?graphics::plot and ?par). This provides many options, but may not be the most concise way to generate maps for publication (see Chapter 9). A simple illustration of the flexibility of plot() is provided in Figure 2.6, which adds circles representing population size to a map of the world. A basic version of the map can be created with the following commands (see exercises in section 2.5 and the script 02-contplot in the GitHub repo for further details): plot(world[&quot;continent&quot;], reset = FALSE) cex = sqrt(world$pop) / 10000 plot(st_geometry(world_centroids), add = TRUE, cex = cex) Figure 2.6: Country continents (represented by fill color) and 2015 populations (represented by points, with point area proportional to population) worldwide. 2.1.5 Simple feature classes To understand new data formats in depth, it often helps to build them from the ground up. This section walks you through vector spatial classes step-by-step, from the elementary simple feature geometry to simple feature objects of class sf representing complex spatial data. Before describing each geometry type that the sf package supports, it is worth taking a step back to understand the building blocks of sf objects. As stated in section 2.1.1, simple features are simply data frames with at least one special column that makes it spatial. These spatial columns are often called geom or geometry and can be like non-spatial columns: world$geom refers to the spatial element of the world object described above. These geometry columns are ‘list columns’ of class sfc. In turn, sfc objects are composed of one or more objects of class sfg: simple feature geometries. To understand how the spatial components of simple features work, it is vital to understand simple feature geometries. For this reason we cover each currently supported sfg type in the next subsections before moving on to describe how these can be combined to form sfc and eventually full sf objects. 2.1.5.1 Simple feature geometry types Geometries are the basic building blocks of simple features. Simple features in R can take on one of the 17 geometry types supported by the sf package. In this chapter we will focus on the seven most commonly used types: POINT, LINESTRING, POLYGON, MULTIPOINT, MULTILINESTRING, MULTIPOLYGON and GEOMETRYCOLLECTION. Find the whole list of possible feature types in the PostGIS manual. Generally, well-known binary (WKB) or well-known text (WKT) are the standard encoding for simple feature geometries. WKB representations are usually hexadecimal strings easily readable for computers. This is why GIS and spatial databases use WKB to transfer and store geometry objects. WKT, on the other hand, is a human-readable text markup description of simple features. Both formats are exchangeable, and if we present one, we will naturally choose the WKT representation. The basis for each geometry type is the point. A point is simply a coordinate in 2D, 3D or 4D space (see vignette(&quot;sf1&quot;) for more information) such as: POINT (5 2) A linestring is a sequence of points with a straight line connecting the points, for example: LINESTRING (1 5, 4 4, 4 1, 2 2, 3 2) A polygon is a sequence of points that form a closed, non-intersecting ring. Closed means that the first and the last point of a polygon have the same coordinates. By definition, a polygon has one exterior boundary (outer ring) and can have zero or more interior boundaries (inner rings), also known as holes. Polygon without a hole - POLYGON ((1 5, 2 2, 4 1, 4 4, 1 5)) Polygon with one hole - POLYGON ((1 5, 2 2, 4 1, 4 4, 1 5), (2 4, 3 4, 3 3, 2 3, 2 4)) So far we have created geometries with only one geometric entity per feature. However, sf also allows multiple geometries to exist within a single feature (hence the term ‘geometry collection’) using “multi” version of each geometry type: Multipoint - MULTIPOINT (5 2, 1 3, 3 4, 3 2) Multistring - MULTILINESTRING ((1 5, 4 4, 4 1, 2 2, 3 2), (1 2, 2 4)) Multipolygon - MULTIPOLYGON (((1 5, 2 2, 4 1, 4 4, 1 5), (0 2, 1 2, 1 3, 0 3, 0 2))) Finally, a geometry collection might contain any combination of geometry types: Geometry collection - GEOMETRYCOLLECTION (MULTIPOINT (5 2, 1 3, 3 4, 3 2), LINESTRING (1 5, 4 4, 4 1, 2 2, 3 2))) 2.1.5.2 Simple feature geometry (sfg) objects The sfg class represents the different simple feature geometry types: point, linestring, polygon (and their ‘multi’ equivalents, such as multipoints) or geometry collection. Usually you are spared the tedious task of creating geometries on your own since you can simply import an already existing spatial file. However, there are a set of functions to create simple feature geometry objects (sfg) from scratch if needed. The names of these functions are simple and consistent, as they all start with the st_ prefix and end with the name of the geometry type in lowercase letters: A point - st_point() A linestring - st_linestring() A polygon - st_polygon() A multipoint - st_multipoint() A multilinestring - st_multilinestring() A multipolygon - st_multipolygon() A geometry collection - st_geometrycollection() sfg objects can be created from three native data types: A numeric vector - a single point A matrix - a set of points, where each row contains a point - a multipoint or linestring A list - any other set, e.g. a multilinestring or geometry collection To create point objects, we use the st_point() function in conjunction with a numeric vector: # note that we use a numeric vector for points st_point(c(5, 2)) # XY point #&gt; POINT (5 2) st_point(c(5, 2, 3)) # XYZ point #&gt; POINT Z (5 2 3) st_point(c(5, 2, 1), dim = &quot;XYM&quot;) # XYM point #&gt; POINT M (5 2 1) st_point(c(5, 2, 3, 1)) # XYZM point #&gt; POINT ZM (5 2 3 1) XY, XYZ and XYZM types of points are automatically created based on the length of a numeric vector. Only the XYM type needs to be specified using a dim argument. By contrast, use matrices in the case of multipoint (st_multipoint()) and linestring (st_linestring()) objects: # the rbind function simplifies the creation of matrices ## MULTIPOINT multipoint_matrix = rbind(c(5, 2), c(1, 3), c(3, 4), c(3, 2)) st_multipoint(multipoint_matrix) #&gt; MULTIPOINT (5 2, 1 3, 3 4, 3 2) ## LINESTRING linestring_matrix = rbind(c(1, 5), c(4, 4), c(4, 1), c(2, 2), c(3, 2)) st_linestring(linestring_matrix) #&gt; LINESTRING (1 5, 4 4, 4 1, 2 2, 3 2) Finally, use lists for the creation of multilinestrings, (multi-)polygons and geometry collections: ## POLYGON polygon_list = list(rbind(c(1, 5), c(2, 2), c(4, 1), c(4, 4), c(1, 5))) st_polygon(polygon_list) #&gt; POLYGON ((1 5, 2 2, 4 1, 4 4, 1 5)) ## POLYGON with a hole polygon_border = rbind(c(1, 5), c(2, 2), c(4, 1), c(4, 4), c(1, 5)) polygon_hole = rbind(c(2, 4), c(3, 4), c(3, 3), c(2, 3), c(2, 4)) polygon_with_hole_list = list(polygon_border, polygon_hole) st_polygon(polygon_with_hole_list) #&gt; POLYGON ((1 5, 2 2, 4 1, 4 4, 1 5), (2 4, 3 4, 3 3, 2 3, 2 4)) ## MULTILINESTRING multilinestring_list = list(rbind(c(1, 5), c(4, 4), c(4, 1), c(2, 2), c(3, 2)), rbind(c(1, 2), c(2, 4))) st_multilinestring((multilinestring_list)) #&gt; MULTILINESTRING ((1 5, 4 4, 4 1, 2 2, 3 2), (1 2, 2 4)) ## MULTIPOLYGON multipolygon_list = list(list(rbind(c(1, 5), c(2, 2), c(4, 1), c(4, 4), c(1, 5))), list(rbind(c(0, 2), c(1, 2), c(1, 3), c(0, 3), c(0, 2)))) st_multipolygon(multipolygon_list) #&gt; MULTIPOLYGON (((1 5, 2 2, 4 1, 4 4, 1 5)), ((0 2, 1 2, 1 3, 0 3, 0 2))) ## GEOMETRYCOLLECTION gemetrycollection_list = list(st_multipoint(multipoint_matrix), st_linestring(linestring_matrix)) st_geometrycollection(gemetrycollection_list) #&gt; GEOMETRYCOLLECTION (MULTIPOINT (5 2, 1 3, 3 4, 3 2), LINESTRING (1 5, 4 4, 4 1, 2 2, 3 2)) 2.1.5.3 Simple feature geometry column One sfg object contains only a single simple feature geometry. A simple feature geometry column (sfc) is a list of sfg objects, which is additionally able to contain information about the coordinate reference system in use. For instance, to combine two simple features into one object with two features, we can use the st_sfc() function. This is important since sfc represents the geometry column in sf data frames: # sfc POINT point1 = st_point(c(5, 2)) point2 = st_point(c(1, 3)) st_sfc(point1, point2) #&gt; Geometry set for 2 features #&gt; geometry type: POINT #&gt; dimension: XY #&gt; bbox: xmin: 1 ymin: 2 xmax: 5 ymax: 3 #&gt; epsg (SRID): NA #&gt; proj4string: NA #&gt; POINT (5 2) #&gt; POINT (1 3) In most cases, an sfc object contains objects of the same geometry type. Therefore, when we convert sfg objects of type polygon into a simple feature geometry column, we would also end up with an sfc object of type polygon. Equally, a geometry column of multilinestrings would result in an sfc object of type multilinestring: # sfc POLYGON polygon_list1 = list(rbind(c(1, 5), c(2, 2), c(4, 1), c(4, 4), c(1, 5))) polygon1 = st_polygon(polygon_list1) polygon_list2 = list(rbind(c(0, 2), c(1, 2), c(1, 3), c(0, 3), c(0, 2))) polygon2 = st_polygon(polygon_list2) st_sfc(polygon1, polygon2) #&gt; Geometry set for 2 features #&gt; geometry type: POLYGON #&gt; dimension: XY #&gt; bbox: xmin: 0 ymin: 1 xmax: 4 ymax: 5 #&gt; epsg (SRID): NA #&gt; proj4string: NA #&gt; POLYGON ((1 5, 2 2, 4 1, 4 4, 1 5)) #&gt; POLYGON ((0 2, 1 2, 1 3, 0 3, 0 2)) # sfc MULTILINESTRING multilinestring_list1 = list(rbind(c(1, 5), c(4, 4), c(4, 1), c(2, 2), c(3, 2)), rbind(c(1, 2), c(2, 4))) multilinestring1 = st_multilinestring((multilinestring_list1)) multilinestring_list2 = list(rbind(c(2, 9), c(7, 9), c(5, 6), c(4, 7), c(2, 7)), rbind(c(1, 7), c(3, 8))) multilinestring2 = st_multilinestring((multilinestring_list2)) st_sfc(multilinestring1, multilinestring2) #&gt; Geometry set for 2 features #&gt; geometry type: MULTILINESTRING #&gt; dimension: XY #&gt; bbox: xmin: 1 ymin: 1 xmax: 7 ymax: 9 #&gt; epsg (SRID): NA #&gt; proj4string: NA #&gt; MULTILINESTRING ((1 5, 4 4, 4 1, 2 2, 3 2), (1 ... #&gt; MULTILINESTRING ((2 9, 7 9, 5 6, 4 7, 2 7), (1 ... It is also possible to create an sfc object from sfg objects with different geometry types: # sfc GEOMETRY st_sfc(point1, multilinestring1) #&gt; Geometry set for 2 features #&gt; geometry type: GEOMETRY #&gt; dimension: XY #&gt; bbox: xmin: 1 ymin: 1 xmax: 5 ymax: 5 #&gt; epsg (SRID): NA #&gt; proj4string: NA #&gt; POINT (5 2) #&gt; MULTILINESTRING ((1 5, 4 4, 4 1, 2 2, 3 2), (1 ... As mentioned before, sfc objects can additionally store information on the coordinate reference systems (CRS). To specify a certain CRS, we can use the epsg (SRID) or proj4string attributes of an sfc object. The default value of epsg (SRID) and proj4string is NA (Not Available): st_sfc(point1, point2) #&gt; Geometry set for 2 features #&gt; geometry type: POINT #&gt; dimension: XY #&gt; bbox: xmin: 1 ymin: 2 xmax: 5 ymax: 3 #&gt; epsg (SRID): NA #&gt; proj4string: NA #&gt; POINT (5 2) #&gt; POINT (1 3) All geometries in an sfc object must have the same CRS. We can add coordinate reference system as a crs argument of st_sfc(). This argument accepts either an integer with the epsg code (for example, 4326) or a proj4string character string (for example, &quot;+proj=longlat +datum=WGS84 +no_defs&quot;) (see section 2.3). # EPSG definition st_sfc(point1, point2, crs = 4326) #&gt; Geometry set for 2 features #&gt; geometry type: POINT #&gt; dimension: XY #&gt; bbox: xmin: 1 ymin: 2 xmax: 5 ymax: 3 #&gt; epsg (SRID): 4326 #&gt; proj4string: +proj=longlat +datum=WGS84 +no_defs #&gt; POINT (5 2) #&gt; POINT (1 3) # PROJ4STRING definition st_sfc(point1, point2, crs = &quot;+proj=longlat +datum=WGS84 +no_defs&quot;) #&gt; Geometry set for 2 features #&gt; geometry type: POINT #&gt; dimension: XY #&gt; bbox: xmin: 1 ymin: 2 xmax: 5 ymax: 3 #&gt; epsg (SRID): 4326 #&gt; proj4string: +proj=longlat +datum=WGS84 +no_defs #&gt; POINT (5 2) #&gt; POINT (1 3) For example, we can set the UTM Zone 11N projection with epsg code 2955: st_sfc(point1, point2, crs = 2955) #&gt; Geometry set for 2 features #&gt; geometry type: POINT #&gt; dimension: XY #&gt; bbox: xmin: 1 ymin: 2 xmax: 5 ymax: 3 #&gt; epsg (SRID): 2955 #&gt; proj4string: +proj=utm +zone=11 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs #&gt; POINT (5 2) #&gt; POINT (1 3) As you can see above, the proj4string definition was automatically added. Now we can try to set the CRS using proj4string: st_sfc(point1, point2, crs = &quot;+proj=utm +zone=11 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs&quot;) #&gt; Geometry set for 2 features #&gt; geometry type: POINT #&gt; dimension: XY #&gt; bbox: xmin: 1 ymin: 2 xmax: 5 ymax: 3 #&gt; epsg (SRID): NA #&gt; proj4string: +proj=utm +zone=11 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs #&gt; POINT (5 2) #&gt; POINT (1 3) However, the epsg string of our result remained empty. This is because there is no general method to convert from proj4string to epsg. 2.1.5.4 Simple feature objects So far, we have only dealt with the pure geometries. Most of the time, however, these geometries come with a set of attributes describing them. These attributes could represent the name of the geometry, measured values, groups to which the geometry belongs, and many more. For example, we measured a temperature of 25°C on Trafalgar Square in London on June 21st 2017. Hence, we have a specific point in space (the coordinates), the name of the location (Trafalgar Square), a temperature value and the date of the measurement. Other attributes might include a urbanity category (city or village), or a remark if the measurement was made using an automatic station. The simple feature class, sf, is a combination of an attribute table (data.frame) and a simple feature geometry column (sfc). Simple features are created using the st_sf() function: # sfg objects london_point = st_point(c(0.1, 51.5)) ruan_point = st_point(c(-9, 53)) # sfc object our_geometry = st_sfc(london_point, ruan_point, crs = 4326) # data.frame object our_attributes = data.frame(name = c(&quot;London&quot;, &quot;Ruan&quot;), temperature = c(25, 13), date = c(as.Date(&quot;2017-06-21&quot;), as.Date(&quot;2017-06-22&quot;)), category = c(&quot;city&quot;, &quot;village&quot;), automatic = c(FALSE, TRUE)) # sf object sf_points = st_sf(our_attributes, geometry = our_geometry) The above example illustrates the components of sf objects. Firstly, coordinates define the geometry of the simple feature geometry (sfg). Secondly, we can combine the geometries in a simple feature geometry column (sfc) which also stores the CRS. Subsequently, we store the attribute information on the geometries in a data.frame. Finally, the st_sf() function combines the attribute table and the sfc object in an sf object. sf_points #&gt; Simple feature collection with 2 features and 5 fields #&gt; geometry type: POINT #&gt; dimension: XY #&gt; bbox: xmin: -9 ymin: 51.5 xmax: 0.1 ymax: 53 #&gt; epsg (SRID): 4326 #&gt; proj4string: +proj=longlat +datum=WGS84 +no_defs #&gt; name temperature date category automatic geometry #&gt; 1 London 25 2017-06-21 city FALSE POINT (0.1 51.5) #&gt; 2 Ruan 13 2017-06-22 village TRUE POINT (-9 53) class(sf_points) #&gt; [1] &quot;sf&quot; &quot;data.frame&quot; The result shows that sf objects actually have two classes, sf and data.frame. Simple features are simply data frames (square tables), but with spatial attributes (usually stored in a special geom list-column in the data frame). This duality is central to the concept of simple features: most of the time a sf can be treated as and behaves like a data.frame. Simple features are, in essence, data frames with a spatial extension. 2.2 Raster data The geographic raster data model consists of a raster header17 and a matrix (with rows and columns) representing equally spaced cells (often also called pixels; Figure 2.7:A). The raster header defines the coordinate reference system, the extent and the origin. The origin (or starting point) is frequently the coordinate of the lower-left corner of the matrix (the raster package, however, uses the upper left corner, by default (Figure 2.7:B)). The header defines the extent via the number of columns, the number of rows and the cell size resolution. Hence, starting from the origin, we can easily access and modify each single cell by either using the ID of a cell (Figure 2.7:B) or by explicitly specifying the rows and columns. This matrix representation avoids storing explicitly the coordinates for the four corner points (in fact it only stores one coordinate, namely the origin) of each cell corner as would be the case for rectangular vector polygons. This and map algebra makes raster processing much more efficient and faster than vector data processing. However, in contrast to vector data, a raster cell can only hold a single value. The value might be numeric or categorical (Figure 2.7:C). Figure 2.7: Raster data: A - cell IDs; B - cell values; C - a colored raster map. Raster maps usually represent continuous phenomena such as elevation, temperature, population density or spectral data (Figure 2.8). Of course, we can represent discrete features such as soil or land-cover classes also with the help of a raster data model (Figure 2.8). Consequently, the discrete borders of these features become blurred, and depending on the spatial task a vector representation might be more suitable. Figure 2.8: Examples of continuous (left) and categorical (right) raster. 2.2.1 An introduction to raster The raster package supports raster objects in R. It provides an extensive set of functions to create, read, export, manipulate and process raster datasets. Aside from general raster data manipulation, raster provides many low level functions that can form the basis to develop more advanced raster functionality. raster also lets you work on large raster datasets that are too large to fit into the main memory. In this case, raster provides the possibility to divide the raster into smaller chunks (rows or blocks), and processes these iteratively instead of loading the whole raster file into RAM (for more information, please refer to vignette(&quot;functions&quot;, package = &quot;raster&quot;). For the illustration of raster concepts, we will use datasets from the spDataLarge (note these packages were loaded at the beginning of the chapter). It consists of a few raster and one vector datasets covering an area of the Zion National Park (Utah, USA). For example, srtm.tif is a digital elevation model of this area (for more details - see its documentation ?srtm) First of all, we would like to create a RasterLayer object named new_raster: raster_filepath = system.file(&quot;raster/srtm.tif&quot;, package = &quot;spDataLarge&quot;) new_raster = raster(raster_filepath) Typing the name of the raster into the console, will print out the raster header (extent, dimensions, resolution, CRS) and some additional information (class, data source name, summary of the raster values): new_raster #&gt; class : RasterLayer #&gt; dimensions : 457, 465, 212505 (nrow, ncol, ncell) #&gt; resolution : 0.000833, 0.000833 (x, y) #&gt; extent : -113, -113, 37.1, 37.5 (xmin, xmax, ymin, ymax) #&gt; coord. ref. : +proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0 #&gt; data source : /home/travis/R/Library/spDataLarge/raster/srtm.tif #&gt; names : srtm #&gt; values : 1024, 2892 (min, max) To access individual header information, you can use following commands: dim(new_raster) (dimensions - number of rows, number of columns, number of cells), res(new_raster) (spatial resolution), extent(new_raster) (spatial extent), and crs(new_raster) (coordinate reference system). Note that in contrast to the sf package, raster only accepts the proj4string representation of the coordinate reference system. Sometimes it is important to know if all values of a raster are currently in memory or on disk. Find out with the inMemory() function: inMemory(new_raster) #&gt; [1] FALSE help(package = &quot;raster&quot;, topic = &quot;raster-package&quot;) returns a full list of all available raster functions. 2.2.2 Basic map making Similar to the sf package, raster also provides plot() methods for its own classes. plot(new_raster) There are several different approaches to plot raster data in R: You can use spplot() to visualize several (such as spatiotemporal) layers at once. You can also do so with the rasterVis package which provides more advanced methods for plotting raster objects. Packages such as tmap, mapview and leaflet facilitate especially interactive mapping of both raster and vector objects. 2.2.3 Raster classes The RasterLayer class represents the simplest form of a raster object, and consists of only one layer. The easiest way to create a raster object in R is to read-in a raster file from disk or from a server. raster_filepath = system.file(&quot;raster/srtm.tif&quot;, package = &quot;spDataLarge&quot;) new_raster = raster(raster_filepath) The raster package supports numerous drivers with the help of rgdal. To find out which drivers are available on your system, run raster::writeFormats() and rgdal::gdalDrivers(). Rasters can also be created from scratch using the raster() function. This is illustrated in the subsequent code chunk, which results in a new RasterLayer object. The resulting raster consists of 36 cells (6 columns and 6 rows specified by nrow and ncol) centered around the Prime Meridian and the Equator (see xmn, xmx, ymn and ymx parameters). The CRS is the default of raster objects: WGS84. This means the unit of the resolution is in degrees which we set to 0.5 (res). Values (vals) are assigned to each cell: 1 to cell 1, 2 to cell 2, and so on. Remember: raster() fills cells row-wise (unlike matrix()) starting at the upper left corner, meaning the top row contains the values 1 to 6, the second 7 to 12 etc. new_raster2 = raster(nrow = 6, ncol = 6, res = 0.5, xmn = -1.5, xmx = 1.5, ymn = -1.5, ymx = 1.5, vals = 1:36) For still further ways of creating a raster object have a look at the help file - ?raster. Aside from RasterLayer, there are two additional classes: RasterBrick and RasterStack. Both can handle multiple layers, but differ regarding the number of supported file formats, type of internal representation and processing speed. A RasterBrick consists of multiple layers, which typically correspond to a single multispectral satellite file or a single multilayer object in memory. The brick() function creates a RasterBrick object. Usually, you provide it with a filename to a multilayer raster file but might also use another raster object and other spatial objects (see its help page for all supported formats). multilayer_raster_filepath = system.file(&quot;raster/landsat.tif&quot;, package=&quot;spDataLarge&quot;) r_brick = brick(multilayer_raster_filepath) r_brick #&gt; class : RasterBrick #&gt; dimensions : 1428, 1128, 1610784, 4 (nrow, ncol, ncell, nlayers) #&gt; resolution : 30, 30 (x, y) #&gt; extent : 301905, 335745, 4111245, 4154085 (xmin, xmax, ymin, ymax) #&gt; coord. ref. : +proj=utm +zone=12 +datum=WGS84 +units=m +no_defs +ellps=WGS84 +towgs84=0,0,0 #&gt; data source : /home/travis/R/Library/spDataLarge/raster/landsat.tif #&gt; names : landsat.1, landsat.2, landsat.3, landsat.4 #&gt; min values : 7550, 6404, 5678, 5252 #&gt; max values : 19071, 22051, 25780, 31961 The nlayers function retrieves the number of layers stored in a Raster* object: nlayers(r_brick) #&gt; [1] 4 A RasterStack is similar to a RasterBrick in the sense that it consists also of multiple layers. However, in contrast to RasterBrick, RasterStack allows you to connect several raster objects stored in different files or multiply objects in memory. More specifically, a RasterStack is a list of RasterLayer objects with the same extent and resolution. Hence, one way to create it is with the help of spatial objects already existing in R’s global environment. And again, one can simply specify a path to a file stored on disk. raster_on_disk = raster(r_brick, layer = 1) raster_in_memory = raster(xmn = 301905, xmx = 335745, ymn = 4111245, ymx = 4154085, res = 30) values(raster_in_memory) = sample(1:ncell(raster_in_memory)) crs(raster_in_memory) = crs(raster_on_disk) r_stack = stack(raster_in_memory, raster_on_disk) r_stack #&gt; class : RasterStack #&gt; dimensions : 1428, 1128, 1610784, 2 (nrow, ncol, ncell, nlayers) #&gt; resolution : 30, 30 (x, y) #&gt; extent : 301905, 335745, 4111245, 4154085 (xmin, xmax, ymin, ymax) #&gt; coord. ref. : +proj=utm +zone=12 +datum=WGS84 +units=m +no_defs +ellps=WGS84 +towgs84=0,0,0 #&gt; names : layer, landsat.1 #&gt; min values : 1, 7550 #&gt; max values : 1610784, 19071 Another difference is that the processing time for RasterBrick objects is usually shorter than for RasterStack objects. Decision on which Raster* class should be used depends mostly on a character of input data. Processing of a single mulitilayer file or object is the most effective with RasterBrick, while RasterStack allows calculations based on many files, many Raster* objects, or both. Operations on RasterBrick and RasterStack objects will typically return a RasterBrick. 2.3 Coordinate Reference Systems Vector and raster spatial data types share concepts intrinsic to spatial data. Perhaps the most fundamental of these is the Coordinate Reference System (CRS), which defines how the spatial elements of the data relate to the surface of the Earth (or other bodies). CRSs are either geographic or projected, as introduced at the beginning of this chapter (see Figure 2.1). This section will will explain each type, laying the foundations for section 5.2 on CRS transformations. 2.3.1 Geographic coordinate systems Geographic coordinate systems identify any location on the Earth’s surface using two values — longitude and latitude. Longitude is location in the East-West direction in angular distance from the Prime Meridian plane. Latitude is angular distance North or South of the equatorial plane. Distance in geographic CRSs are therefore not measured in meters. This has important consequences, as demonstrated in section 5.2. The surface of the Earth in geographic coordinate systems is represented by a spherical or ellipsoidal surface. Spherical models assume that the Earth is a perfect sphere of a given radius. Spherical models have the advantage of simplicity but are rarely used because they are inaccurate: the Earth is not a sphere! Ellipsoidal models are defined by two parameters: the equatorial radius and the polar radius. These are suitable because the Earth is compressed: the equatorial radius is around 11.5 km longer than the polar radius (Maling 1992).18 Ellipsoids are part of a wider component of CRSs: the datum. This contains information on what ellipsoid to use (with the ellps parameter in the proj4 CRS library) and the precise relationship between the Cartesian coordinates and location on the Earth’s service. These additional details are stored in the towgs84 argument of proj4 notation (see proj4.org/parameters.html for details). These allow local variations in Earth’s surface, e.g. due to large mountain ranges, to be accounted for in a local CRS. There are two types of datum — local and geocentric. In a local datum such as NAD83 the ellipsoidal surface is shifted to align with the surface at a particular location. In a geocentric datum such as WGS84 the center is the Earth’s center of gravity and the accuracy of projections is not optimized for a specific location. Available datum definitions can be seen by executing st_proj_info(type = &quot;datum&quot;). 2.3.2 Projected coordinate systems Projected CRSs are based on Cartesian coordinates on an implicitly flat surface. They have an origin, x and y axes, and a linear unit of measurement such as meters. All projected CRSs are based on a geographic CRS, described in the previous section, and rely on map projections to convert the three-dimensional surface of the Earth into Easting and Northing (x and y) values in a projected CRS. This transition cannot be done without adding some distortion. Therefore, some properties of the Earth’s surface are distorted in this process, such as area, direction, distance, and shape. A projected coordinate system can preserve only one or two of those properties. Projections are often named based on a property they preserve: equal-area preserves area, azimuthal preserve direction, equidistant preserve distance, and conformal preserve local shape. There are three main groups of projection types - conic, cylindrical, and planar. In a conic projection, the Earth’s surface is projected onto a cone along a single line of tangency or two lines of tangency. Distortions are minimized along the tangency lines and rise with the distance from those lines in this projection. Therefore, it is the best suited for maps of mid-latitude areas. A cylindrical projection maps the surface onto a cylinder. This projection could also be created by touching the Earth’s surface along a single line of tangency or two lines of tangency. Cylindrical projections are used most often when mapping the entire world. A planar projection projects data onto a flat surface touching the globe at a point or along a line of tangency. It is typically used in mapping polar regions. st_proj_info(type = &quot;proj&quot;) gives a list of the available projections supported by the PROJ.4 library. 2.3.3 CRSs in R Two main ways to describe CRS in R are an epsg code or a proj4string definition. Both of these approaches have advantages and disadvantages. An epsg code is usually shorter, and therefore easier to remember. The code also refers to only one, well-defined coordinate reference system. On the other hand, a proj4string definition allows you more flexibility when it comes to specifying different parameters such as the projection type, the datum and the ellipsoid.19 This way you can specify many different projections, and modify existing ones. This also makes the proj4string approach more complicated. epsg points to exactly one particular CRS. Spatial R packages support a wide range of CRSs and they use the long-established proj4 library. Other than searching for EPSG codes online, another quick way to find out about available CRSs is via the rgdal::make_EPSG() function, which outputs a data frame of available projections. Before going into more detail, it’s worth learning how to view and filter them inside R, as this could save time trawling the internet. The following code will show available CRSs interactively, allowing you to filter ones of interest (try filtering for the OSGB CRSs for example): crs_data = rgdal::make_EPSG() View(crs_data) In sf the CRS of an object can be retrieved using st_crs(). For this, we need to read-in a vector dataset: vector_filepath = system.file(&quot;vector/zion.gpkg&quot;, package=&quot;spDataLarge&quot;) new_vector = st_read(vector_filepath) Our new object, new_vector, is a polygon representing the borders of Zion National Park (?zion). st_crs(new_vector) # get CRS #&gt; Coordinate Reference System: #&gt; No EPSG code #&gt; proj4string: &quot;+proj=utm +zone=12 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs&quot; In cases when a coordinate reference system (CRS) is missing or the wrong CRS is set, the st_set_crs() function can be used: new_vector = st_set_crs(new_vector, 26912) # set CRS The warning message informs us that the st_set_crs() function does not transform data from one CRS to another. Figure 2.9: Examples of geographic (WGS 84; left) and projected (NAD83 / UTM zone 12N; right) and coordinate systems for a vector data type. The projection() function can be used to access CRS information from a Raster* object: projection(new_raster) # get CRS #&gt; [1] &quot;+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0&quot; The same function, projection(), is used to set a CRS for raster objects. The main difference, compared to vector data, is that raster objects only accept proj4 definitions: projection(new_raster) = &quot;+proj=utm +zone=12 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs&quot; # set CRS Figure 2.10: Examples of geographic (WGS 84; left) and projected (NAD83 / UTM zone 12N; right) and coordinate systems for a raster data type We will expand on CRSs and how to project from one CRS to another in much more detail in chapter 5. 2.4 Units An important feature of CRSs is that they contain information about spatial units. Clearly it is vital to know whether a house’s measurements are in feet or meters, and the same applies to maps. It is good cartographic practice to add a scale bar onto maps to demonstrate the relationship between distances on the page or screen and distances on the ground. Likewise, it is important to formally specify the units in which the geometry data or pixels are measured to provide context, and ensure that subsequent calculations are done in context. A novel feature of geometry data in sf objects is that they have native support for units. This means that distance, area and other geometric calculations in sf return values that come with a units attribute, defined by the units package (Pebesma, Mailund, and Hiebert 2016). This is advantageous because it prevents confusion caused by the fact that different CRSs use different units (most use meters, some use feet). Furthermore, it also provides information on dimensionality, as illustrated by the following calculation which reports the area of Nigeria: nigeria = world[world$name_long == &quot;Nigeria&quot;, ] st_area(nigeria) #&gt; 9.05e+11 m^2 The result is in units of square meters (m2), showing a) that the result represents two-dimensional space and b) and that Nigeria is a large country! This information, stored as an attribute (which interested readers can discover with attributes(st_area(nigeria))) is advantageous for many reasons, for example it could feed into subsequent calculations such as population density. Reporting units prevents confusion. To take the Nigeria example, if the units remained unspecified, one could incorrectly assume that the units were in km2. To translate the huge number into a more digestible size, it is tempting to divide the results by a million (the number of square meters in a square kilometer): st_area(nigeria) / 1e6 #&gt; 905062 m^2 However, the result is incorrectly given again as square meters. The solution is to set the correct units with the units package: units::set_units(st_area(nigeria), km^2) #&gt; 905062 km^2 Units are of equal importance in the case of raster data. However, so far sf is the only spatial package that supports units, meaning that people working on raster data should approach changes in the units of analysis (for example, converting pixel widths from imperial to decimal units) with care. The new_raster object (see above) uses a UTM projection with meters as units. Consequently, its resolution is also given in meters but you have to know it, since the res() function simply returns a numeric vector. res(new_raster) #&gt; [1] 0.000833 0.000833 If we used the WGS84 projection, the units would change. repr = projectRaster(new_raster, crs = &quot;+init=epsg:4326&quot;) res(repr) #&gt; [1] 7.47e-09 7.52e-09 Again, the res() command gives back a numeric vector without any unit, forcing us to know that the unit of the WGS84 projection is decimal degrees. 2.5 Exercises What does the summary of the geometry column tell us about the world dataset, in terms of: The geometry type? How many countries there are? The coordinate reference system (CRS)? Using sf’s plot() command, create a map of Nigeria in context, building on the code that creates and plots Asia above (see Figure 2.5 for an example of what this could look like). Hint: this used the lwd, main and col arguments of plot(). Bonus: make the country boundaries a dotted grey line. Hint: border is an additional argument of plot() for sf objects. What does the cex argument do in the plot() function that generates Figure 2.6? Why was cex set to the sqrt(world$pop) / 10000 instead of just the population directly? Bonus: what equivalent arguments to cex exist in the dedicated visualization package tmap? Re-run the code that ‘generated’ Figure 2.6 at the end of 2.1.4 and find 3 similarities and 3 differences between the plot produced on your computer and that in the book. What is similar? What has changed? Bonus: play around with and research base plotting arguments to make your version of Figure 2.6 more attractive. Which arguments were most useful? Advanced: try to reproduce the map presented in Figure 2.1.4. Copy-and-pasting is prohibited! Read the raster/nlcd2011.tif file from the spDataLarge package. What kind of information can you get about the properties of this file? Create an empty RasterLayer object called my_raster with 10 columns and 10 rows and resolution of 10 units. Assign random values between 0 and 10 to the new raster and plot it. References "],
["attr.html", "3 Attribute operations Prerequisites 3.1 Introduction 3.2 Vector attribute manipulation 3.3 Manipulating raster objects 3.4 Exercises", " 3 Attribute operations Prerequisites This chapter requires the following packages to be installed and loaded: library(sf) library(raster) library(tidyverse) It also relies on spData, which loads datasets used in the code examples of this chapter: library(spData) 3.1 Introduction Attribute data is non-spatial information associated with geographic (geometry) data. A bus stop provides a simple example: its position would typically be represented by latitude and longitude coordinates (geometry data), in addition to its name. The name is an attribute of the feature (to use Simple Features terminology) that bears no relation to its geometry. Another example is the elevation value (attribute) for a specific grid cell in raster data. Unlike vector data, the raster data model stores the coordinate of the grid cell only indirectly: There is a less clear distinction between attribute and spatial information in raster data. Say, we are in the 3rd row and the 4th column of a raster matrix. To derive the corresponding coordinate, we have to move from the origin three cells in x-direction and four cells in y-direction with the cell resolution defining the distance for each x- and y-step. The raster header gives the matrix a spatial dimension which we need when plotting the raster or when we want to combine two rasters, think, for instance, of adding the values of one raster to another (see also next chapter). The focus of this chapter is manipulating geographic objects based on attributes such as the name of a bus stop and elevation. For vector data this means operations such as subsetting and aggregation (see sections 3.2.1 and 3.2.2). These non-spatial operations have spatial equivalents: the [ operator in base R, for example, works equally for subsetting objects based on their attribute and spatial objects, as we will see in Chapter 4. This is good news: skills developed here are cross-transferable, meaning that this chapter lays the foundation for Chapter 4, which extends the methods presented here to the spatial world. Sections 3.2.3 and 3.2.4 demonstrate how to join data onto simple feature objects using a shared ID and how to create new variables, respectively. Raster attribute data operations are covered in Section 3.3, which covers creating continuous and categorical raster layers and extracting cell values from one layer and multiple layers (raster subsetting). Section 3.3.2 provides an overview of ‘global’ raster operations which can be used to characterize entire raster datasets. 3.2 Vector attribute manipulation Geographic vector data in R are well-support by sf, a class which extends the data.frame. Thus sf objects have one column per attribute variable (such as ‘name’) and one row per observation, or feature (e.g. per bus station). sf objects also have a special column to contain geometry data, usually named geometry. The geometry column is special because it is a list-colum, which can contain multiple geographic entities (points, lines, polygons) per row. In Chapter 2 we saw how to perform generic methods such as plot() and summary() on sf objects. sf also provides methods that allow sf objects to behave like regular data frames: methods(class = &quot;sf&quot;) # methods for sf objects, first 12 shown #&gt; [1] aggregate cbind coerce #&gt; [4] initialize merge plot #&gt; [7] print rbind [ #&gt; [10] [[&lt;- $&lt;- show Many of these functions, including rbind() (for binding rows of data together) and $&lt;- (for creating new columns) were developed for data frames. A key feature of sf objects is that they store spatial and non-spatial data in the same way, as columns in a data.frame (the geometry column is typically called geometry). The geometry column of sf objects is typically called geometry but any name can be used. The following command, for example, creates a geometry column named g: st_sf(data.frame(n = world$name_long), g = world$geom) This enables geometries imported from spatial databases to have a variety of names such as wkb_geometry and the_geom. sf objects also support tibble and tbl classes used in the tidyverse, allowing ‘tidy’ data analysis workflows for spatial data. Thus sf enables the full power of R’s data analysis capabilities to be unleashed on geographic data. Before using these capabilities it’s worth re-capping how to discover the basic properties of vector data objects. Let’s start by using base R functions to get a measure of the world dataset: dim(world) # it is a 2 dimensional object, with rows and columns #&gt; [1] 177 11 nrow(world) # how many rows? #&gt; [1] 177 ncol(world) # how many columns? #&gt; [1] 11 Our dataset contains ten non-geographic columns (and one geometry list-column) with almost 200 rows representing the world’s countries. Extracting the attribute data of an sf object is the same as removing its geometry: world_df = st_set_geometry(world, NULL) class(world_df) #&gt; [1] &quot;data.frame&quot; This can be useful if the geometry column causes problems, e.g. by occupying large amounts of RAM, or to focus the attention on the attribute data. For most cases, however, there is no harm in keeping the geometry column because non-spatial data operations on sf objects only change an object’s geometry when appropriate (e.g. by dissolving borders between adjacent polygons following aggregation). This means that proficiency with attribute data in sf objects equates to proficiency with data frames in R. For many applications, the tidyverse package dplyr offers the most effective and intuitive approach of working with data frames, hence the focus on this approach in this section.20 3.2.1 Vector attribute subsetting Base R subsetting functions include [, subset() and $. dplyr subsetting functions include select(), filter(), and pull(). Both sets of functions preserve the spatial components of attribute data in sf objects. The [ operator can subset both rows and columns. You use indices to specify the elements you wish to extract from an object, e.g. object[i, j], with i and j typically being numbers or logical vectors — TRUEs and FALSEs — representing rows and columns (they can also be character strings, indicating row or column names). Leaving i or j empty returns all rows or columns, so world[1:5, ] returns the first five rows and all columns. The examples below demonstrate subsetting with base R. The results are not shown; check the results on your own computer: world[1:6, ] # subset rows by position world[, 1:3] # subset columns by position world[, c(&quot;name_long&quot;, &quot;lifeExp&quot;)] # subset columns by name A demonstration of the utility of using logical vectors for subsetting is shown in the code chunk below. This creates a new object, small_countries, containing nations whose surface area is smaller than 10,000 km2: sel_area = world$area_km2 &lt; 10000 summary(sel_area) # a logical vector #&gt; Mode FALSE TRUE #&gt; logical 170 7 small_countries = world[sel_area, ] The intermediary sel_object is a logical vector that shows that only seven countries match the query. A more concise command, that omits the intermediary object, generates the same result: small_countries = world[world$area_km2 &lt; 10000, ] The base R function subset() provides yet another way to achieve the same result: small_countries = subset(world, area_km2 &lt; 10000) Base R functions are mature and widely used. However, the more recent dplyr approach has several advantages. It enables intuitive workflows. It is fast, due to its C++ backend. This is especially useful when working with big data as well as dplyr’s database integration. The main dplyr subsetting functions are select(), slice(), filter() and pull(). raster and dplyr packages have a function called select(). If both packages are loaded, this can generate error messages containing the text: unable to find an inherited method for function ‘select’ for signature ‘“sf”’. To avoid this error message, and prevent ambiguity, we use the long-form function name, prefixed by the package name and two colons (usually omitted from R scripts for concise code): dplyr::select(). select() selects columns by name or position. For example, you could select only two columns, name_long and pop, with the following command (note the sticky geom column remains): world1 = dplyr::select(world, name_long, pop) names(world1) #&gt; [1] &quot;name_long&quot; &quot;pop&quot; &quot;geom&quot; select() also allows subsetting of a range of columns with the help of the : operator: # all columns between name_long and pop (inclusive) world2 = dplyr::select(world, name_long:pop) Omit specific columns with the - operator: # all columns except subregion and area_km2 (inclusive) world3 = dplyr::select(world, -subregion, -area_km2) Conveniently, select() lets you subset and rename columns at the same time, for example: world4 = dplyr::select(world, name_long, population = pop) names(world4) #&gt; [1] &quot;name_long&quot; &quot;population&quot; &quot;geom&quot; This is more concise than the base R equivalent: world5 = world[, c(&quot;name_long&quot;, &quot;pop&quot;)] # subset columns by name names(world5)[names(world5) == &quot;pop&quot;] = &quot;population&quot; # rename column manually select() also works with ‘helper functions’ for advanced subsetting operations, including contains(), starts_with() and num_range() (see the help page with ?select for details). All dplyr functions including select() always return a dataframe-like object. To extract a single vector, one has to explicitly use the pull() command. The subsetting operator in base R (see ?[), by contrast, tries to return objects in the lowest possible dimension. This means selecting a single column returns a vector in base R. To turn off this behavior, set the drop argument to FALSE. # create throw-away dataframe d = data.frame(pop = 1:10, area = 1:10) # return dataframe object when selecting a single column d[, &quot;pop&quot;, drop = FALSE] select(d, pop) # return a vector when selecting a single column d[, &quot;pop&quot;] pull(d, pop) Due to the sticky geometry column, selecting a single attribute from an sf-object with the help of [() returns also a dataframe. Contrastingly, pull() and $ will give back a vector. # dataframe object world[, &quot;pop&quot;] # vector objects world$pop pull(world, pop) slice() is the row-equivalent of select(). The following code chunk, for example, selects the 3rd to 5th rows: slice(world, 3:5) filter() is dplyr’s equivalent of base R’s subset() function. It keeps only rows matching given criteria, e.g. only countries with a very high average of life expectancy: # Countries with a life expectancy longer than 82 years world6 = filter(world, lifeExp &gt; 82) The standard set of comparison operators can be used in the filter() function, as illustrated in Table 3.1: Table 3.1: Table of comparison operators that result in boolean (TRUE/FALSE) outputs. Symbol Name == Equal to != Not equal to &gt;, &lt; Greater/Less than &gt;=, &lt;= Greater/Less than or equal &amp;, |, ! Logical operators: And, Or, Not A benefit of dplyr is its compatibility with the pipe operator %&gt;%. This ‘R pipe’, which takes its name from the Unix pipe | and is part of the magrittr package, enables expressive code by ‘piping’ the output of a previous command into the first argument of the next function. This allows chaining data analysis commands, with the data frame being passed from one function to the next. This is illustrated below, in which the world dataset is subset by columns (name_long and continent) and the first five rows (result not shown). world7 = world %&gt;% filter(continent == &quot;Asia&quot;) %&gt;% dplyr::select(name_long, continent) %&gt;% slice(1:5) The above chunk shows how the pipe operator allows commands to be written in a clear order: the above run from top to bottom (line-by-line) and left to right. Without %&gt;% one would be forced to create intermediary objects or use nested function calls, e.g.: world8 = slice( dplyr::select( filter(world, continent == &quot;Asia&quot;), name_long, continent), 1:5) This generates the same result — verify this with identical(world7, world8) — in the same number of lines of code, but in a much more confusing way, starting with the function that is called last! There are additional advantages of pipes from a communication perspective: they encourage adding comments to self-contained functions and allow single lines commented-out without breaking the code. 3.2.2 Vector attribute aggregation Aggregation operations summarize datasets by a ‘grouping variable’, typically an attribute column (spatial aggregation is covered in the next chapter). An example of attribute aggregation is calculating the number of people per continent based on country-level data (one row per country). The world dataset contains the necessary ingredients: the columns pop and continent, the population and the grouping variable respectively. The aim is to find the sum() of country populations for each continent. This can be done with the base R function aggregate() as follows: world_agg1 = aggregate(pop ~ continent, FUN = sum, data = world, na.rm = TRUE) class(world_agg1) #&gt; [1] &quot;data.frame&quot; The result is a non-spatial data frame with six rows, one per continent, and two columns reporting the name and population of each continent (see Table 3.2 with results for the top 3 most populous continents). aggregate() is a generic function which means that it behaves differently depending on its inputs. sf provides a function that can be called directly with sf:::aggregate() that is activated when a by argument is provided, rather than using the ~ to refer to the grouping variable: world_agg2 = aggregate(world[&quot;pop&quot;], by = list(world$continent), FUN = sum, na.rm = TRUE) class(world_agg2) #&gt; [1] &quot;sf&quot; &quot;data.frame&quot; As illustrated above, an object of class sf is returned this time. world_agg2 which is a spatial object containing 6 polygons representing the columns of the world. summarize() is the dplyr equivalent of aggregate(). It usually follows group_by(), which specifies the grouping variable, as illustrated below: world_agg3 = group_by(world, continent) %&gt;% summarize(pop = sum(pop, na.rm = TRUE)) This approach is flexible, allowing the resulting columns to be named. summarize() can also be used to calculate the Earth’s total population (~7 billion) and number of countries as you will see if execute the following command (result not shown): world %&gt;% summarize(pop = sum(pop, na.rm = TRUE), n_countries = n()) Again, this returns spatial data frame of class sf: the aggregation procedure dissolves boundaries within continental land masses (explained in detail in section 5.3.6), resulting in a single feature representing the world. In the previous code chunk pop and n_countries are column names in the result. sum() and n() were the aggregating functions. Let’s combine what we’ve learned so far about dplyr by chaining together functions to find the world’s 3 most populous continents (with dplyr::n() ) and the number of countries they contain. The output of the following code is presented in Table 3.2): world %&gt;% dplyr::select(pop, continent) %&gt;% group_by(continent) %&gt;% summarize(pop = sum(pop, na.rm = TRUE), n_countries = n()) %&gt;% top_n(n = 3, wt = pop) %&gt;% st_set_geometry(value = NULL) Table 3.2: The top 3 most populous continents, and the number of countries in each. continent pop n_countries Africa 1.15e+09 51 Asia 4.31e+09 47 Europe 7.39e+08 39 More details are provided in the help pages (which can be accessed via ?summarize and vignette(package = &quot;dplyr&quot;) and Chapter 5 of R for Data Science. 3.2.3 Vector attribute joining Combining data from different sources is a common task in data preparation. Joins do this by combining tables based on a shared ‘key’ variable. dplyr has multiple join functions including left_join() and inner_join() — see vignette(&quot;two-table&quot;) for a full list. These function names follow conventions used in the database language SQL (Grolemund and Wickham 2016, Chapter 13); using them to join non spatial datasets to sf objects is the focus of this section. dplyr join functions work the same on data frames and sf objects, the only important difference being the geometry list column. The result of data joins can be either an sf or data.frame object. The most common type of attribute join on spatial data takes an sf object as the first argument and adds columns to it from a data.frame specified as the second argument. To illustrate the utility of joins, imagine that you are researching global coffee production. You have managed to extract data hidden-away in a PDF document supplied by the International Coffee Organization (ICO). The results are stored in a data frame called coffee_data which has 3 columns: one (name_long) containing the names of coffee-producing nations and the other two reporting coffee production statistics for 2016 and 2017 (see ?coffee_data for further information). We will use a ‘left join’ (meaning the left-hand dataset is kept intact) to merge this dataset with the pre-existing world dataset. The result of the code chunk below is a new sf object. world_coffee = left_join(world, coffee_data) #&gt; Joining, by = &quot;name_long&quot; class(world_coffee) #&gt; [1] &quot;sf&quot; &quot;data.frame&quot; The resulting simple features object is the same as the orignal world object but has two new variables (with column indeces 11 and 12) reporting coffee production by year. This can be plotted as a map, as illustrated in Figure 3.1, generated with the plot() function below: names(world_coffee) #&gt; [1] &quot;iso_a2&quot; &quot;name_long&quot; #&gt; [3] &quot;continent&quot; &quot;region_un&quot; #&gt; [5] &quot;subregion&quot; &quot;type&quot; #&gt; [7] &quot;area_km2&quot; &quot;pop&quot; #&gt; [9] &quot;lifeExp&quot; &quot;gdpPercap&quot; #&gt; [11] &quot;coffee_production_2016&quot; &quot;coffee_production_2017&quot; #&gt; [13] &quot;geom&quot; plot(world_coffee[&quot;coffee_production_2017&quot;]) Figure 3.1: World coffee production (thousand 60 kg bags) by country, 2017. Source: International Coffee Organization. For joining to work a ‘key variable’ must be supplied in both datasets. By default dplyr uses all variables with matching names. In this case both world_coffee and world objects contained a variable called name_long, explaining the message Joining, by = &quot;name_long&quot;. In the majority of cases where variable names are not the same you have two options: Rename the key variable in one of the objects so they match. Use the by argument to specify the joining variables. The latter approach is demonstrated below on a renamed version of coffee_data: coffee_renamed = rename(coffee_data, nm = name_long) world_coffee2 = left_join(world, coffee_renamed, by = c(name_long = &quot;nm&quot;)) Note that the name in the original object is kept, meaning that world_coffee and the new object world_coffee2 are identical. Another feature of the result is that it has the same number of rows as the original dataset. Although there are only 47 rows of data in coffee_data all 177 the country records are kept intact in world_coffee and world_coffee2: rows in the original dataset with no match are assigned NA values for the new coffee production variables. What if we only want to keep countries that have a match in the key variable? In that case an inner join can be used: world_coffee_inner = inner_join(world, coffee_data) #&gt; Joining, by = &quot;name_long&quot; nrow(world_coffee_inner) #&gt; [1] 44 Note that the result of inner_join() has only 44 rows compared with 47 in coffee_data. What happened to the remaining rows? We can identify the rows that did not match using the setdiff() function as follows: setdiff(coffee_data$name_long, world$name_long) #&gt; [1] &quot;Congo, Dem. Rep. of&quot; &quot;Côte d&#39;Ivoire&quot; &quot;Others&quot; The result shows that 2 countries have not matched due to name discrepancies. These discrepancies can be fixed by identifying the value that they were expected to have in the world dataset and updating the names accordingly. In this case we will use string matching to find out what Congo, Dem. Rep. of and Côte d'Ivoire are called: str_subset(world$name_long, &quot;Ivo|Cong&quot;) #&gt; [1] &quot;Ivory Coast&quot; &quot;Democratic Republic of the Congo&quot; #&gt; [3] &quot;Republic of Congo&quot; From these results we can identify the matching names and update the names in coffee_data accordingly. As demonstrated below, inner_join()ing the updated data frame returns a result with all 46 coffee producing nations represented in the dataset: coffee_data_match = mutate_if(coffee_data, is.character, recode, `Congo, Dem. Rep. of` = &quot;Democratic Republic of the Congo&quot;, `Côte d&#39;Ivoire` = &quot;Ivory Coast&quot;) world_coffee_match = inner_join(world, coffee_data_match) #&gt; Joining, by = &quot;name_long&quot; nrow(world_coffee_match) #&gt; [1] 46 It is also possible to join in the other direction: starting with a non-spatial dataset and adding variables from a simple features object. This is demonstrated below, which starts with the coffee_data_match object and adds variables from the original world dataset. In contrast with the previous joins, the result is not another simple feature object, but a data frame in the form of a tidyverse tibble: the output of a join tends to match its first argument: coffee_world = left_join(coffee_data_match, world) #&gt; Joining, by = &quot;name_long&quot; class(coffee_world) #&gt; [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; In most cases the geometry column is only useful in an sf object. The geometry column can only be used for creating maps and spatial operations if R ‘knows’ it is a spatial object, defined by a spatial package such as sf. Fortunately non-spatial data frames with a geometry list column (like coffee_world) can be coerced into an sf object as follows: st_as_sf(coffee_world). The contents of this section should equip you with know-how to deal with the majority of spatial data use cases. For more advanced coverage of joins, beyond that in Grolemund and Wickham (2016), we recommend checking-out the capabilities of data.table, a high-performance data processing package that is compatible with sf objects, and other on-line materials.21 Another type of join is a spatial join, covered in the next chapter (section 4.2.3). 3.2.4 Creating attributes and removing spatial information Often, we would like to create a new column based on already existing columns. For example, we want to calculate population density for each country. For this we need to divide a population column, here pop, by an area column , here area_km2 with unit area in square km. Using base R, we can type: world_new = world # do not overwrite our original data world_new$pop_dens = world_new$pop / world_new$area_km2 Alternatively, we can use one of dplyr functions - mutate() or transmute(). mutate() adds new columns at the penultimate position in the sf object (the last one is reserved for the geometry): world %&gt;% mutate(pop_dens = pop / area_km2) The difference between mutate() and transmute() is that the latter skips all other existing columns (except for the sticky geometry column): world %&gt;% transmute(pop_dens = pop / area_km2) unite() pastes together existing columns. For example, we want to combine the continent and region_un columns into a new column named con_reg. Additionally, we can define a separator (here: a colon :) which defines how the values of the input columns should be joined, and if the original columns should be removed (here: TRUE): world_unite = world %&gt;% unite(&quot;con_reg&quot;, continent:region_un, sep = &quot;:&quot;, remove = TRUE) The separate() function does the opposite of unite(): it splits one column into multiple columns using either a regular expression or character positions. world_separate = world_unite %&gt;% separate(con_reg, c(&quot;continent&quot;, &quot;region_un&quot;), sep = &quot;:&quot;) The two functions rename() and set_names() are useful for renaming columns. The first replaces an old name with a new one. The following command, for example, renames the lengthy name_long column to simply name: world %&gt;% rename(name = name_long) set_names() changes all column names at once, and requires a character vector with a name matching each column. This is illustrated below, which outputs the same world object, but with very short names: new_names = c(&quot;i&quot;, &quot;n&quot;, &quot;c&quot;, &quot;r&quot;, &quot;s&quot;, &quot;t&quot;, &quot;a&quot;, &quot;p&quot;, &quot;l&quot;, &quot;gP&quot;, &quot;geom&quot;) world %&gt;% set_names(new_names) It is important to note that attribute data operations preserve the geometry of the simple features. As mentioned at the outset of the chapter, it can be useful to remove the geometry. To do this, you have to explicitly remove it because sf explicitly makes the geometry column sticky. This behavior ensures that data frame operations do not accidentally remove the geometry column. Hence, an approach such as select(world, -geom) will be unsuccessful and you should instead use st_set_geometry().22 world_data = world %&gt;% st_set_geometry(NULL) class(world_data) #&gt; [1] &quot;data.frame&quot; 3.3 Manipulating raster objects In contrast to the vector data model underlying simple features (which represents points, lines and polygons as discrete entities in space), raster data represent continuous surfaces. This section shows how raster objects work, by creating them from scratch, building on section 2.2.1. Because of their unique structure, subsetting and other operations on raster datasets work in a different way, as demonstrated in section 3.3.1. The following code recreates the raster dataset used in section 2.2.3, the result of which is illustrated in Figure 3.2. This demonstrates how the raster() function works to create an example raster named elev (representing elevations). elev = raster(nrow = 6, ncol = 6, res = 0.5, xmn = -1.5, xmx = 1.5, ymn = -1.5, ymx = 1.5, vals = 1:36) The result is a raster object with 6 rows and 6 columns (specified by the nrow and ncol arguments), and a minimum and maximum spatial extent in x and y direction (xmn, xmx, ymn, ymax). The vals argument sets the values that each cell contains: numeric data ranging from 1 to 36 in this case. Raster objects can also contain categorical values of class logical or factor variables in R. The following code creates a raster representing grain sizes (Figure 3.2): grain_order = c(&quot;clay&quot;, &quot;silt&quot;, &quot;sand&quot;) grain_char = sample(grain_order, 36, replace = TRUE) grain_fact = factor(grain_char, levels = grain_order) grain = raster(nrow = 6, ncol = 6, res = 0.5, xmn = -1.5, xmx = 1.5, ymn = -1.5, ymx = 1.5, vals = grain_fact) raster objects can contain values of class numeric, integer, logical or factor, but not character. To use character values they must first be converted into an appropriate class, for example using the function factor(). The levels argument was used in the preceding code chunk to create an ordered factor: clay &lt; silt &lt; sand in terms of grain size. See the Data structures chapter of Wickham (2014) for further details on classes. raster objects represent categorical variables as integers, so grain[1, 1] returns a number that represents a unique identifier, rather than “clay”, “silt” or “sand”. The raster object stores the corresponding look-up table or “Raster Attribute Table” (RAT) as a data frame in a new slot named attributes, which can be viewed with ratify(grain) (see ?ratify() for more information). Use the function levels() for retrieving and adding new factor levels to the attribute table: levels(grain)[[1]] = cbind(levels(grain)[[1]], wetness = c(&quot;wet&quot;, &quot;moist&quot;, &quot;dry&quot;)) levels(grain) #&gt; [[1]] #&gt; ID VALUE wetness #&gt; 1 1 clay wet #&gt; 2 2 silt moist #&gt; 3 3 sand dry This behavior demonstrates that raster cells can only possess one value, an identifier which can be used to look up the attributes in the corresponding attribute table (stored in a slot named attributes). This is illustrated in command below, which returns the grain size and wetness of cell IDs 1, 11 and 35, we can run: factorValues(grain, grain[c(1, 11, 35)]) #&gt; VALUE wetness #&gt; 1 sand dry #&gt; 2 silt moist #&gt; 3 clay wet Figure 3.2: Raster datasets with numeric (left) and categorical values (right). 3.3.1 Raster subsetting Raster subsetting is done with the base R operator [, which accepts a variety of inputs: row-column indexing cell IDs coordinates another raster object The latter two represent spatial subsetting (see section 3.3.1 in the next chapter). The first two subsetting options are demonstrated in the commands below — both return the value of the top left pixel in the raster object elev (results not shown): # row 1, column 1 elev[1, 1] # cell ID 1 elev[1] To extract all values or complete rows, you can use values() and getValues(). For multi-layered raster objects stack or brick, this will return the cell value(s) for each layer. For example, stack(elev, grain)[1] returns a matrix with one row and two columns — one for each layer. For multi-layer raster objects another way to subset is with raster::subset(), which extracts layers from a raster stack or brick. The [[ and $ operators can also be used: r_stack = stack(elev, grain) names(r_stack) = c(&quot;elev&quot;, &quot;grain&quot;) # three ways to extract a layer of a stack raster::subset(r_stack, &quot;elev&quot;) r_stack[[&quot;elev&quot;]] r_stack$elev Cell values can be modified by overwriting existing values in conjunction with a subsetting operation. The following code chunk, for example, sets the upper left cell of elev to 0: elev[1, 1] = 0 elev[] #&gt; [1] 0 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 #&gt; [24] 24 25 26 27 28 29 30 31 32 33 34 35 36 Leaving the square brackets empty is a shortcut version of values() for retrieving all values of a raster. Multiple cells can also be modified in this way: elev[1, 1:2] = 0 3.3.2 Summarizing raster objects raster contains functions for extracting descriptive statistics for entire rasters. Printing a raster object to the console by typing its name, returns minimum and maximum values of a raster. summary() provides common descriptive statistics (minimum, maximum, interquartile range and number of NAs). Further summary operations such as the standard deviation (see below) or custom summary statistics can be calculated with cellStats(). cellStats(elev, sd) If you provide the summary() and cellStats() functions with a raster stack or brick object, they will summarize each layer separately, as can be illustrated by running: summary(brick(elev, grain)) Raster value statistics can be visualized in a variety of ways. Specific functions such as boxplot(), density(), hist() and pairs() work also with raster objects, as demonstrated in the histogram created with the command below (not shown): hist(elev) In case a visualization function does not work with raster objects, one can extract the raster data to be plotted with the help of values() or getValues(). Descriptive raster statistics belong to the so-called global raster operations. These and other typical raster processing operations are part of the map algebra scheme which are covered in the next chapter (section 4.3.2). Some function names clash between packages (e.g. select, as discussed in a previous note). In addition to not loading packages by referring to functions verbosely (e.g. dplyr::select()) another way to prevent function names clashes is by unloading the offending package with detach(). The following command, for example, unloads the raster package (this can also be done in the package tab which resides by default in the right-bottom pane in RStudio): detach(“package:raster”, unload = TRUE, force = TRUE). The force argument makes sure that the package will be detached even if other packages depend on it. This, however, may lead to a restricted usability of packages depending on the detached package, and is therefore not recommended. 3.4 Exercises For these exercises we will use the us_states and us_states_df datasets from the spData package: library(spData) data(us_states) data(us_states_df) us_states is a spatial object (of class sf), containing geometry and a few attributes (including name, region, area, and population) of states within the contiguous United States. us_states_df is a data frame (of class data.frame) containing the name and additional variables (including median income and poverty level, for years 2010 and 2015) of US states, including Alaska, Hawaii and Puerto Rico. The data comes from the US Census Bureau, and is documented in ?us_states and ?us_states_df. Create a new object called us_states_name that contains only the NAME column from the us_states object. What is the class of the new object? Select columns from the us_states object which contain population data. Obtain the same result using a different command (bonus: try to find three ways of obtaining the same result). Hint: try to use helper functions, such as contains or starts_with from dplyr (see ?contains). Find all states with the following characteristics (bonus find and plot them): Belong to the Midwest region. Belong to the West region, have an area below 250,000 km2 and in 2015 a population greater than 5,000,000 residents (hint: you may need to use the function units::set_units() or as.numeric()). Belong to the South region, had an area larger than 150,000 km2 or a total population in 2015 larger than 7,000,000 residents. What was the total population in 2015 in the us_states dataset? What was the minimum and maximum total population in 2015? How many states are there in each region? What was the minimum and maximum total population in 2015 in each region? What was the total population in 2015 in each region? Add variables from us_states_df to us_states, and create a new object called us_states_stats. What function did you use and why? Which variable is the key in both datasets? What is the class of the new object? us_states_df has two more variables than us_states. How can you find them? (hint: try to use the dplyr::anti_join function) What was the population density in 2015 in each state? What was the population density in 2010 in each state? How much has population density changed between 2010 and 2015 in each state? Calculate the change in percentages and map them. Change the columns names in us_states to lowercase. (Hint: helper functions - tolower() and colnames() may help). Using us_states and us_states_df create a new object called us_states_sel. The new object should have only two variables - median_income_15 and geometry. Change the name of the median_income_15 column to Income. Calculate the change in median income between 2010 and 2015 for each state. Bonus: what was the minimum, average and maximum median income in 2015 for each region? What is the region with the largest increase of the median income? Create a raster from scratch with nine rows and columns and a resolution of 0.5 decimal degrees (WGS84). Fill it with random numbers. Extract the values of the four corner cells. What is the most common class of our example raster grain (hint: modal())? Plot the histogram and the boxplot of the data(dem, package = &quot;RQGIS&quot;) raster. Now attach also data(ndvi, package = &quot;RQGIS&quot;). Create a raster stack using dem and ndvi, and make a pairs() plot References "],
["spatial-operations.html", "4 Spatial operations Prerequisites 4.1 Introduction 4.2 Spatial operations on vector data 4.3 Spatial operations on raster data 4.4 Exercises", " 4 Spatial operations Prerequisites This chapter requires the same packages used in Chapter 3: library(sf) library(raster) library(tidyverse) library(spData) 4.1 Introduction Spatial operations are a vital part of geocomputation. This chapter shows how spatial objects can be modified in a multitude of ways based on their location and shape. The content builds on the previous chapter because many spatial operations have a non-spatial (attribute) equivalent. This is especially true for vector operations: section 3.2 on vector attribute manipulation provides the basis for understanding its spatial counterpart, namely spatial subsetting (covered in section 4.2.1). Spatial joining (section 4.2.3) and aggregation (4.2.5) also have non-spatial counterparts, covered in the previous chapter. Spatial operations differ from non-spatial operations in some ways, however. To illustrate the point, imagine you are researching road safety. Spatial joins can be used to find road speed limits related with administrative zones, even when no zone ID is provided. But this raises the question: should the road completely fall inside a zone for its values to be joined? Or is simply crossing or being within a certain distance sufficent? When posing such questions it becomes apparent that spatial operations differ substantially from attribute operations on data frames: the type of spatial relationship between objects must be considered. These are covered in section 4.2.2, on topological relations. Another unique aspect of spatial objects is distance. All spatial objects are related through space and distance calculations, covered in section 4.2.6, can be used to explore the strength of this relationship. Naturally, we can also subset rasters based on location and coordinates (section 3.3.1) and merge different raster tiles into one raster (covered in section 4.3.7). The most important spatial operation on raster data, however, is map algebra. Map algebra makes raster processing very elegant and fast (covered in sections 4.3.2 to 4.3.6). Map algebra is also the prerequisite for distance calculations on rasters 4.3.6. It is important to note that spatial operations that use two spatial objects rely on both objects having the same coordinate reference system, a topic that was introduced in 2.3 and which will be covered in more depth in Chapter 5. 4.2 Spatial operations on vector data This section provides an overview of spatial operations on vector geographic data represented as simple features in the sf package before section 4.3, which presents spatial methods using the raster package. 4.2.1 Spatial subsetting Spatial subsetting is the process of selecting features of a spatial object based on whether or not they in some way relate in space to another object. It is analogous to attribute subsetting (covered in section 3.2.1) and can be done with the base R square bracket ([) operator or with the filter() function from the tidyverse. An example of spatial subsetting is provided by the nz and nz_height datasets in spData. These contain projected data on the 16 main regions and 101 highest points in New Zealand respectively (Figure 4.1). The following code chunk first creates an object representing Canterbury, then uses spatial subsetting to return all high points in the region: canterbury = nz %&gt;% filter(Name == &quot;Canterbury&quot;) canterbury_height = nz_height[canterbury, ] Figure 4.1: Illustration of spatial subsetting with red triangles representing 101 high points in New Zealand, clustered near the central Canterbuy region (left). The points in Canterbury were created with the [ subsetting operator (highlighted in grey, right). Like attribute subsetting x[y, ] subsets features of a target x using the contents of a source object y. Instead of y being of class logical or integer — a vector of TRUE and FALSE values or whole numbers — for spatial subsetting it is another spatial (sf) object. Various topological relations can be used for spatial subsetting. These determine the type of spatial relationship that features in the target object must have with the subsetting object to be selected, including touches, crosses or within (see section 4.2.2). Intersects is the default spatial subsetting operator, a default that returns TRUE for many types of spatial relations, including touches, crosses and is within. These alternative spatial operators can be specified with the op = argument, a third argument that can be passed to the [ operator for sf objects. This is demonstrated in the following command which returns the opposite of st_intersect(), points that do not intersect with Cantebury (see in section 4.2.2): nz_height[canterbury, , op = st_disjoint] Note the empty argument — donoted with , , — in the preceding code chunk is not necessary (nz_height[canterbury, op = st_disjoint] returns the same result) but is included to emphasise the fact that op is the third argument in [ for sf objects, after arguments for subsetting rows and columns. nz_height[canterbury, 2, op = st_disjoint], for example, returns the same rows but only includes the second attribute column. Interested readers can see this default value of op set in the first line of the function call by entering its long-form name into the console sf:::`[.sf`. The ?sf help page documents this also. For many applications this is all you’ll need to know about spatial subsetting for vector data. If this is the case, you can safely skip to the next section (4.2.2). If you’re interested in the details, including other ways of subsetting, read-on. Another way of doing spatial subsetting uses objects returned by topological operators. This is demonstrated in the first command below which uses the sparse = FALSE (meaning in English ‘return a dense matrix not a sparse one’) argument in the topological operator st_intersects() to ensure a logical result is returned: sel = st_intersects(x = nz_height, y = canterbury, sparse = FALSE) canterbury_height2 = nz_height[sel, ] This creates sel (short for ‘selection object’) containing only TRUE and FALSE values, which are used in the second line of the pevious code chunk to create canterbury_height2, which is identical to canterbury_height. sel can also be used with filter(): canterbury_height3 = nz_height %&gt;% filter(sel) At this point there are three versions of canterbury_height, one created with spatial subsetting directly and the other two via the intermediary object sel. We can test whether they are identical as follows: identical(x = canterbury_height, y = canterbury_height2) #&gt; [1] TRUE identical(x = canterbury_height, y = canterbury_height3) #&gt; [1] FALSE What is different about canterbury_height3? The only difference is that filter() changed the row names: row.names(canterbury_height)[1:3] #&gt; [1] &quot;5&quot; &quot;6&quot; &quot;7&quot; row.names(canterbury_height3)[1:3] #&gt; [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; If the row names are re-set, the objects become identical: attr(canterbury_height3, &quot;row.names&quot;) = attr(x = canterbury_height, &quot;row.names&quot;) identical(canterbury_height, canterbury_height3) #&gt; [1] TRUE This discarding of row names is not something that is specific to spatial data, as illustrated in the code chunk below. dplyr discards row names by design. For further discussion of this decision, and some controversy, see the (closed) issue #366 in the package’s issue tracker. sel is not, as one might imagine, a logical vector (although it behaves as one as it only has one column) but a logical matrix: class(sel) #&gt; [1] &quot;matrix&quot; typeof(sel) #&gt; [1] &quot;logical&quot; dim(sel) #&gt; [1] 101 1 The dimensions of sel (returned by the base R command dim()) show one row per feature in the target object (nz_height) and a column per feature in the subsetting object (canterbury). The general pattern here is that sel[i, j] is TRUE if the ith feature in the target object intersects with the jth feature in the subsetting object. If there is more than one feature in y the resulting selection matrix must be converted into a vector before it is used for subsetting, e.g. with rowSums(sel_matrix) &gt; 0. Another solution is to convert the default sparse matrix (list) output from st_intersects() to a logical vector using the function lengths(). This approach to spatial subsetting, used internally by sf (see the source code of sf:::`[.sf`), is illustrated in the code chunk below: co = filter(nz, grepl(&quot;Canter|Otag&quot;, Name)) sel_sparse = st_intersects(nz_height, co) sel_vector = lengths(sel_sparse) &gt; 0 heights_co = nz_height[sel_vector, ] The above code chunk results in an object, heights_co, that represents the high points that intersect with either Canterbury or Otago region (hence the object name co). It did this in four stages: Subset the regions of nz containing “Canter” or “Otago” in their names. This was done using the pattern matching function grepl() in combination with the | character, which means ‘or’, resulting in the subsetting object co. Create a sparse geometry binary predicate sgbp object, a list representing which features of nz_height intersect with the regions in co. Convert the selection list into a logical ‘selection vector’. lengths() finds the features in nz_height matching any features in co. Use the result to subset nz_heights, creating a new object heights_co. 4.2.2 Topological relations Topological relations define the spatial relationships between objects. To understand them, it helps to have some simple test data to work with. Figure 4.2 contains a polygon (a), a line (l) and some points (p), which are created in the code below. a_poly = st_polygon(list(rbind(c(-1, -1), c(1, -1), c(1, 1), c(-1, -1)))) a = st_sfc(a_poly) l_line = st_linestring(x = matrix(c(-1, -1, -0.5, 1), , 2)) l = st_sfc(l_line) p_matrix = matrix(c(0.5, 1, -1, 0, 0, 1, 0.5, 1), ncol = 2) p_multi = st_multipoint(x = p_matrix) p = st_sf(st_cast(st_sfc(p_multi), &quot;POINT&quot;)) Figure 4.2: Points (p 1 to 4), line and polygon objects arranged to demonstrate spatial relations. A simple query is: which of the points in p intersect in some way with polygon a? The question can be answered by inspection (points 1 and 2 are over or touch the triangle). It can also be answered by using the topological relation intersects, implemented in sf as follows: st_intersects(p, a) #&gt; Sparse geometry binary predicate list of length 4, where the predicate was `intersects&#39; #&gt; 1: 1 #&gt; 2: 1 #&gt; 3: (empty) #&gt; 4: (empty) The contents of the result should be as you expected: the function returns a positive (1) result for the first two points, and a negative result (represented by an empty vector) for the last two. What may be unexpected is that the result comes in the form of a list of vectors. This sparse matrix output only registers a relation if one exists, reducing the memory requirements of topological operations on multi-feature objects. As we saw in the previous section a dense matrix consisting of TRUE or FALSE values for each combination of features can also be returned when sparse = FALSE: st_intersects(p, a, sparse = FALSE) #&gt; [,1] #&gt; [1,] TRUE #&gt; [2,] TRUE #&gt; [3,] FALSE #&gt; [4,] FALSE The output is a matrix in which each row represents a feature in the target object and each column represents a feature in the selecting object. In this case only the first two features in p intersect with a and there is only one feature in a so the result has only one column. The result can be used for subsetting as we saw in section 4.2.1. Note that st_intersects() returns TRUE for the second feature in the object p even though it just touches the polygon a: intersects is a ‘catch-all’ topological operation which identifies many types of spatial relation. The opposite of st_intersects() is st_disjoint(), which returns only objects that do not spatially relate in any way to the selecting object (note [, 1] converts the result into a vector): st_disjoint(p, a, sparse = FALSE)[, 1] #&gt; [1] FALSE FALSE TRUE TRUE st_within() returns TRUE only for objects that are completely within the selecting object. This applies only to the second object, which is inside the triangular polygon, as illustrated below: st_within(p, a, sparse = FALSE)[, 1] #&gt; [1] TRUE FALSE FALSE FALSE Note that although the first point is within the triangle, it does not touch any part of its border. For this reason st_touches() only returns TRUE for the second point: st_touches(p, a, sparse = FALSE)[, 1] #&gt; [1] FALSE TRUE FALSE FALSE What about features that do not touch, but almost touch the selection object? These can be selected using st_is_within_distance(), which has an additional dist argument. It can be used to set how close target objects need to be before they are selected. Note that although point 4 is one unit of distance from the nearest node of a (at point 2 in Figure 4.2), it is still selected when the distance is set to 0.9. This is illustrated in the code chunk below, the second line of which converts the lengthy list output into a logical object: sel = st_is_within_distance(p, a, dist = 0.9) # can only return a sparse matrix lengths(sel) &gt; 0 #&gt; [1] TRUE TRUE FALSE TRUE 4.2.3 Spatial joining Joining two non-spatial datasets relies on a shared ‘key’ variable, as described in section 3.2.3. Spatial data joining applies the same concept, but instead relies on shared areas of geographic space. As with attribute data, joining adds a new column to the target object (the argument x in joining functions) from a source object (y). The process is illustrated in Figure 4.3, which shows a target object (the asia dataset, left) being joined to a source dataset (the three most populous cities of the world), resulting in a new attribute being added to the joined dataset (right). asia = world %&gt;% filter(continent == &quot;Asia&quot;) urb = urban_agglomerations %&gt;% filter(year == 2020) %&gt;% top_n(n = 3, wt = population_millions) joined = st_join(x = asia, y = urb) %&gt;% na.omit() Figure 4.3: Illustration of a spatial join: the populations of the world’s three largest agglomerations joined onto their respective countries. This operation is also know as spatial overlay. By default, st_join() performs a left join (see section 3.2.3), but it can also do inner joins by setting the argument left = FALSE. Like spatial subsetting, the default topological operator used by st_join() is st_intersects(). This can be changed with the join argument (see ?st_join for details). In the example above, we have added features of a point layer to a polygon layer but there might be multiple point matches per polygon. Had we chosen to select the four (instead of three) most populous cities in the world, two of them would have belonged to China (Shanghai and Beijing, give it a try yourself). In such a case st_join() simply adds a new row. In our example we would have ended up with two polygons representing China. 4.2.4 Non-overlapping joins Sometimes two geographic datasets do not touch but still have a strong geographic relationship enabling joins. The datasets cycle_hire and cycle_hire_osm, already loaded in the spData package, provide a good example. Plotting them shows that they are often closely related but they do not touch, as shown in Figure 4.4, a base version of which is created with the following code below: plot(cycle_hire$geometry, col = &quot;blue&quot;) plot(cycle_hire_osm$geometry, add = TRUE, pch = 3, col = &quot;red&quot;) We can check if any points are the same st_intersects() as shown below: any(st_touches(cycle_hire, cycle_hire_osm, sparse = FALSE)) #&gt; [1] FALSE Figure 4.4: The spatial distribution of cycle hire points in London based on official data (blue) and OpenStreetMap data (red). Imagine that we need to join the capacity variable in cycle_hire_osm onto the official ‘target’ data contained in cycle_hire. This is when a non-overlapping join is needed. The simplest method is to use the topological operator st_is_within_distance() shown in section 4.2.2, using a threshold distance of 20 m. Note that before performing the relation both datasets must be transformed into a projected CRS, saved as new objects denoted by the affix P (for projected) below: cycle_hire_P = st_transform(cycle_hire, 27700) cycle_hire_osm_P = st_transform(cycle_hire_osm, 27700) sel = st_is_within_distance(cycle_hire_P, cycle_hire_osm_P, dist = 20) summary(lengths(sel) &gt; 0) #&gt; Mode FALSE TRUE #&gt; logical 304 438 This shows that there are 438 points in the target object cycle_hire_P within the threshold distance of cycle_hire_osm_P. How to retrieve the values associated with the respective cycle_hire_osm_P points? The solution is again with st_join(), but with an addition dist argument (set to 20 m below): z = st_join(cycle_hire_P, cycle_hire_osm_P, st_is_within_distance, dist = 20) nrow(cycle_hire) #&gt; [1] 742 nrow(z) #&gt; [1] 762 Note that the number of rows in the joined result is greater than the target. This is because some cycle hire stations in cycle_hire_P have multiple matches in cycle_hire_osm_P. To aggregate the values for the overlapping points and return the mean, we can use the aggregation methods learned in Chapter 3, resulting in an object with the same number of rows as the target: z = z %&gt;% group_by(id) %&gt;% summarize(capacity = mean(capacity)) nrow(z) == nrow(cycle_hire) #&gt; [1] TRUE The capacity of nearby stations can be verified by comparing a plot of the capacity of the source cycle_hire_osm data with the results in this new object (plots not shown): plot(cycle_hire_osm[&quot;capacity&quot;]) plot(z[&quot;capacity&quot;]) The result of this join has used a spatial operation to change the attribute data associated with simple features but the geometry associated with each feature has remained unchanged. 4.2.5 Spatial data aggregation Like attribute data aggregation, covered in section 3.2.2, spatial data aggregation can be a way of condensing data. Aggregated data show some statistics about a variable (typically average or total) in relation to some kind of grouping variable. Section 3.2.2 demonstrated how aggregate() and group_by() %&gt;% summarize() condense data based on attribute variables. This section demonstrates how the same functions work using spatial grouping variables. Returning to the example of New Zealand, imagine you want to find out the average height of high points in each region. This is a good example of spatial aggregation: it is the geometry of the source (y or nz in this case) that defines how values in the target object (x or nz_height) are grouped. This is illustrated using the base aggregate() function below: nz_avheight = aggregate(x = nz_height, nz, FUN = mean) The result of the previous command is an sf object with the same geometry as the (spatial) aggregating object (nz).23 The result of the previous operation is illustrated in Figure 4.5. The same result can also be generated using the ‘tidy’ functions group_by() and summarize() (used in combination with st_join()): Figure 4.5: Average height of the top 101 high points across the regions of New Zealand. nz_avheight2 = st_join(nz, nz_height) %&gt;% group_by(Name) %&gt;% summarize(elevation = mean(elevation, na.rm = TRUE)) The resulting nz_avheight objects have the same geometry as the aggregating object nz but with a new column representing the mean average height of points within each region of New Zealand (other summary functions such as median() and sd() can be used in place of mean()). Note that regions containing no points have an associated elevation value of NA. For aggregating operations which also create new geometries, see section 5.3.6. Spatial congruence is an important concept related to spatial aggregation. An aggregating object (which we will refer to as y) is congruent with the target object (x) if the two objects have shared borders. Often this is the case for administrative boundary data, whereby the larger units (e.g. Middle Layer Super Output Areas in the UK or districts in many other European countries) are composed of many smaller units (Output Areas in the UK, see ons.gov.uk for further details or municipalities in many other European countries). Incongruent aggregating objects, by contrast, do not share common borders with the target (Qiu, Zhang, and Zhou 2012). This is problematic for spatial aggregation (and other spatial operations) illustrated in Figure 4.6. Areal interpolation can help to alleviate this issue. It helps to transfer data from one set of areal units to another. A number of algorithms have been developed for areal interpolation, including area weighted and pycnophylactic interpolation methods (Tobler 1979). Figure 4.6: Illustration of congruent (left) and incongruent (right) areal units with respect to larger aggregating zones (translucent blue borders). The simplest useful method for spatial interpolation is area weighted spatial interpolation. This is implemented in st_interpolate_aw(), as demonstrated in the code chunk below. In this case values from the incongruent object are allocated to the aggregating_zones in proportion to the area: agg_aw = st_interpolate_aw(incongruent[, &quot;value&quot;], aggregating_zones, extensive = TRUE) #&gt; Warning in st_interpolate_aw(incongruent[, &quot;value&quot;], aggregating_zones, : #&gt; st_interpolate_aw assumes attributes are constant over areas of x Instead of simply taking the mean average of each area within each aggregating feature, st_interpolate_aw applies a weight to each value in proportion to its area in each aggregating zone (use extensive = FALSE for ‘spatially intensive’ variables such as population density which should be averaged rather than summed). For instance, if one intersection of incongruent and aggregating_zones is 0.5 km2 but the whole incongruent polygon in question has 1 km2 and 100 inhabitants, then the target aggregating zone will obtain half of the population, in this case 50 inhabitants. 4.2.6 Distance relations While topological relations are binary — a feature either intersects with another or does not — distance relations are continuous. The distance between two objects is calculated with the st_distance() function. This is illustrated in the code chunk below, which finds the distance between the highest point in New Zealand and the geographic centroid of the Canterbury region, created in section 4.2.1: nz_heighest = nz_height %&gt;% top_n(n = 1, wt = elevation) canterbury_centroid = st_centroid(canterbury) st_distance(nz_heighest, canterbury_centroid) #&gt; Units: m #&gt; [,1] #&gt; [1,] 115540 There are two potentially surprising things about the result: 1) it comes with a units attribute, so you know that it’s just over 100,000 m (not 100,000 inches, or any other measure of distance!); and 2) it is returned as a matrix, even though the result only contains a single value. This second feature hints at another useful feature of st_distance(), its ability to return distance matrices between all combinations of features in objects x and y. This is illustrated in the command below, which finds the distances between the first three features in nz_height and the Otago and Canterbury regions of New Zealand represented by the object co. st_distance(nz_height[1:3, ], co) #&gt; Units: m #&gt; [,1] [,2] #&gt; [1,] 123537 15498 #&gt; [2,] 94283 0 #&gt; [3,] 93019 0 Note that the distance between the second and third feature in nz_height and the second feature in co is zero. This demonstrates the fact that distances between points and polygons refer to the distance to any part of the polygon: The second and third points in nz_height are in Otago, which can be verified by plotting them (result not shown): plot(co$geometry[2]) plot(nz_height$geometry[2:3], add = TRUE) 4.3 Spatial operations on raster data This section builds on section 3.3, which highlights various basic methods for manipulating raster datasets, to demonstrate more advanced and explicitly spatial raster operations, and uses the objects elev and grain manually created in section 3.3. For the reader’s convenience, these datasets can be also found in the spData package. 4.3.1 Spatial subsetting The previous chapter (section 3.3) demonstrated how to subset raster datasets using cell IDs. Raster cell values can also be extracted by location (coordinates) and other spatial objects. To use coordinates for subsetting, one can ‘translate’ the coordinates into a cell ID with the raster function cellFromXY(). An alternative is to use raster::extract() (there is also a function called extract() in the tidyverse) to extract values. Both methods are demonstrated below to find the value of the cell that covers a point located 0.1 units from the origin. id = cellFromXY(elev, xy = c(0.1, 0.1)) elev[id] # the same as raster::extract(elev, data.frame(x = 0.1, y = 0.1)) It is convenient that both functions also accept objects of class SpatialObjects and sf. Raster objects can also be subset with another raster object, as illustrated in Figure 4.7 (left panel) and demonstrated in the code chunk below: clip = raster(nrow = 3, ncol = 3, res = 0.3, xmn = 0.9, xmx = 1.8, ymn = -0.45, ymx = 0.45, vals = rep(1, 9)) elev[clip] #&gt; [1] 18 24 # we can also use extract # extract(elev, extent(clip)) Basically, this amounts to retrieving the values of the first raster (here: elev) falling within the extent of a second raster (here: clip). To retrieve a spatial output, we can tell R to keep the matrix structure. This will return the two output values as a raster object. elev[clip, drop = FALSE] #&gt; class : RasterLayer #&gt; dimensions : 2, 1, 2 (nrow, ncol, ncell) #&gt; resolution : 0.5, 0.5 (x, y) #&gt; extent : 1, 1.5, -0.5, 0.5 (xmin, xmax, ymin, ymax) #&gt; coord. ref. : +proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0 #&gt; data source : in memory #&gt; names : layer #&gt; values : 18, 24 (min, max) For the same operation we can also use the intersect() and crop() command. Figure 4.7: Subsetting raster values with the help of another raster (left). Raster mask (middle). Output of masking a raster. Frequently, however, we have two rasters with the same extent and resolution where one raster object serves as a mask (Figure 4.7 middle and right panel). In these cases intersect() and crop() are of little use. Instead we can use the [ again or the mask() and overlay() commands: rmask = raster(nrow = 6, ncol = 6, res = 0.5, xmn = -1.5, xmx = 1.5, ymn = -1.5, ymx = 1.5, vals = sample(c(FALSE, TRUE), 36, replace = TRUE)) elev[rmask, drop = FALSE] # using the mask command mask(elev, rmask, maskvalue = TRUE) # using overlay # first we replace FALSE by NA rmask[rmask == FALSE] = NA # then we retrieve the maximum values overlay(elev, rmask, fun = &quot;max&quot;) In the code chunk above, we have created a mask object called rmask randomly setting its values to FALSE and TRUE. Next we only want to keep those values of elev which are TRUE in rmask, or expressed differently, we want to mask elev with rmask. These operations are in fact Boolean local operations since we compare cell-wise two rasters. The next subsection explores these and related operations in more detail. 4.3.2 Map algebra Map algebra makes raster processing really fast. This is because raster datasets only implicitly store coordinates. To derive the coordinate of a specific cell, we have to calculate it using its matrix position and the raster resolution and origin. For the processing, however, the geographic position of a cell is barely relevant as long as we make sure that the cell position is still the same after the processing (one-to-one locational correspondence). Additionally, if two or more raster datasets share the same extent, projection and resolution, one could treat them as matrices for the processing. This is exactly what map algebra is doing. First, it checks the headers of the rasters on which to perform any algebraic operation, and only if they correspondent to each other, the processing goes on. And secondly, map algebra retains the so-called one-to-one locational correspondence. This is where it substantially differs from matrix algebra which changes positions when for example multiplying or dividing matrices. Map algebra (or cartographic modeling) divides raster operations into four subclasses (Tomlin 1990), with each of them either working on one or several grids simultaneously: Local or per-cell operations. Focal or neighborhood operations. Most often the output cell value is the result of a 3 x 3 input cell block. Zonal operations are similar to focal operations but instead of a predefined neighborhood, classes, which can take on any, i.e. also an irregular size and shape, are the basis for calculations. Global or per-raster operations, that means the output cell derives its value potentially from one or several entire rasters. This classification scheme uses basically the number of cells involved in a processing step as distinguishing feature. For the sake of completeness, we should mention that raster operations can also be classified by discipline such as terrain, hydrological analysis or image classification. The following sections explain how each type of map algebra operations can be used, with reference to worked examples (also see vignette(&quot;Raster&quot;) for a technical description of map algebra). 4.3.3 Local operations Local operations comprise all cell-by-cell operations in one or several layers. A good example is the classification of intervals of numeric values into groups such as grouping a digital elevation model into low (class 1), middle (class 2) and high elevations (class 3). Using the reclassify() command, we need first to construct a reclassification matrix, where the first column corresponds to the lower and the second column to the upper end of the class. The third column represents the new value for the specified ranges in column one and two. Here, we assign the raster values in the ranges 0–12, 12–24 and 24–36 are reclassified to take values 1, 2 and 3, respectively. rcl = matrix(c(0, 12, 1, 12, 24, 2, 24, 36, 3), ncol = 3, byrow = TRUE) recl = reclassify(elev, rcl = rcl) We will perform several reclassifactions in chapter 8. Raster algebra is another classical use case of local operations. This includes adding, subtracting and squaring two rasters. Raster algebra also allows logical operations such as finding all raster cells that are greater than a specific value (5 in our example below). The raster package supports all these operations and more, as described in vignette(&quot;Raster&quot;) and demonstrated below (results not show): elev + elev elev^2 log(elev) elev &gt; 5 Instead of arithmetic operators, one can also use the calc() and overlay() functions. These functions are more efficient, hence, they are preferable in the presence of large raster datasets. Additionally, they allow you to directly store an output file. The calculation of the normalized difference vegetation index (NDVI) is one of the most famous local, i.e. pixel-by-pixel, raster operations. It ranges between - 1 and 1 with positive values indicating the presence of living plants (mostly &gt; 0.2). To calculate the NDVI, one uses the red and near-infrared bands of remotely sensed imagery (e.g. Landsat or Sentinel imagery) exploiting the fact that vegetation absorbs light heavily in the visible light spectrum, and especially in the red channel, while reflecting it in the near-infrared spectrum. \\[ \\begin{split} NDVI&amp;= \\frac{\\text{NIR} - \\text{Red}}{\\text{NIR} + \\text{Red}}\\\\ \\end{split} \\] where NIR refers to the near infrared channel and Red to the red channel. Predictive mapping is another interesting application of local raster operations. The response variable corresponds to measured or observed points in space, for example, species richness, the presence of landslides, tree disease or crop yield. Consequently, we can easily retrieve space- or airborne predictor variables from various rasters (elevation, pH, precipitation, temperature, landcover, soil class, etc.). Subsequently, we model our response as a function of our predictors using lm, glm, gam or a machine-learning technique. To make a spatial prediction, all we have to do, is to apply the estimated coefficients to the predictor rasters, and summing up the resulting output rasters (see also Muenchow et al. (2013)). 4.3.4 Focal operations While local functions operate on one cell, though possibly from multiple layers, focal operations take into account a central cell and its neighbors. The neighborhood (also named kernel, filter or moving window) under consideration is typically of size 3-by-3 cells (that is the central cell and its eight surrounding neighbors) but can take on any other (not necessarily rectangular) shape as defined by the user. A focal operation applies an aggregation function to all cells within the specified neighborhood, uses the corresponding output as the new value for the the central cell, and moves on to the next central cell (Figure 4.8). Other names for this operation are spatial filtering and convolution (Burrough, McDonnell, and Lloyd 2015). In R, we can use the focal() function to perform spatial filtering. We define the shape of the moving window with a matrix whose values correspond to weights (see w parameter in the code chunk below). Secondly, the fun parameter lets us specify the function we wish to apply to this neighborhood. Here, we choose the minimum, but any other summary function, including sum(), mean(), or var() can be used. r_focal = focal(elev, w = matrix(1, nrow = 3, ncol = 3), fun = min) Figure 4.8: Input raster (left) and resulting output raster (right) due to a focal operation - summing up 3-by-3 windows. We can quickly check if the output meets our expectations. In our example, the minimum value has to be always the upper left corner of the moving window (remember we have created the input raster by rowwise incrementing the cell values by one starting at the upper left corner). In this example, the weighting matrix consists only of 1s, meaning each cell has the same weight on the output, but this can be changed. Focal functions or filters play a dominant role in image processing. Low-pass or smoothing filters use the mean function to remove extremes. In the case of categorical data, we can replace the mean with the mode, which is the most common value. By contrast, high-pass filters accentuate features. The line detection Laplace and Sobel filters might serve as an example here. Check the focal() help page for how to use them in R (this will also be used in the excercises at the end of this chapter). Also, terrain processing uses heavily focal functions. Think, for instance, of the calculation of the slope, aspect and flow directions. The terrain() function lets you compute a few of these terrain characteristics but has not implemented all popular methods. For example, the Zevenbergen and Thorne method to compute the slope is missing. Equally, many other terrain and GIS functions are not implemented in R such as curvatures, contributing areas, different wetness indexes, and many more. Fortunately, desktop GIS commonly provide these algorithms. In Chapter 13 we will learn how to access GIS functionality from within R. 4.3.5 Zonal operations Zonal operations are similar to focal operations. The difference is that zonal filters can take on any shape instead of just a predefined window. Our grain size raster is a good example (Figure 3.2) because the different grain sizes are spread in an irregular fashion throughout the raster. To find the mean elevation for each grain size class, we can use the zonal() command. This kind of operation is also known as zonal statistics in the GIS world. z = zonal(elev, grain, fun = &quot;mean&quot;) %&gt;% as.data.frame z #&gt; zone mean #&gt; 1 1 17.8 #&gt; 2 2 18.5 #&gt; 3 3 19.2 This returns the statistics for each category, here the mean altitude for each grain size class, and can be added to the attribute table of the ratified raster (see previous chapter). 4.3.6 Global operations and distances Global operations are a special case of zonal operations with the entire raster dataset representing a single zone. The most common global operations are descriptive statistics for the entire raster dataset such as the minimum or maximum (see previous chapter). Aside from that, global operations are also useful for the computation of distance and weight rasters. In the first case, one can calculate the distance from each cell to a specific target cell. For example, one might want to compute the distance to the nearest coast (see also raster::distance()). We might also want to consider topography, that means, we are not only interested in the pure distance but would like also to avoid the crossing of mountain ranges when going to the coast. To do so, we can weight the distance with elevation so that each additional altitudinal meter ‘prolongs’ the euclidean distance. Visibility and viewshed computations also belong to the family of global operations. Many map algebra operations have a counterpart in vector processing (Liu and Mason 2009). Computing a distance raster (zonal operation) while only considering a maximum distance (logical focal operation) is the equivalent to a vector buffer operation (section 5.3.5). Reclassifying raster data (either local or zonal function depending on the input) is equivalent to dissolving vector data (section 4.2.3). Overlaying two rasters (local operation), where one contains NULL or NA values representing a mask, is similar to vector clipping (section 5.3.5). Quite similar to spatial clipping is intersecting two layers (section 4.2.1). The difference is that these two layers (vector or raster) simply share an overlapping area (see Figure 5.12 for an example). However, be careful with the wording. Sometimes the same words have slightly different meanings for raster and vector data models. Aggregating in the case of vector data refers to dissolving polygons while it means increasing the resolution in the case of raster data. In fact, one could see dissolving or aggregating polygons as decreasing the resolution. However, zonal operations might be the better raster equivalent compared to changing the cell resolution. Zonal operations can dissolve the cells of one raster in accordance with the zones (categories) of another raster using an aggregation function (see above). 4.3.7 Merging rasters Suppose we would like to compute the NDVI (see section 4.3.3), and additionally want to compute terrain attributes from elevation data for observations within a study area. Before such computations we would have to acquire airborne or remotely sensed information. The corresponding imagery is often divided into scenes covering a specific spatial extent. Frequently, a study area covers more than one scene. In these cases we would like to merge the scenes covered by our study area. In the easiest case, we can just merge these scenes, that is put them side to side. This is possible with digital elevation data (SRTM, ASTER). In the following code chunk we first download the SRTM elevation data for Austria and Switzerland (for the country codes have a look at ccodes()). In a second step, we merge the two rasters into one. aut = getData(&quot;alt&quot;, country = &quot;AUT&quot;, mask = TRUE) ch = getData(&quot;alt&quot;, country = &quot;CHE&quot;, mask = TRUE) aut_ch = merge(aut, ch) Raster’s merge() command combines two images, and in case they overlap, it uses the value of the first raster. You can do exactly the same with gdalUtils::mosaic_rasters() which is faster, and therefore recommended if you have to merge a multitude of large rasters stored on disk. The merging approach is of little use when the overlapping values do not correspond to each other. This is frequently the case when you want to combine spectral imagery from scenes that were taken on different dates. The merge() command will still work but you will see a clear border in the resulting image. The mosaic() command lets you define a function for the overlapping area. For instance, we could compute the mean value. This might smooth the clear border in the merged result but it will most likely not make it disappear. To do so, we need a more advanced approach. Remote sensing scientists frequently apply histogram matching or use regression techniques to align the values of the first image with those of the second image. The packages landsat (histmatch(), relnorm(), PIF()), satellite (calcHistMatch()) and RStoolbox (histMatch(), pifMatch()) provide the corresponding functions. 4.4 Exercises It was established in section 4.2 that Canterbury was the region of New Zealand containing most of 100 highest points in the country. How many of these high points does Canterbury Region contain? Which region has the second highest number of nz_height points in, and how many does it have? Generalizing the question to all regions: how many of New Zealand’s 16 regions contain points which belong to the top 100 highest points in the country? Which regions? Bonus: create a table listing these regions in order of the number of points and their name. Use data(dem, package = &quot;RQGIS&quot;), and reclassify the elevation in three classes: low, middle and high. Secondly, compute the NDVI (data(ndvi, package = &quot;RQGIS&quot;)) and the mean elevation for each altitudinal class. Apply a line detection filter to data(dem, package = &quot;RQGIS&quot;). Calculate the NDVI of a Landsat image. Use the Landsat image provided by the spDataLarge package (system.file(&quot;raster/landsat.tif&quot;, package=&quot;spDataLarge&quot;)). This post shows how to compute distances to the nearest coastline using raster::distance(). Retrieve a digital elevation model of Spain, and compute a raster which represents the distance to the coast. (Hint: Have a look at getData() to retrieve a digital elevation model and administrative boundaries for Spain.) Before computing the distance raster, you might want to increase the resolution of the input dem raster, otherwise computing time might become too long. Secondly, weight the distance raster with elevation. Every 100 altitudinal meters should increase the distance to the coast by 10 km. Finally, compute the difference between the raster using the euclidean distance and the raster weighted by elevation. (Note that this is a very simple weighting approach. A more advanced approach might instead weight by flow direction, i.e. favor the steepest drop or the slightest increase in elevation.) References "],
["transform.html", "5 Geometric operations Prerequisites 5.1 Introduction 5.2 Reprojecting geographic data 5.3 Geometric operations on vector data 5.4 Geometric operations on raster data 5.5 Exercises", " 5 Geometric operations Prerequisites This chapter uses the same packages as Chapter 4 but with the addition of spDataLarge, which was installed in Chapter 2 (lwgeom is also used, but does not need to be loaded): library(sf) library(raster) library(tidyverse) library(spData) library(spDataLarge) 5.1 Introduction The previous three chapters have demonstrated how geographic datasets are structured in R (Chapter 2) and how to manipulate them based on their non-geographic attributes (Chapter 3) and spatial properties (Chapter 4). This chapter extends these skills by demonstrating how to interact with a modify the geometry underlying spatial datasets. Section 5.3 covers transforming vector geometries with ‘unary’ and ‘binary’ operations. Unary operations work on a single geometry in isolation. This includes simplification (of lines and polygons), the creation of buffers and centroids, and shifting/scaling/rotating single geometries using ‘affine transformations’ (sections 5.3.1 to 5.3.4). Binary transformations modify one geometry based on the shape of another. This includes clipping and geometry unions, covered in sections 5.3.5 and 5.3.6 respectively. Type transformations (from a polygon to a line, for example) are demonstrated in section 5.3.7. Section 5.4 covers geometric transformations on raster objects. This involves changing the size and number of the underlying pixels, and assigning them new values. It teaches how to change the resolution (also called raster aggregation and disaggregation), the extent and the origin of a raster. These operations are especially useful if one would like to align raster datasets from diverse sources. Aligned raster objects share the same header information, allowing them to be processed using map algebra operations, described in section 4.3.2. A vital type of geometry transformation is reprojecting from one coordinate reference system (CRS) to another. Because of the importance of reprojection, introduced in Chapter 2, and the fact that it applies to raster and vector geometries alike, it is the topic of the first section in this chapter. 5.2 Reprojecting geographic data Section 2.3 introduced coordinate reference systems (CRSs) and demonstrated their importance for geocomputation. This section goes further, by demonstrating some problems that can arise when using an inappropriate CRS and how to transform geometries from one CRS to another. Many spatial operations assume that you are using a projected CRS. The GEOS engine underlying most spatial operations in sf, for example, assumes your data is in a projected CRS. For this reason sf contains a function for checking if geometries have a geographic or projected CRS. This is illustrated below using the example of London introduced in section 2.1, which is created by coercing a data.frame into an sf object (the coords argument specifies the coordinates): london = data.frame(lon = -0.1, lat = 51.5) %&gt;% st_as_sf(coords = c(&quot;lon&quot;, &quot;lat&quot;)) st_is_longlat(london) #&gt; [1] NA The results show that when geographic data is created from scratch, or is loaded from a source that has no CRS metadata, the CRS is unspecified by default. The CRS can be set with st_set_crs():24 london = st_set_crs(london, 4326) st_is_longlat(london) #&gt; [1] TRUE Many spatial operations assume that input vector objects are projected, even when in reality they are not. This can lead to problems, as illustrated by the following code chunk, which creates a buffer of one degree around london: london_buff = st_buffer(london, dist = 1) #&gt; Warning in st_buffer.sfc(st_geometry(x), dist, nQuadSegs): st_buffer does #&gt; not correctly buffer longitude/latitude data #&gt; dist is assumed to be in decimal degrees (arc_degrees). The message stating that dist is assumed to be in decimal degrees is useful: it warns that the result may be of limited use because it is in units of latitude and longitude, rather than meters or some other suitable measure of distance. The consequences of a failure to work on projected data are illustrated in Figure 5.1 (left panel): the buffer is elongated in the north-south direction because lines of longitude converge towards the Earth’s poles. The distance between two lines of longitude, called meridians, is around 111 km at the equator (execute geosphere::distGeo(c(0, 0), c(1, 0)) to find the precise distance). This shrinks to zero at the poles. At the latitude of London, for example, meridians are less than 70 km apart (challenge: execute code that verifies this). Lines of latitude, by contrast, have are of constant distance from each other irrespective of latitude: they are always around 111 km apart, including at the equator and near the poles. This is illustrated in Figures 5.1 and 5.3. Do not interpret the warning about the geographic (longitude/latitude) CRS as “the CRS should not be set”: it almost always should be! It is better understood as a suggestion to reproject the data onto a projected CRS. This suggestion does not always need to be heeded: performing spatial and geometric operations makes little or no difference in some cases (e.g. spatial subsetting). But for operations involving distances such as buffering, the only way to ensure a good result is to create a projected copy of the data and run the operation on that. This is done in the code chunk below: london_proj = data.frame(x = 530000, y = 180000) %&gt;% st_as_sf(coords = 1:2, crs = 27700) The result is a new object that is identical to london, but reprojected onto a suitable CRS (the British National Grid, which has an EPSG code of 27700 in this case) that has units of meters. We can verify that the CRS has changed using st_crs() as follows (some of the output has been replace by ...): st_crs(london_proj) #&gt; Coordinate Reference System: #&gt; EPSG: 27700 #&gt; proj4string: &quot;+proj=tmerc +lat_0=49 +lon_0=-2 ... +units=m +no_defs&quot; Notable components of this CRS description include the EPSG code (EPSG: 27700), the projection (transverse Mercator, +proj=tmerc), the origin (+lat_0=49 +lon_0=-2) and units (+units=m).25 The fact that the units of the CRS are meters (rather than degrees) tells us that this is a projected CRS: st_is_longlat(london_proj) now returns FALSE and geometry operations on london_proj will work without a warning, meaning buffers can be produced from it using proper units of distance. As pointed out above, moving one degree means moving a bit more than 111 km at the equator (to be precise: 111,320 meters; to verify this, check out also geosphere::alongTrackDistance(c(0, 0), c(1, 0), c(1, 0))). This is used as the new buffer distance: london_proj_buff = st_buffer(london_proj, 111320) The result in Figure 5.1 (right panel) shows that buffers based on a projected CRS are not distorted: every part of the buffer’s border is equidistant to London. Figure 5.1: Buffer on vector represenations of London with a geographic (left) and projected (right) CRS. The circular point represents London and the grey outline represents the outline of the UK. The importance of CRSs (primarily whether they are projected or geographic) has been demonstrated using the example of London. The subsequent sections go into more depth, exploring which CRS to use and the details of reprojecting vector and raster objects. 5.2.1 Which CRS to use? While CRSs can be set manually — as illustrated in the previous section with st_set_crs(london, 4326) — it is more common in real world applications for CRSs to be set automatically when data is read-in. The main task involving CRSs is often to transform objects provided in one CRS into another. But when should data be transformed? And into which CRS? There are no clear-cut answers to these questions and CRS selection always involves trade-offs (Maling 1992). However there are some general principles, provided in this section, that can help decide. The question of when to transform is easier to answer. In some cases transformation to a projected CRS is essential for geocomputational work. An example is when geometric operations involving distance measurements or area calculations are required. Conversely, if the outputs of a project are to be published in an online map, it may be necessary to convert them to a geographic CRS. If the visualization phase of a project involves publishing results using leaflet via the common format GeoJSON (a common scenario) projected data should probably be transformed to WGS84. Another case is when two objects with different CRSs must be compared or combined: performing a geometric operation on two objects with different CRSs results in an error. This is demonstrated in the code chunk below, which attempts to find the distance between the projected and unprojected versions of london: st_distance(london, london_proj) # &gt; Error: st_crs(x) == st_crs(y) is not TRUE To make the london and london_proj objects geographically comparable one of them must be transformed into the CRS of the other. But which CRS to use? The answer is usually ‘to the projected CRS’, which in this case is the British National Grid (BNG, EPSG:27700): london2 = st_transform(london, 27700) Now that a transformed version of london has been created, using the sf function st_transform(), the distance between the two representations of London can be found. It may come as a surprise that london and london2 are just over 2 km apart!26 st_distance(london2, london_proj) #&gt; Units: m #&gt; [,1] #&gt; [1,] 2018 The question of which CRS is tricky, and there is often no ‘right’ answer: “There exist no all-purpose projections, all involve distortion when far from the center of the specified frame” (Bivand, Pebesma, and Gómez-Rubio 2013). For geographic CRSs the answer is often WGS84, not only for web mapping (covered in the previous paragraph) but also because GPS datasets and thousands of raster and vector datasets are provided in this CRS by default. WGS84 is the most common CRS in the world, so it is worth knowing its EPSG code: 4326. This ‘magic number’ can be used to convert objects with unusual projected CRSs into something that is widely understood. What about when a projected CRS is required? In some cases it is not something that we are free to decide: “often the choice of projection is made by a public mapping agency” (Bivand, Pebesma, and Gómez-Rubio 2013). This means that when working with local data sources, it is likely preferable to work with the CRS in which the data was provided, to ensure compatibility, even if the ‘official’ CRS is not the most accurate. The example of London was easy to answer because a) the CRS ‘BNG’ (with its associated EPSG code 27700) is well-known and b) the original dataset (london) already had that CRS. What about when a projected CRS is needed but the study region lacks a well-known CRS? Again, although there is no universal answer there is a sensible default CRS: Universal Transverse Mercator (UTM). UTM is not actually a single CRS but a system of CRSs that covers the entire world, and breaks it into 60 segments, each containing 6 degrees of longitude. All UTM projections have the same datum (WGS84) and their EPSG codes run sequentially from 32601 to 32660. This makes it possible to create a function (we’ll call it lonlat2UTM) to calculate the EPSG code associated with any point on the planet as follows:27 lonlat2UTM = function(lonlat) { utm = (floor((lonlat[1] + 180) / 6) %% 60) + 1 utm + 32600 } The following command uses this function to identify the UTM zone and associated EPSG code for London: (epsg_utm = lonlat2UTM(st_coordinates(london))) #&gt; [1] 32630 st_crs(epsg_utm) #&gt; Coordinate Reference System: #&gt; EPSG: 32630 #&gt; proj4string: &quot;+proj=utm +zone=30 +datum=WGS84 +units=m +no_defs&quot; As expected by viewing a map of UTM zones (such as that provided by dmap.co.uk), the EPSG code returned refers to UTM zone 30, which would represent a good projected CRS for England if the BNG did not exist. Another approach to automatically selecting a projected CRS specific to a local dataset is to create an azimuthal equidistant (AEQD) projection for the center-point of the study area. This involves creating a custom CRS (with no EPSG code) with units of meters based on the centrepoint of a dataset. This approach should be used with caution: no other datasets will be compatible with the custom CRS created and results may not be accurate when used on extensive datasets covering hundreds of kilometers. Although we used vector datasets to illustrate the points outlined in this section, the principles apply equally to raster datasets. The subsequent sections explain features of CRS transformation that are unique to each geographic data model, continuing with vector data in the next section (section 5.2.2) and moving-on to explain how raster transformation is different, in section 5.2.4. 5.2.2 Reprojecting vector geometries Chapter 2 demonstrated how vector geometries are made-up of points, and how points form the basis of more complex objects such as lines and polygons. Reprojecting vectors thus consists of transforming the coordinates of these points. This is illustrated by cycle_hire_osm, an sf object from spData that represents cycle hire locations across London. The previous section showed how the CRS of vector data can be queried with st_crs(). Although the output of this function is printed as a single entity, the result is in fact a named list of class crs, with names proj4string (which contains full details of the CRS) and epsg for its code. This is demonstrated below: crs_lnd = st_crs(cycle_hire_osm) class(crs_lnd) #&gt; [1] &quot;crs&quot; crs_lnd$epsg #&gt; [1] 4326 This duality of CRS objects means that they can be set either using an EPSG code or a proj4string. This means that st_crs(&quot;+proj=longlat +datum=WGS84 +no_defs&quot;) is equivalent to st_crs(4326), although not all proj4strings have an associated EPSG code. Both elements of the CRS are changed by transforming the object to a projected CRS: cycle_hire_osm_projected = st_transform(cycle_hire_osm, 27700) The resulting object has a new CRS with an EPSG code 27700. But how to find out more details about this EPSG code, or any code? One option is to search for it online. Another option is to use a function from the rgdal package to find the name of the CRS: crs_codes = rgdal::make_EPSG()[1:2] dplyr::filter(crs_codes, code == 27700) #&gt; code note #&gt; 1 27700 # OSGB 1936 / British National Grid The result shows that the EPSG code 27700 represents the British National Grid, a result that could have been found by searching online for “EPSG 27700”. But what about the proj4string element? proj4strings are text strings in a particular format the describe the CRS. They can be seen as formulas for converting a projected point into a point on the surface of the Earth and can be accessed from crs objects as follows (see proj4.org for further details of what the output means): st_crs(27700)$proj4string #&gt; [1] &quot;+proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 ... Printing a spatial object in the console, automatically returns its coordinate reference system. To access and modify it explicitly, use the st_crs function, for example, st_crs(cycle_hire_osm). 5.2.3 Modifying map projections Established CRSs captured by EPSG codes are well-suited for many applications. However in some cases it is desirable to create a new CRS, using a custom proj4string. This system allows a very wide range of projections to be created, as we’ll see in some of the custom map projections in this section. A long and growing list of projections has been developed and many of these these can be set with the +proj= element of proj4strings.28 When mapping the world while preserving areal relationships, the Mollweide projection is a good choice (Jenny et al. 2017) (Figure 5.2). To use this projection, we need to specify it using the proj4string element, &quot;+proj=moll&quot;, in the st_transform function: world_mollweide = st_transform(world, crs = &quot;+proj=moll&quot;) Figure 5.2: Mollweide projection of the world. On the other hand, when mapping the world, it is often desirable to have as little distortion as possible for all spatial properties (area, direction, distance). One of the most popular projections to achieve this is the Winkel tripel projection (Figure 5.3).29 st_transform_proj() from the lwgeom package allows for coordinate transformations to a wide range of CRSs, including the Winkel tripel projection: world_wintri = lwgeom::st_transform_proj(world, crs = &quot;+proj=wintri&quot;) Figure 5.3: Winkel tripel projection of the world. The two main functions for transformation of simple features coordinates are sf::st_transform() and sf::sf_project(). The st_transform function uses the GDAL interface to PROJ.4, while sf_project() (which works with two-column numeric matrices, representing points) and lwgeom::st_transform_proj() use the PROJ.4 API directly. The first one is appropriate for most situations, and provides a set of the most often used parameters and well defined transformations. The second one allows for greater customization of a projection, which includes cases when some of the PROJ.4 parameters (e.g., +over) or projection (+proj=wintri) is not available in st_transform(). Moreover, PROJ.4 parameters can be modified in most CRS definitions. The below code transforms the coordinates to the Lambert azimuthal equal-area projection centered on longitude and latitude of 0 (Figure 5.4). world_laea1 = st_transform(world, crs = &quot;+proj=laea +x_0=0 +y_0=0 +lon_0=0 +lat_0=0&quot;) Figure 5.4: Lambert azimuthal equal-area projection of the world centered on longitude and latitude of 0. We can change the PROJ.4 parameters, for example the center of the projection using the +lon_0 and +lat_0 parameters. The code below gives the map centered on New York City (Figure 5.5). world_laea2 = st_transform(world, crs = &quot;+proj=laea +x_0=0 +y_0=0 +lon_0=-74 +lat_0=40&quot;) Figure 5.5: Lambert azimuthal equal-area projection of the world centered on New York City. More information on CRS modifications can be found in the Using PROJ.4 documentation. 5.2.4 Reprojecting raster geometries The projection concepts described in the previous section apply equally to rasters. However, there are important differences in reprojection of vectors and rasters: transforming a vector object involves changing the coordinates of every vertex but this does not apply to raster data. Rasters are composed of rectangular cells of the same size (expressed by map units, such as degrees or meters), so it is impossible to transform coordinates of pixels separately. Raster reprojection involves creating a new raster object, often with a different number of columns and rows than the original. The attributes must subsequently be re-estimated, allowing the new pixels to be ‘filled’ with appropriate values. This two-stage process is done with projectRaster() from the raster package. Like the st_transform() function demonstrated in the previous section, projectRaster() takes a geographic object (a raster dataset in this case) and a crs argument. However, projectRaster() only accepts the lengthy proj4string definitions of a CRS rather than concise EPSG codes. It is possible to use a EPSG code in a proj4string definition with &quot;+init=epsg:MY_NUMBER&quot;. For example, one can use the &quot;+init=epsg:4326&quot; definition to set CRS to WGS84 (EPSG code of 4326). The PROJ.4 library automatically adds the rest of parameters and converts it into &quot;+init=epsg:4326 +proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0&quot;, Let’s take a look at two examples of raster transformation - using categorical and continuous data. Land cover data are usually represented by categorical maps. The nlcd2011.tif file provides information for a small area in Utah, USA obtained from National Land Cover Database 2011 in the NAD83 / UTM zone 12N CRS. cat_raster = raster(system.file(&quot;raster/nlcd2011.tif&quot;, package = &quot;spDataLarge&quot;)) crs(cat_raster) #&gt; CRS arguments: #&gt; +proj=utm +zone=12 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m #&gt; +no_defs In this region, 14 land cover classes were distinguished30: unique(cat_raster) #&gt; [1] 11 21 22 23 31 41 42 43 52 71 81 82 90 95 When reprojecting categorical raster, we need to ensure that our new estimated values would still have values of our original classes. This could be done using the nearest neighbor method (ngb). In this method, value of the output cell is calculated based on the nearest cell center of the input raster. For example, we want to change the CRS to WGS 84. It can be desired when we want to visualize a raster data on top of a web basemap, such as the Google or OpenStreetMap map tiles. The first step is to obtain the proj4 definition of this CRS, which can be done using the http://spatialreference.org webpage. The second and last step is to define the reprojection method in the projectRaster() function, which in case of categorical data is the nearest neighbor method (ngb): wgs84 = &quot;+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs&quot; cat_raster_wgs84 = projectRaster(cat_raster, crs = wgs84, method = &quot;ngb&quot;) Many properties of the new object differ from the previous one, which include the number of columns and rows (and therefore number of cells), resolution (transformed from meters into degrees), and extent, as illustrated in Table 5.1 (note that the number of categories increases from 14 to 15 because of the addition of NA values, not because a new category has been created — the land cover classes are preserved). Table 5.1: Key attributes in the original and projected categorical raster datasets. CRS nrow ncol ncell resolution unique_categories NAD83 1359 1073 1458207 31.5275 14 WGS84 1394 1111 1548734 0.0003 15 Reprojecting raster data with continuous (numeric or in this case integer) values follows an almost identical procedure. srtm.tif in spDataLarge contains a digital elevation model from the Shuttle Radar Topography Mission (SRTM) representing height above sea level (elevation) in meters. Its CRS is WGS84: con_raster = raster(system.file(&quot;raster/srtm.tif&quot;, package = &quot;spDataLarge&quot;)) crs(con_raster) #&gt; CRS arguments: #&gt; +proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0 We will reproject this dataset into a projected CRS, but not with the nearest neighbor method which is appropriate for categorical data. Instead we will use the bilinear method which computes the output cell value based on the four nearest cells in the original raster. The values in the projected dataset are the distance-weighted average of the values from these four cells: the closer the input cell is to the center of the output cell, the greater its weight. The following commands create a text string representing the Oblique Lambert azimuthal equal-area projection, and reproject the raster into this CRS, using the bilinear method: equalarea = &quot;+proj=laea +lat_0=37.32 +lon_0=-113.04&quot; con_raster_ea = projectRaster(con_raster, crs = equalarea, method = &quot;bilinear&quot;) crs(con_raster_ea) #&gt; CRS arguments: #&gt; +proj=laea +lat_0=37.32 +lon_0=-113.04 +ellps=WGS84 Raster reprojection on numeric variables also leads to small changes values and spatial properties, such as the number of cells, resolution, and extent. These changes are demonstrated in Table 5.231: Table 5.2: Key attributes original and projected continuous (numeric) raster datasets. CRS nrow ncol ncell resolution mean WGS84 457 465 212505 31.5275 1843 Equal-area 467 478 223226 0.0003 1842 Of course, the limitations of 2D Earth projections apply as much to vector as to raster data. At best we can comply with two out of three spatial properties (distance, area, direction). Therefore, the task at hand determines which projection to choose. For instance, if we are interested in a density (points per grid cell or inhabitants per grid cell) we should use an equal-area projection (see also chapter 8). 5.3 Geometric operations on vector data This section is about operations that in some way change the geometry of vector (sf) objects. It is more advanced than the spatial data operations presented in the previous chapter (in section 4.2) because here we drill down into the geometry: the functions discussed in this section work on objects of class sfc in addition to objects of class sf. 5.3.1 Simplification Simplification is a process for generalization of vector objects (lines and polygons) usually for use in smaller scale maps. Another reason for simplifying objects is to reduce the amount of memory, disk space and network bandwidth they consume: it may be wise to simplify complex geometries before publishing them as interactive maps. The sf package provides st_simplify(), which uses the GEOS implementation of the Douglas-Peucker algorithm to reduce the vertex count. st_simplify() uses the dTolerance to control the level of generalization in map units (see Douglas and Peucker 1973 for details). Figure 5.6 illustrates simplification of a LINESTRING geometry representing the river Seine and tributaries. The simplified geometry was created by the following command: seine_simp = st_simplify(seine, dTolerance = 2000) # 2000 m Figure 5.6: Comparison of the original and simplified seine geometry. The resulting seine_simp object is a copy of the original seine but with fewer vertices. This is apparent, with the result being visually simpler (Figure 5.6, right) and consuming less memory than the original object, as verified below: object.size(seine) #&gt; 16768 bytes object.size(seine_simp) #&gt; 7808 bytes Simplification is also applicable for polygons. This is illustrated using us_states, representing the contiguous United States. As we showed in section 5.2, GEOS assumes that the data is in a projected CRS and this could lead to unexpected results when using a geographic CRS. Therefore, the first step is to project the data into some adequate projected CRS, such as US National Atlas Equal Area (epsg = 2163) (on the left in Figure 5.7): us_states2163 = st_transform(us_states, 2163) st_simplify() works equally well with projected polygons: us_states_simp1 = st_simplify(us_states2163, dTolerance = 100000) # 100 km A limitation with st_simplify() is that it simplifies objects on a per-geometry basis. This means the ‘topology’ is lost, resulting in overlapping and ‘holy’ areal units illustrated in Figure 5.7 (middle panel). ms_simplify() from rmapshaper provides an alternative that overcomes this issue. By default it uses the Visvalingam algorithm, which overcomes some limitations of the Douglas-Peucker algorithm (Visvalingam and Whyatt 1993). The following code chunk uses this function to simplify us_states2163. The result has only 1% of the vertices of the input (set using the argument keep) but its number of objects remains intact because we set keep_shapes = TRUE32: # proportion of points to retain (0-1; default 0.05) us_states2163$AREA = as.numeric(us_states2163$AREA) us_states_simp2 = rmapshaper::ms_simplify(us_states2163, keep = 0.01, keep_shapes = TRUE) Finally, the visual comparison of the original dataset and the two simplified versions shows differences between the Douglas-Peucker (st_simplify) and Visvalingam (ms_simplify) algorithm outputs (Figure 5.7): Figure 5.7: Polygon simplification in action, comparing the original geometry of the contiguous United States with simplified versions, generated with functions from sf (center) and rmapshaper (right) packages. 5.3.2 Centroids There are two main functions that create single point representations of more complex vector objects - st_centroid() and st_point_on_surface(). The st_centroid() function calculates the geometric center of a geometry. We can create centroids for polygons, lines (see black points on Figure 5.8) and multipoints: nz_centroid = st_centroid(nz) seine_centroid = st_centroid(seine) Centroids could be useful to represent more complex objects - lines and polygons, for example to calculate distances between centers of polygons. They are also often used as places where polygons or lines labels are put. However, it is important to know that centroids could be located outside of the given object, e.g. in cases of irregular shaped polygons or lines. Examples of this can be seen on the right plot on Figure 5.8. Alternatively, the st_point_on_surface() can be used. nz_pos = st_point_on_surface(nz) seine_pos = st_point_on_surface(seine) This ensures that the created point lies on the given object (see red points on Figure 5.8). Figure 5.8: Centroids (black points) and ‘points on surface’ (red points) of New Zeleand’s regions (left) and the Seine (right) datasets. 5.3.3 Buffers Buffers are polygons representing the area within a given distance of a geometric feature: regardless of whether the input is a point, line or polygon, the output is polygon. Unlike simplification (which is often used for visualization and reducing file size) buffering tends to be used for geographic data analysis. How many points are within a given distance of this line? Which demographic groups are within travel distance of this new shop? These kinds of questions can be answered and visualized by creating buffers around the geographic entities of interest. Figure 5.9 illustrates buffers of different sizes (5 and 20 km) surrounding the river Seine and tributaries. These buffers were created with commands below, which show that the command st_buffer() requires at least two arguments: an input geometry and a distance: seine_buff_5km = st_buffer(seine, dist = 5000) seine_buff_50km = st_buffer(seine, dist = 50000) Figure 5.9: Buffers around the seine datasets of 5km (left) and 50km (right). Note the colors, which reflect the fact that one buffer is created per geometry feature. The third and final argument of st_buffer() is nQuadSegs, which means ‘number of segments per quadrant’ and is set by default to 30 (meaning circles created by buffers are composed of \\(4 \\times 30 = 120\\) lines). This argument rarely needs be set. Unusual cases where it may be useful include when the memory consumed by the output of a buffer operation is a major concern (in which case it should be reduced) or when very high precision is needed (in which case it should be increased). 5.3.4 Affine transformations Affine transformation is any transformation that preserves lines and parallelism. However, angles or length are not necessarily preserved. Affine transformations include, among others, shifting (translation), scaling and rotation. Additionally, it is possible to use any combination of these. Affine transformations are an essential part of geocomputation, e.g. when reprojecting or when improving the geometry of a vector dataset that was created based on a distorted or wrongly projected map. The sf package implements affine transformation for objects of classes sfg and sfc. nz_sfc = st_geometry(nz) Shifting moves every point by the same distance in map units. It could be done by adding a numerical vector to a vector object. For example, the code below shifts all y-coordinates by 100,000 meters to the north but leaves the x-coordinates untouched (left panel on the Fig. 5.10). nz_shift = nz_sfc + c(0, 100000) Scaling enlarges or shrinks objects by a factor. It can be applied either globally or locally. Global scaling increases or decreases all coordinates values in relation to the origin coordinates, while keeping all geometries topological relations intact. It can by done by subtraction or multiplication of asfg or sfc object. Local scaling treats geometries independently and requires points around which geometries are going to be scaled, e.g. centroids. In the example below, each geometry is shrunk by a factor of two around the centroids (central panel on the Fig. 5.10). nz_centroid_sfc = st_centroid(nz_sfc) nz_scale = (nz_sfc - nz_centroid_sfc) * 0.5 + nz_centroid_sfc Rotation of two-dimensional coordinates requires a rotation matrix: \\[ R = \\begin{bmatrix} \\cos \\theta &amp; -\\sin \\theta \\\\ \\sin \\theta &amp; \\cos \\theta \\\\ \\end{bmatrix} \\] It rotates points in a counterclockwise direction. The rotation matrix could be implemented in R as: rotation = function(a){ r = a * pi/180 #degrees to radians matrix(c(cos(r), sin(r), -sin(r), cos(r)), nrow = 2, ncol = 2) } The rotation function accepts one argument a - a rotation angle in degrees. Rotation could be done around selected points, such as centroids (right panel on the Fig. 5.10). See vignette(&quot;sf3&quot;) for more examples. nz_rotate = (nz_sfc - nz_centroid_sfc) * rotation(30) + nz_centroid_sfc Figure 5.10: Illustrations of affine transformations: shift, scale and rotate. Finally, the newly created geometries can replace the old ones with the st_set_geometry() function: nz_scale_sf = st_set_geometry(nz, nz_scale) 5.3.5 Clipping Spatial clipping is a form of spatial subsetting that involves changes to the geometry columns of at least some of the affected features. Clipping can only apply to features more complex than points: lines, polygons and their ‘multi’ equivalents. To illustrate the concept we will start with a simple example: two overlapping circles with a center point one unit away from each other and a radius of one: b = st_sfc(st_point(c(0, 1)), st_point(c(1, 1))) # create 2 points b = st_buffer(b, dist = 1) # convert points to circles l = c(&quot;x&quot;, &quot;y&quot;) plot(b) text(x = c(-0.5, 1.5), y = 1, labels = l) # add text Figure 5.11: Overlapping circles. Imagine you want to select not one circle or the other, but the space covered by both x and y. This can be done using the function st_intersection(), illustrated using objects named x and y which represent the left and right-hand circles: x = b[1] y = b[2] x_and_y = st_intersection(x, y) plot(b) plot(x_and_y, col = &quot;lightgrey&quot;, add = TRUE) # color intersecting area The subsequent code chunk demonstrates how this works for all combinations of the ‘Venn’ diagram representing x and y, inspired by Figure 5.1 of the book R for Data Science (Grolemund and Wickham 2016). Figure 5.12: Spatial equivalents of logical operators. To illustrate the relationship between subsetting and clipping spatial data, we will subset points that cover the bounding box of the circles x and y in Figure 5.12. Some points will be inside just one circle, some will be inside both and some will be inside neither. There are two different ways to subset points that fit into combinations of the circles: via clipping and logical operators. But first we must generate some points. We will use the simple random sampling strategy to sample from a box representing the extent of x and y using the sf function st_sample(). This generates objects plotted in Figure 5.13: bb = st_bbox(st_union(x, y)) pmat = matrix(c(bb[c(1, 2, 3, 2, 3, 4, 1, 4, 1, 2)]), ncol = 2, byrow = TRUE) box = st_polygon(list(pmat)) set.seed(2017) p = st_sample(x = box, size = 10) plot(box) plot(x, add = TRUE) plot(y, add = TRUE) plot(p, add = TRUE) text(x = c(-0.5, 1.5), y = 1, labels = l) Figure 5.13: Randomly distributed points within the bounding box enclosing circles x and y. 5.3.6 Geometry unions Spatial aggregation can also be done in the tidyverse, using dplyr functions as follows: group_by(us_states, REGION) %&gt;% summarize(sum(pop = total_pop_15, na.rm = TRUE)) Further to what was covered in section 3.2.2, aggregation of polygons often silently dissolves the geometries of touching polygons in the same group. This is demonstrated in the code chunk below, in which the REGION variable in us_states is used to aggregate the states into four regions, illustrated in Figure 5.14: regions = aggregate(x = us_states[, &quot;total_pop_15&quot;], by = list(us_states$REGION), FUN = sum, na.rm = TRUE) Figure 5.14: Spatial aggregation on contiguous polygons, illustrated by aggregating the population of US states into regions, with population represented by color. Note the operation automatically dissolves boundaries between states. The equivalent result can be achieved using tidyverse functions as follows (result not shown): regions2 = us_states %&gt;% group_by(REGION) %&gt;% summarize(sum(pop = total_pop_15, na.rm = TRUE)) 5.3.7 Type transformations Geometry casting is a powerful operation which enables transformation of the geometry type. It is implemented in the st_cast function from the sf package. Importantly, st_cast behaves differently on single simple feature geometry (sfg) objects, simple feature geometry column (sfc) and simple features objects. Let’s create a multipoint to illustrate how geometry casting works on simple feature geometry (sfg) objects: multipoint = st_multipoint(matrix(c(1, 3, 5, 1, 3, 1), ncol = 2)) In this case, st_cast can be useful to transform the new object into linestring or polygon (Figure 5.15): linestring = st_cast(multipoint, &quot;LINESTRING&quot;) polyg = st_cast(multipoint, &quot;POLYGON&quot;) #&gt; Found more than one class &quot;XY&quot; in cache; using the first, from namespace &#39;sf&#39; #&gt; Also defined by &#39;mapview&#39; #&gt; Found more than one class &quot;XY&quot; in cache; using the first, from namespace &#39;sf&#39; #&gt; Also defined by &#39;mapview&#39; #&gt; Found more than one class &quot;XY&quot; in cache; using the first, from namespace &#39;sf&#39; #&gt; Also defined by &#39;mapview&#39; Figure 5.15: Examples of linestring and polygon ‘casted’ from a multipoint geometry. This process can be also reversed using st_cast: multipoint_2 = st_cast(linestring, &quot;MULTIPOINT&quot;) multipoint_3 = st_cast(polyg, &quot;MULTIPOINT&quot;) all.equal(multipoint, multipoint_2, multipoint_3) #&gt; Found more than one class &quot;XY&quot; in cache; using the first, from namespace &#39;sf&#39; #&gt; Also defined by &#39;mapview&#39; #&gt; Found more than one class &quot;XY&quot; in cache; using the first, from namespace &#39;sf&#39; #&gt; Also defined by &#39;mapview&#39; #&gt; Found more than one class &quot;XY&quot; in cache; using the first, from namespace &#39;sf&#39; #&gt; Also defined by &#39;mapview&#39; #&gt; Found more than one class &quot;XY&quot; in cache; using the first, from namespace &#39;sf&#39; #&gt; Also defined by &#39;mapview&#39; #&gt; Found more than one class &quot;XY&quot; in cache; using the first, from namespace &#39;sf&#39; #&gt; Also defined by &#39;mapview&#39; #&gt; Found more than one class &quot;XY&quot; in cache; using the first, from namespace &#39;sf&#39; #&gt; Also defined by &#39;mapview&#39; #&gt; [1] TRUE For single simple feature geometries (sfg), st_cast also provides geometry casting from non-multi to multi types (e.g. POINT to MULTIPOINT) and from multi types to non-multi types. However, only the first element of the old object would remain in the second group of cases. Geometry casting of simple features geometry column (sfc) and simple features objects works the same as for single geometries in most of the cases. One important difference is conversion between multi to non-multi types. As a result of this process, multi-objects are split into many non-multi objects. We would use a new object, multilinestring_sf, as an example (on the left in Figure 5.16): multilinestring_list = list(matrix(c(1, 4, 5, 3), ncol = 2), matrix(c(4, 4, 4, 1), ncol = 2), matrix(c(2, 4, 2, 2), ncol = 2)) multilinestring = st_multilinestring((multilinestring_list)) multilinestring_sf = st_sf(geom = st_sfc(multilinestring)) multilinestring_sf #&gt; Simple feature collection with 1 feature and 0 fields #&gt; geometry type: MULTILINESTRING #&gt; dimension: XY #&gt; bbox: xmin: 1 ymin: 1 xmax: 4 ymax: 5 #&gt; epsg (SRID): NA #&gt; proj4string: NA #&gt; geom #&gt; 1 MULTILINESTRING ((1 5, 4 3)... You can imagine it as a road or river network. The new object has only one row that defines all the lines. This restricts the number of operations that can be done, for example it prevents adding names to each line segment or calculating lengths of single lines. The st_cast function can be used in this situation, as it separates one mutlilinestring into three linestrings: linestring_sf2 = st_cast(multilinestring_sf, &quot;LINESTRING&quot;) linestring_sf2 #&gt; Simple feature collection with 3 features and 0 fields #&gt; geometry type: LINESTRING #&gt; dimension: XY #&gt; bbox: xmin: 1 ymin: 1 xmax: 4 ymax: 5 #&gt; epsg (SRID): NA #&gt; proj4string: NA #&gt; geometry #&gt; 1 LINESTRING (1 5, 4 3) #&gt; 2 LINESTRING (4 4, 4 1) #&gt; 3 LINESTRING (2 2, 4 2) Figure 5.16: Examples of type casting between MULTILINESTRING (left) and LINESTRING (right). The newly created object allows for attributes creation (see more in section 3.2.4) and length measurements: linestring_sf2$name = c(&quot;Riddle Rd&quot;, &quot;Marshall Ave&quot;, &quot;Foulke St&quot;) linestring_sf2$length = st_length(linestring_sf2) linestring_sf2 #&gt; Simple feature collection with 3 features and 2 fields #&gt; geometry type: LINESTRING #&gt; dimension: XY #&gt; bbox: xmin: 1 ymin: 1 xmax: 4 ymax: 5 #&gt; epsg (SRID): NA #&gt; proj4string: NA #&gt; geometry name length #&gt; 1 LINESTRING (1 5, 4 3) Riddle Rd 3.61 #&gt; 2 LINESTRING (4 4, 4 1) Marshall Ave 3.00 #&gt; 3 LINESTRING (2 2, 4 2) Foulke St 2.00 5.4 Geometric operations on raster data Geometric raster operations include the shift, flipping, mirroring, scaling, rotation or warping of images. These operations are e.g. necessary when geolocating a raster image. In turn, geolocating requires the rectification of the image, which includes one or several of the following steps depending on the task at hand (see also Liu and Mason 2009): Georeferencing with ground control points. Orthorectification also georeferences an image but additionally takes into account local topography. Image (co-)registration is the process of aligning one image with another (in terms of CRS, origin and resolution). Registration becomes necessary for images from the same scene but shot from different sensors or from different angles or at different points in time. In this section we will first show how to change the extent, the resolution and the origin of an image. As mentioned before, most of the times, we need these operations in order to align several images. A matching projection is of course also required but is already covered in section 5.2.4. In any case, there are other reasons why to perform a geometric operation on a single raster image. For instance, in chapter 8 we define metropolitan areas in Germany as 20 km2 pixels with more than 500,000 inhabitants. The original inhabitant raster, however, has a resolution of 1 km2 which is why we will decrease (aggregate) the resolution by a factor of 20 (see section 8.5). 5.4.1 Extent and origin When merging or performing map algebra on rasters, their resolution, projection, origin and/or extent has to match. Otherwise, how should we add the values of one raster with a resolution of 0.2 decimal degrees to a second with a resolution of 1 decimal degree? The same problem arises when we would like to merge satellite imagery from different sensors with different projections and resolutions. We can deal with such mismatches by aligning the rasters. In the simplest case, two images only differ in a mismatch in extent. Following code adds one row and two columns to each side of the raster while setting all new values to an elevation of 1000 meters (5.17). data(elev, package = &quot;spData&quot;) elev_2 = extend(elev, c(1, 2), value = 1000) plot(elev_2) Figure 5.17: Original raster extended by 1 one row on each side (top, bottom) and two columns on each side (right, left). Performing an algebraic operation on two objects with differing extents in R, the raster package returns the result for the intersection, and says so in a warning. elev_3 = elev + elev_2 #&gt; Warning in elev + elev_2: Raster objects have different extents. Result for #&gt; their intersection is returned However, we can also align the extent of two rasters with extend(). Instead of telling the function how many rows or columns should be added (as done before), we allow it to figure it out by using another raster object. Here, we extend the elev object to the extent of elev_2. The newly added rows and column receive the default value of the value parameter, i.e. NA. elev_4 = extend(elev, elev_2) The origin is the point closest to (0, 0) if you moved towards it in steps of x and y resolution. origin(elev_4) #&gt; [1] 0 0 If two rasters have different origins, their cells do not overlap completely which would make map algebra impossible. To change the origin , use origin().33 Looking at figure 5.18 reveals the effect of changing the origin. # change the origin origin(elev_4) = c(0.25, 0.25) plot(elev_4) # and add the original raster plot(elev, add = TRUE) Figure 5.18: Plotting rasters with the same values but different origins. Note that changing the resolution frequently (next section) also changes the origin. 5.4.2 Aggregation and disaggregation Raster datasets can also differ with regard to their resolution. To match resolutions, one can either decrease (aggregate()) or increase (disaggregate()) the resolution of one raster.34 As an example, we here change the spatial resolution of elev from 0.5 to 2 decimal degree, that means, we aggregate by a factor of 2 (Fig. 5.19). Additionally, the output cell value should correspond to the mean of the input cells (note that one could use other functions as well, such as median(), sum() etc.): elev_agg = aggregate(elev, fact = 2, fun = mean) par(mfrow = c(1, 2)) plot(elev) plot(elev_agg) Figure 5.19: Original raster (left). Aggregated raster (right). By contrast, thedisaggregate() function increases the resolution. However, we have to specify a method how to fill the new cells. The disaggregate() function provides two methods. The first (nearest neighbor, method = &quot;&quot;) simply gives all output cells the value of the nearest input cell, and hence duplicates values which leads to a blocky output image. For example, the four cells building up the upper left cell of the aggregated raster (Fig. 5.19) will retrieve all the same value, namely 4.5. The bilinear method, in turn, is an interpolation technique that uses the four nearest pixel centers of the input image (salmon colored points in Fig. 5.20) to compute an average weighted by distance (arrows in Fig. 5.20 as the value of the output cell - square in the upper left corner in Fig. 5.20). elev_disagg = disaggregate(elev_agg, fact = 2, method = &quot;bilinear&quot;) all(values(elev) == values(elev_disagg)) #&gt; [1] TRUE Figure 5.20: The distance-weighted average of the four closest input cells determine the output when using the bilinear method for disaggregation. Comparing the values of elev and elev_disagg tells us that both are identical (you can also use compareRaster() or all.equal()). Please note that the disaggregation only predicted correctly the values at a higher resolution due to our artificial input data set (elev) and the fact that we have used the mean for the aggregation (elev_agg). However, this is usually not to be expected, since disaggregating is a simple interpolation technique. It is important to keep in mind that disaggregating results in a finer resolution, the corresponding values, however, are only as accurate as their lower resolution source. The process of computing values for new pixel locations is also called resampling. In fact, the raster package provides a resample() function. It lets you align several raster properties in one go, namely origin, extent and resolution. By default, it uses the bilinear-interpolation. # add 2 rows and columns, i.e. change the extent elev_agg = extend(elev_agg, 2) elev_disagg_2 = resample(elev_agg, elev) Finally, in order to align many (possibly hundreds or thousands of) images stored on disk, you could use the gdalUtils::align_rasters() function. However, you may also use raster with very large datasets. This is because raster: Lets you work with raster datasets that are too large to fit into the main memory (RAM) by only processing chunks of it. Tries to facilitate parallel processing. For more information see the help pages of beginCluster() and clusteR(). Additionally, check out the Multi-core functions section in vignette(&quot;functions&quot;, package = &quot;raster&quot;). 5.5 Exercises Create a new object called nz_wgs by transforming nz object into the WGS84 CRS. Create an object of class crs for both and use this to query their CRSs. With reference to the bounding box of each object, what units does each CRS use? Remove the CRS from nz_wgs and plot the result: what is wrong with this map of New Zealand and why? Transform the world dataset to the transverse Mercator projection (&quot;+proj=tmerc&quot;) and plot the result. What has changed and why? Try to transform it back into WGS 84 and plot the new object. Why does the new object differ from the original one? Generate and plot simplified versions of the nz dataset. Experiment with different values of keep (ranging from 0.5 to 0.00005) for ms_simplify() and dTolerance (from 100 to 100,000) st_simplify() . At what value does the form of the result start to break-down for each method, making New Zealand unrecognizable? Advanced: What is different about the geometry type of the results from st_simplify() compared with the geometry type of ms_simplify()? What problems does this create and how can this be resolved? In the first exercise in Chapter 4 it was established that Canterbury region had 61 of the 101 highest points in New Zealand. Using st_buffer(), how many points in nz_height are within 100 km of Canturbury? Find the geographic centroid of New Zealand. How far is it from the geographic centroid of Canterbury? Most world maps have a north-up orientation. A world map with a south-up orientation could be created by a reflection (one of the affine transformations not mentioned in 5.3.4) of the world object’s geometry. Write code to do so. Hint: you need to use a two-element vector for this transformation. Bonus: create a upside down map of your country. Transform the continuous raster (cat_raster) into WGS 84 using the nearest neighbor interpolation method. What has changed? How does it influence the results? Transform the categorical raster (cat_raster) into WGS 84 using the bilinear interpolation method. What has changed? How does it influence the results? Subset the point in p that is contained within x and y (see section 5.3.5 and Figure 5.12). Using base subsetting operators. Using an intermediary object created with st_intersection(). Calculate the length of the boundary lines of US states in meters. Which state has the longest border and which has the shortest? Hint: The st_length function computes the length of a LINESTRING or MULTILINESTRING geometry. Aggregate the raster counting high points in New Zealand (created in the previous exercise), reduce its geographic resolution by half (so cells are 6 by 6 km) and plot the result. Resample the lower resolution raster back to a resolution of 3 km. How have the results changed? Name two advantages and disadvantages of reducing raster resolution. References "],
["read-write.html", "6 Geographic data I/O Prerequisites 6.1 Introduction 6.2 Retrieving open data 6.3 File formats 6.4 Data Input (I) 6.5 Data output (O) 6.6 Visual outputs 6.7 Exercises", " 6 Geographic data I/O Prerequisites This chapter requires the following packages: library(sf) library(raster) library(tidyverse) library(spData) 6.1 Introduction This chapter is about reading and writing geographic data. Geographic data import is an essential part of geocomputational software because without data real-world applications are impossible. The skills taught in this book will enable you to add value to data meaning that, for others to benefit from the results, data output is also vital. These two processes go hand-in-hand and are referred to as I/O — short for input/output — in Computer Science (Gillespie and Lovelace 2016). Hence the title of this chapter. Geographic data I/O is almost always part of a wider process. It depends on knowing which datasets are available, where they can be found and how to retrieve them, topics covered in section 6.2. This section demonstrates how to access open access geoportals which collectively contain many terrabytes of data. There is a wide range of geographic file formats, each of which has pros and cons. These are described in section 6.3. The process of actually reading and writing such file formats efficiently is not covered until sections 6.4 and 6.5 respectively. The final section (6.6) demonstrates methods for saving visual outputs (maps), in preparation for a subsequent chapter dedicated to visualization. 6.2 Retrieving open data Nowadays, a vast amount of spatial data is available on the internet. Best of all, much of it is freely available. You just have to know where to look. While we cannot provide a comprehensive guide to all available geodata, we point to a few of the most important sources. Various ‘geoportals’ (web services providing geographic data such as the geospatial datasets in Data.gov) are a good place to start, providing a wide range of geographic data. Geoportals are a very useful data source but often only contain data for a specific locations (see the Wikipedia page on geoportals for a list of geoportals covering many areas of the world). To overcome this limitation some global geoportals have been developed. The GEOSS portal, for example, provides global remote sensing data. Additional geoportals with global coverage and an emphasis on raster data include the EarthExplorer and the Copernicus Open Access Hub. A wealth of European data is available from the INSPIRE geoportal. Typically, geoportals provide an interface that lets you query interactively the existing data (spatial and temporal extent, and product). However, in this book we encourage you to create reproducible workflows. In many cases data downloads can be scripted via download calls to static URLs or APIs (see the Sentinel API for example), saving time and enabling others to repeat and update the unglamorous data download process. Traditionally, files have been stored on servers. You can easily download such files with the download.file() command. For example, to download National Park Service units in the United States, run: url = file.path(&quot;http://www.naturalearthdata.com/http//www.naturalearthdata.com&quot;, &quot;download/10m/cultural/ne_10m_parks_and_protected_lands.zip&quot;) download.file(url = url, destfile = &quot;USA_parks.zip&quot;) unzip(zipfile = &quot;USA_parks.zip&quot;) usa_parks = st_read(&quot;ne_10m_parks_and_protected_lands_area.shp&quot;) Specific R packages providing an interface to spatial libraries or geoportals are even more user-friendly (Table 6.1). Table 6.1: Selected R packages for spatial data retrieval. Package name Description osmdata Download and import of OpenStreetMap data. raster The getData() function downloads and imports administrative country, SRTM/ASTER elevation, WorldClim data. rnaturalearth Functions to download Natural Earth vector and raster data, including world country borders. rnoaa An R interface to National Oceanic and Atmospheric Administration (NOAA) climate data. rWBclimate An access to the World Bank climate data. For example, you can get the borders of any country with the ne_countries() function from the rnaturalearth package: library(rnaturalearth) usa = ne_countries(country = &quot;United States of America&quot;) # United States borders class(usa) #&gt; [1] &quot;SpatialPolygonsDataFrame&quot; #&gt; attr(,&quot;package&quot;) #&gt; [1] &quot;sp&quot; # you can do the same with raster::getData() # getData(&quot;GADM&quot;, country = &quot;USA&quot;, level = 0) As a default, rnaturalearth returns the output as a Spatial* class. You can easily convert it to the sf class with st_as_sf(): usa_sf = st_as_sf(usa) As a second example, we will download a raster dataset. The code below downloads a series of rasters that contains global monthly precipitation sums. The spatial resolution is ten minutes. The result is a multilayer object of class RasterStack. library(raster) worldclim_prec = getData(name = &quot;worldclim&quot;, var = &quot;prec&quot;, res = 10) class(worldclim_prec) #&gt; [1] &quot;RasterStack&quot; #&gt; attr(,&quot;package&quot;) #&gt; [1] &quot;raster&quot; A third example uses the recently developed package osmdata (Padgham et al. 2018) to find parks from the OpenStreetMap (OSM) database. As illustrated in the code-chunk below, queries begin with the function opq() (short for OpenStreetMap query), the first argument of which is bounding box, or text string representing a bounding box (the city of Leeds in this case). The result is passed to a function for selecting which OSM elements we’re interested in (parks in this case), represented by key-value pairs, which in turn is passed to the function osmdata_sf() which does the work of downloading the data and converting it into a list of sf objects (see vignette('osmdata') for further details): library(osmdata) parks = opq(bbox = &quot;leeds uk&quot;) %&gt;% add_osm_feature(key = &quot;leisure&quot;, value = &quot;park&quot;) %&gt;% osmdata_sf() OpenStreetMap is a vast global database of crowd-sourced data and it is growing by the minute. Although the quality is not as spatially consistent as many official datasets, OSM data have many advantages: they are globally available free of charge and using crowd-source data can encourage ‘citizen science’ and contributions back to the digital commons. Further examples of osmdata in action are provided in Chapters 7 and 8. Finally, R packages might contain or just consist of spatial data, such as spData which provides example datasets used in this book. You can access such data with the data() function. For example, you can get a list of dataset in a package, data(package = &quot;spData&quot;). To attach the dataset to the global environment specify the name of a dataset (data(&quot;cycle_hire&quot;, package = &quot;spData&quot;)). Sometimes, packages come also with the original files.35 To load such a file from the package, you need to specify the package name and the relative path to the dataset, for example: world_raw_filepath = system.file(&quot;shapes/world.gpkg&quot;, package = &quot;spData&quot;) world_raw = st_read(world_raw_filepath) #&gt; Reading layer `wrld.gpkg&#39; from data source `/home/travis/R/Library/spData/shapes/world.gpkg&#39; using driver `GPKG&#39; #&gt; Simple feature collection with 177 features and 10 fields #&gt; geometry type: MULTIPOLYGON #&gt; dimension: XY #&gt; bbox: xmin: -180 ymin: -90 xmax: 180 ymax: 83.6 #&gt; epsg (SRID): 4326 #&gt; proj4string: +proj=longlat +datum=WGS84 +no_defs Find more information on getting data using R packages in section 5.5 and section 5.6 of Gillespie and Lovelace (2016). 6.3 File formats Spatial datasets are usually stored as files or in spatial databases. File formats can either store vector or raster data, while spatial databases such as PostGIS can store both. Today file formats may seem bewildering but there has been much consolidation and standardisation since the beginnings of GIS software in the 1960s when the first widely distributed program (SYMAP) for spatial analysis was created at Harvard University (Coppock and Rhind 1991). GDAL,36 the Geospatial Data Abstraction Library, has resolved many issues associated with incompatibility between file formats since its release in 2000. GDAL provides a unified and high-performance interface for reading and writing of many raster and vector data formats. Many open and proprietary GIS programs, including GRASS, ArcGIS and QGIS, use GDAL behind their GUIs for doing the legwork of ingesting and spitting-out geographic data in appropriate formats. An important development ensuring greater standardisation and open-sourcing of file formats was the founding of the Open Geospatial Consortium (OGC) in 1994.37 The OGC coordinates the development of open standards for geospatial content including file formats such as KML and GeoPackage. As described in Chapter 2 the OGC publishes the simple feature data model, which underlies the vector data classes provided by sf and used in this book. Open file formats of the kind endorsed by the OGC have several advantages over proprietary formats: the standards are published, ensuring transparency and enabling innovation to improve the file formats. There are more than 100 spatial data formats exist available to R users via GDAL. Table 6.2 presents some basic information about selected and often used spatial file formats. Table 6.2: Selected spatial file formats. Name Extension Info Type Model ESRI Shapefile .shp (the main file) One of the most popular vector file formats. Consists of at least three files. The main files size cannot exceed 2 GB. It lacks support for mixed type. Column names are limited to 10 characters, and number of columns are limited at 255. It has poor support for Unicode standard. Vector Partially open GeoJSON .geojson Extends the JSON exchange format by including a subset of the simple feature representation. Vector Open KML .kml XML-based format for spatial visualization, developed for use with Google Earth. Zipped KML file forms the KMZ format. Vector Open GPX .gpx XML schema created for exchange of GPS data. Vector Open GeoTIFF .tiff GeoTIFF is one of the most popular raster formats. Its structure is similar to the regular .tif format, however, additionally stores the raster header. Raster Open Arc ASCII .asc Text format where the first six lines represent the raster header, followed by the raster cell values arranged in rows and columns. Raster Open R-raster .gri, .grd Native raster format of the R-package raster. Raster Open SQLite/SpatiaLite .sqlite SQLite is a standalone, relational database management system. It is used as a default database driver in GRASS GIS 7. SpatiaLite is the spatial extension of SQLite providing support for simple features. Vector and raster Open ESRI FileGDB .gdb Collection of spatial and nonspatial objects created in the ArcGIS software. It allows storage of multiple feature classes and enables use of topological definitions. Limited access to this format is provided by GDAL with the use of the OpenFileGDB and FileGDB drivers. Vector and raster Proprietary GeoPackage .gpkg Lightweight database container based on SQLite allowing an easy and platform-independent exchange of geodata Vector and raster Open 6.4 Data Input (I) Executing commands such as sf::st_read() (the main function we use for loading vector data) or raster::raster() (the main function used for loading raster data) silently sets off a chain of events that reads data from files. Moreover, there are many R packages containing a wide range of spatial data or providing simple access to different data sources. All of them load the data into R or, more precisely, assign objects to your workspace, stored in RAM accessible from the .GlobalEnv38 of your current R session. 6.4.1 Vector data Spatial vector data comes in a wide variety of file formats, most of which can be read-in via the sf function st_read(). Behind the scenes this calls GDAL. To find out which data formats sf supports, run st_drivers(). Here, we show only the first five drivers (see Table 6.3): sf_drivers = st_drivers() head(sf_drivers, n = 5) Table 6.3: Sample of available drivers for reading/writing vector data (it could vary between different GDAL versions). name long_name write copy is_raster is_vector vsi ESRI Shapefile ESRI Shapefile TRUE FALSE FALSE TRUE TRUE GPX GPX TRUE FALSE FALSE TRUE TRUE KML Keyhole Markup Language (KML) TRUE FALSE FALSE TRUE TRUE GeoJSON GeoJSON TRUE FALSE FALSE TRUE TRUE GPKG GeoPackage TRUE TRUE TRUE TRUE TRUE The first argument of st_read() is dsn, which should be a text string or an object containing a single text string. The content of a text string could vary between different drivers. In most cases, as with the ESRI Shapefile (.shp) or the GeoPackage format (.gpkg), the dsn would be a file name. st_read() guesses the driver based on the file extension, as illustrated for a .gpkg file below: vector_filepath = system.file(&quot;shapes/world.gpkg&quot;, package = &quot;spData&quot;) world = st_read(vector_filepath) #&gt; Reading layer `wrld.gpkg&#39; from data source `/home/travis/R/Library/spData/shapes/world.gpkg&#39; using driver `GPKG&#39; #&gt; Simple feature collection with 177 features and 10 fields #&gt; geometry type: MULTIPOLYGON #&gt; dimension: XY #&gt; bbox: xmin: -180 ymin: -90 xmax: 180 ymax: 83.6 #&gt; epsg (SRID): 4326 #&gt; proj4string: +proj=longlat +datum=WGS84 +no_defs For some drivers, dsn could be provided as a folder name, access credentials for a database, or a GeoJSON string representation (see the examples of the st_read() help page for more details). Some vector driver formats can store multiple data layers. By default, st_read automatically reads the first layer of the file specified in dsn, however, using the layer argument you can specify any other layer. Naturally, some options are specific to certain drivers.39 For example, think of coordinates stored in a spreadsheet format (.csv). To read in such files as spatial objects, we naturally have to specify the names of the columns (X and Y in our example below) representing the coordinates. We can do this with the help of the options parameter. To find out about possible options, please refer to the ‘Open Options’ section of the corresponding GDAL driver description. For the comma-separated value (csv) format, visit http://www.gdal.org/drv_csv.html. cycle_hire_txt = system.file(&quot;misc/cycle_hire_xy.csv&quot;, package = &quot;spData&quot;) cycle_hire_xy = st_read(cycle_hire_txt, options = c(&quot;X_POSSIBLE_NAMES=X&quot;, &quot;Y_POSSIBLE_NAMES=Y&quot;)) Instead of columns describing xy-coordinates, a single column can also contain the geometry information. Well-known text (WKT), well-known binary (WKB), and the GeoJSON formats are examples of this. For instance, the world_wkt.csv file has a column named WKT representing polygons of the world’s countries. We will again use the options parameter to indicate this. Here, we will use read_sf() which does exactly the same as st_read() except it does not print the driver name to the console and stores strings as characters instead of factors. world_txt = system.file(&quot;misc/world_wkt.csv&quot;, package = &quot;spData&quot;) world_wkt = read_sf(world_txt, options = &quot;GEOM_POSSIBLE_NAMES=WKT&quot;) # the same as world_wkt = st_read(world_txt, options = &quot;GEOM_POSSIBLE_NAMES=WKT&quot;, quiet = TRUE, stringsAsFactors = FALSE) Not all of the supported vector file formats store information about their coordinate reference system. In these situations, it is possible to add the missing information using the st_set_crs() function. Please refer also to section 2.3 for more information. As a final example, we will show how st_read() also reads KML files. A KML file stores geographic information in XML format - a data format for the creation of web pages and the transfer of data in an application-independent way (Nolan and Lang 2014). Here, we access a KML file from the web. This file contains more than one layer. st_layers() lists all available layers. We choose the first layer Placemarks and say so with the help of the layer parameter in read_sf(). url = &quot;https://developers.google.com/kml/documentation/KML_Samples.kml&quot; st_layers(url) #&gt; Driver: LIBKML #&gt; Available layers: #&gt; layer_name geometry_type features fields #&gt; 1 Placemarks 3 11 #&gt; 2 Styles and Markup 1 11 #&gt; 3 Highlighted Icon 1 11 #&gt; 4 Ground Overlays 1 11 #&gt; 5 Screen Overlays 0 11 #&gt; 6 Paths 6 11 #&gt; 7 Polygons 0 11 #&gt; 8 Google Campus 4 11 #&gt; 9 Extruded Polygon 1 11 #&gt; 10 Absolute and Relative 4 11 kml = read_sf(url, layer = &quot;Placemarks&quot;) 6.4.2 Raster data Similar to vector data, raster data comes in many file formats with some of them supporting even multilayer files. raster’s raster() command reads in a single layer. raster_filepath = system.file(&quot;raster/srtm.tif&quot;, package = &quot;spDataLarge&quot;) single_layer = raster(raster_filepath) In case you want to read in a single band from a multilayer file use the band parameter to indicate a specific layer. multilayer_filepath = system.file(&quot;raster/landsat.tif&quot;, package = &quot;spDataLarge&quot;) band3 = raster(multilayer_filepath, band = 3) If you want to read in all bands, use brick() or stack(). multilayer_brick = brick(multilayer_filepath) multilayer_stack = stack(multilayer_filepath) Please refer to section 2.2.3 for information on the difference between raster stacks and bricks. 6.5 Data output (O) Writing spatial data allows you to convert from one format to another and to save newly created objects. Depending on the data type (vector or raster), object class (e.g multipoint or RasterLayer), and type and amount of stored information (e.g. object size, range of values) - it is important to know how to store spatial files in the most efficient way. The next two sections will demonstrate how to do this. 6.5.1 Vector data The counterpart of st_read() is st_write(). It allows you to write sf objects to a wide range of geographic vector file formats, including the most common such as .geojson, .shp and .gpkg. Based on the file name, st_write() decides automatically which driver to use. The speed of the writing process depends also on the driver. st_write(obj = world, dsn = &quot;world.gpkg&quot;) #&gt; Writing layer `world&#39; to data source `world.gpkg&#39; using driver `GPKG&#39; #&gt; features: 177 #&gt; fields: 10 #&gt; geometry type: Multi Polygon Note: if you try to write to the same data source again, the function will fail: st_write(obj = world, dsn = &quot;world.gpkg&quot;) #&gt; Updating layer `world&#39; to data source `/home/travis/build/Robinlovelace/geocompr/world.gpkg&#39; using driver `GPKG&#39; #&gt; Warning in CPL_write_ogr(obj, dsn, layer, driver, as.character(dataset_options), : GDAL Error 1: Layer world already exists, CreateLayer failed. #&gt; Use the layer creation option OVERWRITE=YES to replace it. #&gt; Creating layer world failed. #&gt; Error in CPL_write_ogr(obj, dsn, layer, driver, as.character(dataset_options), : Layer creation failed. The error message provides some information as to why the function failed. The GDAL Error 1 statement makes clear that the failure occurred at the GDAL level. Additionally, the suggestion to use OVERWRITE=YES provides a clue about how to fix the problem. However, this is not a st_write() argument, it is a GDAL option. Luckily, st_write provides a layer_options argument through which we can pass driver-dependent options: st_write(obj = world, dsn = &quot;world.gpkg&quot;, layer_options = &quot;OVERWRITE=YES&quot;) Another solution is to use the st_write() argument delete_layer. Setting it to TRUE deletes already existing layers in the data source before the function attempts to write (note there is also a delete_dsn argument): st_write(obj = world, dsn = &quot;world.gpkg&quot;, delete_layer = TRUE) You can achieve the same with write_sf() since it is equivalent to (technically an alias for) st_write(), except that its defaults for delete_layer and quiet is TRUE. write_sf(obj = world, dsn = &quot;world.gpkg&quot;) The layer_options argument could be also used for many different purposes. One of them is to write spatial data to a text file. This can be done by specifying GEOMETRY inside of layer_options. It could be either AS_XY for simple point datasets (it creates two new columns for coordinates) or AS_WKT for more complex spatial data (one new column is created which contains the well-known-text representation of spatial objects). st_write(cycle_hire_xy, &quot;cycle_hire_xy.csv&quot;, layer_options = &quot;GEOMETRY=AS_XY&quot;) st_write(world_wkt, &quot;world_wkt.csv&quot;, layer_options = &quot;GEOMETRY=AS_WKT&quot;) 6.5.2 Raster data The writeRaster() function saves Raster* objects to files on disk. The function expects input regarding output datatype and file format, but also accepts GDAL options specific to a selected file format (see ?writeRaster for more details). The raster package offers nine datatypes when saving a raster: LOG1S, INT1S, INT1U, INT2S, INT2U, INT4S, INT4U, FLT4S, and FLT8S.40 The datatype determines the bit representation of the raster object written to disk (6.4). Which datatype to use depends on the range of the values of your raster object. The more values a datatype can represent, the larger the file will get on disk. Commonly, one would use LOG1S for bitmap (binary) rasters. Unsigned integers (INT1U, INT2U, INT4U) are suitable for categorical data, while float numbers (FLT4S and FLTS8S) usually represent continuous data. writeRaster() uses FLT4S as the default. While this works in most cases, the size of the output file will be unnecessarly large if you save binary or categorical data. Therefore, we would recommend to use the datatype that needs the least storage space but is still able to represent all values (check the range of values with the summary() function). Table 6.4: Datatypes supported by the raster package. Datatype Minimum value Maximum value LOG1S FALSE (0) TRUE (1) INT1S -127 127 INT1U 0 255 INT2S -32,767 32,767 INT2U 0 65,534 INT4S -2,147,483,647 2,147,483,647 INT4U 0 4,294,967,296 FLT4S -3.4e+38 3.4e+38 FLT8S -1.7e+308 1.7e+308 The file extension determines the output file when saving a Raster* object to disk. For example, the .tif extension will create a GeoTIFF file: writeRaster(x = single_layer, filename = &quot;my_raster.tif&quot;, datatype = &quot;INT2U&quot;) The raster file format (native to the raster package) is used when a file extension is invalid or missing. Some raster file formats come with additional options. You can use them with the options parameter.41 For example, GeoTIFF allows you to compress the output raster with the COMPRESS option42: writeRaster(x = single_layer, filename = &quot;my_raster.tif&quot;, datatype = &quot;INT2U&quot;, options = c(&quot;COMPRESS=DEFLATE&quot;), overwrite = TRUE) Note that writeFormats() returns a list with all supported file formats on your computer. 6.6 Visual outputs R supports many different static and interactive graphics formats. The most general method to save a static plot is to open a graphic device, create a plot, and close it, for example: png(filename = &quot;lifeExp.png&quot;, width = 500, height = 350) plot(world[&quot;lifeExp&quot;]) dev.off() Other available graphic devices include pdf(), bmp(), jpeg(), png(), and tiff(). You can specify several properties of the output plot, including width, height and resolution. Additionally, several graphic packages provide thier own functions to save a graphical output. For example, the tmap package has the tmap_save() function. You can save a tmap object to different graphic formats by specifying the object name and a file path to a new graphic file. library(tmap) tmap_obj = tm_shape(world) + tm_polygons(col = &quot;lifeExp&quot;) tmap_save(tm = tmap_obj, filename = &quot;lifeExp_tmap.png&quot;) On the other hand, you can save interactive maps created in the mapview package as an HTML file or image using the mapshot() function: library(mapview) mapview_obj = mapview(world, zcol = &quot;lifeExp&quot;, legend = TRUE) mapshot(mapview_obj, file = &quot;my_interactive_map.html&quot;) 6.7 Exercises List and describe three types of vector, raster, and geodatabase formats. Name at least two differences between read_sf() and the more well-known function st_read(). Read the cycle_hire_xy.csv file from the spData package (Hint: it is located in the misc\\ folder). What is a geometry type of the loaded object? Download the borders of Germany using rnaturalearth, and create a new object called germany_borders. Write this new object to a file of the GeoPackage format. Download the global monthly minimum temperature with a spatial resolution of five minutes using the raster package. Extract the June values, and save them to a file named tmin_june.tif file (hint: use raster::subset()). Create a static map of Germany’s borders, and save it to a PNG file. Create an interactive map using data from the cycle_hire_xy.csv file. Export this map to a file called cycle_hire.html. References "],
["transport.html", "7 Transport applications Prerequisites 7.1 Introduction 7.2 A case study of Bristol 7.3 Transport zones 7.4 Desire lines 7.5 Routes 7.6 Nodes 7.7 Route networks 7.8 Prioritizing new infrastructure 7.9 Future directions of travel 7.10 Exercises", " 7 Transport applications Prerequisites This chapter requires the following packages to be loaded (it also uses osmdata and nabor although these do not need to be loaded): library(sf) library(tidyverse) library(stplanr) library(spDataLarge) 7.1 Introduction In few other sectors is geographic space more tangible than transport. The effort of moving (overcoming distance) is central to the ‘first law’ of geography, defined by Waldo Tobler in 1970 as follows (Miller 2004): Everything is related to everything else, but near things are more related than distant things This ‘law’ is the basis for spatial autocorrelation and other key geographic concepts. It applies to phenomena as diverse as friendship networks and ecological diversity and can be explained by the costs of transport — in terms of time, energy and money — which constitute the ‘friction of distance’. From this perspective transport technologies are disruptive, changing geographic relationships between geographic entities including mobile humans and goods: “the purpose of transportation is to overcome space” (Rodrigue, Comtois, and Slack 2013). Transport is an inherently geospatial activity. It involves traversing continuous geographic space between A and B, and infinite localities in between. It is therefore unsurprising that transport researchers have long turned to geocomputational methods to understand movement patterns and that transport problems are a motivator of geocomputational methods. This chapter provides an introduction to geographic analysis of transport systems. We will explore how movement patterns can be understood at multiple geographic levels, including: Areal units: transport patterns can be understood with reference to zonal aggregates such as the main mode of travel (by car, bike or foot, for example) and average distance of trips made by people living in a particular zone. Desire lines: straight lines that represent ‘origin-destination’ data that records how many people travel (or could travel) between places (points or zones) in geographic space. Routes: these are circuitous (non-straight) routes, typically representing the ‘optimal’ path along the route network between origins and destinations along the desire lines defined in the previous bullet point. Nodes: these are points in the transport system that can represent common origins and destinations (e.g. with one centroid per zone) and public transport stations such as bus stops and rail stations. Route networks: these represent the system of roads, paths and other linear features in an area. They can be represented as geographic features (representing route segments) or structured as an interconnected graph. Each can be assigned values representing the level of traffic on different parts of the network, referred to as ‘flow’ by transport modelers (Hollander 2016). Another key level is agents, mobile entities like you and me. These can be represented computationally thanks to software such as MATSim, which captures the dynamics of transport systems using an agent-based modelling (ABM) approach at high spatial and temporal resolution (Horni, Nagel, and Axhausen 2016). ABM is a powerful approach to transport research with great potential for integration with R’s spatial classes (Thiele 2014; Lovelace and Dumont 2016), but is outside the scope of this chapter. Beyond geographic levels and agents, the basic unit of analysis in most transport models is the trip, a single purpose journey from an origin ‘A’ to a destination ‘B’ (Hollander 2016). Trips join-up the different levels of transport systems: they are usually represented as desire lines connecting zone centroids (nodes), they can be allocated onto the route network as routes, and are made by people who can be represented as agents. Another complication is time. Although many trips are regular and predictable — such as the daily commute to work — transport systems are dynamic and constantly evolving over time. The purpose of geographic transport modeling can be interpreted as simplifying this complexity in a way that captures the essence of transport problems. Selecting an appropriate level of geographic analysis can help simplify this complexity, to capture the essence of a transport system without losing its most important features and variables (Hollander 2016). Typically, models are designed to solve a particular problem. For this reason, this chapter is based around a policy scenario that asks: how to increase walking and cycling in the city of Bristol? Both policies aim to prevent traffic jams, reduce carbon emissions, and promote a healthier life style, all of which makes the city greener and thus more attractive and enjoyable to live in. 7.2 A case study of Bristol The case study used for this chapter is located in Bristol, a city in the west of England, around 30 km east of the Welsh capital Cardiff. An overview of the region’s transport network is illustrated in Figure 7.1, which shows a diversity of transport infrastructure, for cycling, public transport, and private motor vehicles. Figure 7.1: Bristol’s transport network represented by colored lines for active (green), public (railways, black) and private motor (red) modes of travel. Blue border lines represent the inner city boundary and the larger Travel To Work Area (TTWA). Bristol is the 10th largest city council in England, with a population of half a million people, although its travel catchment area is larger (see section 7.3). It has a vibrant economy with aerospace, media, financial service and tourism companies, alongside two major universities. Bristol shows a high average income per capita but also contains areas of severe deprivation (Bristol City Council 2015). In terms of transport, Bristol is well served by rail and road links, and has a relatively high level of active travel. 19% of its citizens cycle and 88% walk at least once per month according to the Active People Survey (the national average is 15% and 81%, respectively). 8% of the population reported to cycle to work in the 2011 census, compared with only 3% nationwide. Despite impressive walking and cycling statistics, the city has a major congestion problem. Part of the solution is to continue to increase the proportion of trips made by cycling. Cycling has a greater potential to replace car trips than walking because of the speed of this mode, around 3-4 times faster than walking (with typical speeds of 15-20 km/h vs 4-6 km/h for walking). There is an ambitious plan to double the share of cycling by 2020. In this policy context the aim of this chapter, beyond demonstrating how geocomputation with R can be used to support sustainable transport planning, is to provide evidence for decision-makers in Bristol to decide how best to increase the share of walking and cycling in particular in the city. This high-level aim will be met via the following objectives: Describe the geographical pattern of transport behavior in the city. Identify key public transport nodes and routes along which cycling to rail stations could be encouraged, as the first stage in multi-model trips. Analyze travel ‘desire lines’ in the city to identify those with greatest potential for modal shift. Building on the desire-line level analysis, identify which routes would most benefit from having dedicated cycleways and improved provision for pedestrians. To get the wheels rolling on the practical aspects of this chapter, we begin by loading zonal data on travel patterns. These zone-level data are small but often vital for gaining a basic understanding of a settlement’s overall transport system. 7.3 Transport zones Although transport systems are primarily based on linear features and nodes — including pathways and stations — it often makes sense to start with areal data, to break continuous space into tangible units (Hollander 2016). Two zone types will typically be of particular interest: the study region and origin (typically residential areas) and destination (typically containing ‘trip attractors’ such as schools and shops) zones. Often the geographic units of destinations are the geographic units that comprise the origins, but a different zoning system, such as ‘Workplace Zones’, may be appropriate to represent the increased density of trip destinations in central areas (Office for National Statistics 2014). The simplest way to define a study area is often the first matching boundary returned by OpenStreetMap, which can be obtained using osmdata as follows: # requires the development version of osmdata # devtools::install_github(&quot;ropensci/osmdata&quot;) bristol_region = osmdata::getbb(&quot;Bristol&quot;, format_out = &quot;sf_polygon&quot;) The result is an sf object representing the bounds of the largest matching city region, either a rectangular polygon of the bounding box or a detailed polygonal boundary.43 In this case the command returns a detailed polygon representing the official boundary of Bristol (see the inner blue boundary in Figure 7.1). There are a couple of issues associated with using OSM definitions of regions, however. First, the first OSM boundary returned by OSM may not be the official boundary used by local authorities. Second, even if OSM returns the official boundary, this may be inappropriate for transport research because they bear little relation to where people travel. Travel to Work Areas (TTWAs) overcome the second issue by creating a zoning system analogous to hydrological watersheds. TTWAs were first defined as contiguous zones within which 75% of the population travels to work (Coombes, Green, and Openshaw 1986), and this is the definition used in this chapter. Because Bristol is a major employer attracting travel from surrounding towns, its TTWA is substantially larger than the city bounds (see Figure 7.1). The polygon representing this transport-orientated boundary is stored in the object bristol_ttwa, provided by the spDataLarge package loaded at the beginning of this chapter. The origin and destination zones used in this chapter are the same: officially defined zones of intermediate geographic resolution (their official name is Middle layer Super Output Areas or MSOAs). Each house around 8,000 people. Such administrative zones can provide vital context to transport analysis, such as the type of people who might benefit most from particular interventions (e.g. Moreno-Monroy, Lovelace, and Ramos 2017). The geographic resolution of these zones is important: small zones with high geographic resolution are usually preferable but their high number in large regions can have consequences for processing (especially for origin-destination analysis in which the number of possibilities increases as a non-linear function of the number of zones) (Hollander 2016). Another issue with small zones is related to anonymity rules. To make it impossible to infer the identity of individuals in zones, detailed socio-demographic variables are often only available at low geographic resolution. Breakdowns of travel mode by age and sex, for example, are available at the Local Authority level in the UK, but not at the much higher Output Area level, each of which contains around 100 households — see ons.gov.uk for further details. The 102 zones used are illustrated in Figure 7.2. These are stored in the bristol_zones object from the spDataLarge package, which divides-up space into sensible parcels, with each zone housing a similar number of people. However at present the dataset contains no attribute data, other than the zone name and zone code: names(bristol_zones) #&gt; [1] &quot;geo_code&quot; &quot;name&quot; &quot;geometry&quot; Chapter 3 demonstrated how to join attribute data onto geographic variables. This is a common task which usually involves joining a table containing official zonal statistics to a geographic object via a shared key variable, as described in section 3.2.3. In this case we use data provided by ons.gov.uk, which maintains datasets at various geographic levels across the UK. Instead of accessing an area-level travel dataset directly, we will create zonal data by aggregating an object (bristol_od from spDataLarge) representing origin-destination (OD) data by zone of origin (exactly what this means and the nature of this OD dataset is described in more detail in section 7.4). This is demonstrated below: zones_attr = group_by(bristol_od, o) %&gt;% summarize_if(is.numeric, sum) %&gt;% rename(geo_code = o) What just happened? The code first read-in data to create bristol_od, a data frame object representing travel to work between zones from the UK’s 2011 Census in which the first column is the ID of the zone of origin and the second column is the zone of destination (bristol_od is described more fully in the next section). Then the chained operation performed three main steps: Grouped the data by zone of origin (contained in the column o). Aggregated the variables in the bristol_od dataset if they were numeric, to find the total number of people living in each zone by mode of transport.44 Renamed the grouping variable o so it matches the ID column geo_code in the bristol_zones object. The resulting object zones_attr is a data frame with rows representing zones and an ID variable. We can verify that the IDs match those in the zones dataset using %in% operator as follows: summary(zones_attr$geo_code %in% bristol_zones$geo_code) #&gt; Mode TRUE #&gt; logical 102 The results show that all 102 zones are present in the new object and that zone_attr is in a form that can be joined onto the zones.45 This is done using the joining function left_join() (note that inner_join() would produce here the same result): zones_joined = left_join(bristol_zones, zones_attr) #&gt; Joining, by = &quot;geo_code&quot; sum(zones_joined$all) #&gt; [1] 238805 names(zones_joined) #&gt; [1] &quot;geo_code&quot; &quot;name&quot; &quot;all&quot; &quot;bicycle&quot; &quot;foot&quot; #&gt; [6] &quot;car_driver&quot; &quot;train&quot; &quot;geometry&quot; The result is zones_joined, which contains new columns representing the total number of trips originating in each zone in the study area (almost 1/4 of a million) and their mode of travel (by bicycle, foot, car and train). The geographic distribution of trip origins is illustrated in the left-hand map in Figure 7.2. This shows that most zones have between 0 and 4,000 trips originating from them in the study area. More trips are made by people living near the center of Bristol and fewer on the outskirts. Why is this? Remember that we are only dealing with trips within the study region: low trip numbers in the outskirts of the region can be explained by the fact that many people in these peripheral zones will travel to other regions outside of the study area. Trips outside the study region can be included in regional model by a special destination ID covering any trips that go to a zone not represented in the model (Hollander 2016). The data in bristol_od, however, simply ignores such trips: it is an ‘intra-zonal’ model. In the same way that OD datasets can be aggregated to the zone of origin, they can also be aggregated to provide information about destination zones. People tend to gravitate towards central places. This explains why the spatial distribution represented in the right panel in Figure 7.2 is relatively uneven, with the most common destination zones concentrated in Bristol city center. The result is zones_od, which contains a new column reporting the number of trip destinations by any mode, is created as follows: zones_od = group_by(bristol_od, d) %&gt;% summarize_if(is.numeric, sum) %&gt;% dplyr::select(geo_code = d, all_dest = all) %&gt;% inner_join(zones_joined, ., by = &quot;geo_code&quot;) Figure 7.2: Number of trips (commuters) living and working in the region. The left map shows zone of origin of commute trips; the right map shows zone of destination (generated by the script 07-zones.R). 7.4 Desire lines Unlike zones, which represent trip origins and destinations, desire lines connect the centroid of the origin and the destination zone, and thereby represent where people desire to go between zones. If it were not for obstacles such as buildings and windy roads, people would travel in a ‘bee-line’ from one place to the next, explaining why desire lines are straight (we will see how to convert desire lines into routes in the next section). We have already loaded data representing desire lines in the dataset bristol_od. This origin-destination (OD) data frame object represents the number of people traveling between the zone represented in o and d, as illustrated in Table 7.1. To arrange the OD data by all trips and then filter-out only the top 5, type (please refer to Chapter 3 for a detailed description of non-spatial attribute operations): od_top5 = arrange(bristol_od, desc(all)) %&gt;% top_n(5, wt = all) Table 7.1: Sample of the origin-destination data stored in the data frame object bristol_od. These represent the top 5 most common desire lines between zones in the study area. o d all bicycle foot car_driver train E02003043 E02003043 1493 66 1296 64 8 E02003047 E02003043 1300 287 751 148 8 E02003031 E02003043 1221 305 600 176 7 E02003037 E02003043 1186 88 908 110 3 E02003034 E02003043 1177 281 711 100 7 The resulting table provides a snapshot of Bristolian travel patterns in terms of commuting (travel to work). It demonstrates that walking is the most popular mode of transport among the top 5 origin-destination pairs, that zone E02003043 is a popular destination (Bristol city center, the destination of all the top 5 OD pairs), and that the intrazonal trips, from one part of zone E02003043 to another (first row of table 7.1), constitute the most traveled OD pair in the dataset. But from a policy perspective 7.1 is of limited use: aside from the fact that it contains only a tiny portion of the 2,910 OD pairs, it tells us little about where policy measures are needed. What is needed is a way to plot this origin-destination data on the map. The solution is to convert the non-geographic bristol_od dataset into geographical desire lines that can be plotted on a map. The geographic representation of the interzonal OD pairs (in which the destination is different from the origin) presented in Table 7.1 are displayed as straight black lines in 7.3. These are clearly more useful from a policy perspective. The conversion from data.frame to sf class is done by the stplanr function od2line(), which matches the IDs in the first two columns of the bristol_od object to the zone_code ID column in the geographic zones_od object.46 od_intra = filter(bristol_od, o == d) od_inter = filter(bristol_od, o != d) desire_lines = od2line(od_inter, zones_od) #&gt; Warning in st_centroid.sfc(st_geometry(x), of_largest_polygon = #&gt; of_largest_polygon): st_centroid does not give correct centroids for #&gt; longitude/latitude data The first two lines of the preceding code chunk split the bristol_od dataset into two mutually exclusive objects, od_intra (which only contains OD pairs representing intrazone trips) and od_inter (which represents interzonal travel). The third line generates a geographic object desire_lines (of class sf) that allows a subsequent geographic visualization of interzone trips. An illustration of the results is presented in Figure 7.3 (we will cover the visualization methods that produced this plot in Chapter 9). The map shows that the city center dominates transport patterns in the region, suggesting policies should be prioritized there, although a number of peripheral sub-centers can also be seen. Next it would be interesting to have a look at the distribution of interzonal modes, e.g. between which zones is cycling the least or the most common means of transport. plot(desire_lines$geometry, lwd = desire_lines$all / 500) Figure 7.3: Desire lines representing trip patterns in the Bristol Travel to Work Area. The four black lines represent the object the top 5 desire lines illustrated in Table 7.1. 7.5 Routes From a geographical perspective routes are desire lines that are no longer straight: the origin and destination points are the same, but the pathway to get from A to B is more complex. Desire lines contain only two vertices (their beginning and end points) but routes can contain hundreds of vertices if they cover a large distance or represent travel patterns on an intricate road network (routes on simple grid-based road networks require relatively few vertices). Routes are generated from desire lines — or more commonly origin-destination pairs — using routing algorithms. Routing algorithms can either run locally or remotely. Routing is computationally intensive and might be slow on a local machine (depending on the hardware). In any case, local routing requires the route network to be stored on a local computer (we will see how in section 7.7). By contrast, remote routing services use a web API to receive queries about origin and destination points, run the routing on the route network on a powerful server and return just the results, the routes, to the user, the so-called client. There are many advantages of using remote routing services, including the fact that they can be up-to-date, have global coverage, and run on a set-up that was designed for the job, explaining this section’s focus on online routing services. Before proceeding, there are couple of important disadvantages of online routing services to consider: they can be slow (because they rely on data transfer over the internet) and expensive. The Google routing API, for example, has a limit of 2500 free queries per day.47 A popular and free routing service is the Open Source Routing Machine (OSRM).48 Instead of routing all desire lines generated in the previous section, which would be time and memory-consuming, we will focus on the desire lines of policy interest. The benefits of cycling trips are greatest when they replace car trips. Clearly not all car trips can realistically be replaced by cycling, but 5 km Euclidean distance (or around 6-8 km of route distance) is easily accessible within 30 minutes for most people. Based on this reasoning we will only route desire lines along which a high (300+) number of car trips take place that are up to 5 km in distance. This routing is done by the stplanr function line2route() which takes straight lines in Spatial or sf objects, and returns ‘bendy’ lines representing routes on the transport network in the same class as the input. desire_lines$distance = as.numeric(st_length(desire_lines)) desire_carshort = dplyr::filter(desire_lines, car_driver &gt; 300 &amp; distance &lt; 5000) route_carshort = line2route(desire_carshort, route_fun = route_osrm) st_length() determines the length of a linestring, and falls into the distance relations category (see also section 4.2.6). Subsequently, we apply a simple attribute filter operation (see section 3.2.1) before letting the OSRM service do the routing on a remote server. Note that the routing only works with a working internet connection. We could keep the new route_carshort object separate from the straight line representation of the same trip in desire_carshort but, from a data management perspective, it makes more sense to combine them: they represent the same trip. The new route dataset contains distance (referring to route distance this time) and duration fields (in seconds) which could be useful. However, for the purposes of this chapter we are only interested in the geometry, from which route distance can be calculated. The following command makes use of the ability of simple features objects to contain multiple geographic columns: desire_carshort$geom_car = route_carshort$geometry This allows plotting the desire lines along which many short car journeys take place alongside likely routes traveled by cars, with the width of the routes proportional to the number of car journeys that could potentially be replaced. The code below results in Figure 7.4, demonstrating along which routes people are driving short distances49: plot(desire_carshort$geometry) plot(desire_carshort$geom_car, col = &quot;red&quot;, add = TRUE) plot(st_centroid(zones_od)$geometry, add = TRUE) Figure 7.4: Routes along which many (300+) short (&lt;5km Euclidean distance) car journeys are made (red) overlaying desire lines representing the same trips (black) and zone centroids (dots). The results show that the short desire lines along which most people travel by car are geographically clustered. Plotting the results on an interactive map, for example, with mapview::mapview(desire_carshort), reveals that these car trips take place in and around Bradley Stoke. According to Wikipedia, Bradley Stoke is “Europe’s largest new town built with private investment”, which may help understand its high level of car dependency due to limited public transport provision. The excessive number of short car journeys in this area can also be understood in terms of the car-orientated transport infrastructure surrounding this northern ‘edge city’ which includes “major transport nodes such as junctions on both the M4 and M5 motorways” (Tallon 2007). There are many benefits of converting travel desire lines into likely routes of travel from a policy perspective, primary among them the ability to understand what it is about the surrounding environment that makes people travel by a particular mode. We discuss future directions of research building on the routes in section 7.9. For the purposes of this case study, suffice to say that the roads along which these short car journeys travel should be prioritized for investigation to understand how they can be made more conducive to sustainable transport modes. One option would be to add new public transport nodes to the network. Such nodes are described in the next section. 7.6 Nodes Nodes in geographic transport data are zero dimensional features (points) among the predominantly one dimensional features (lines) that comprise the network. There are two types of transport nodes: Nodes not directly on the network such as zone centroids — covered in the next section — or individual origins and destinations such as houses and workplaces. Nodes that are a part of transport networks, representing individual pathways, intersections between pathways (junctions) and points for entering or exiting a transport network such as bus stops and train stations. From a mathematical perspective transport networks can be represented as graphs, in which each segment is connected (via edges representing geographic lines) to one or more other edges in the network. The first type of node can be connected to the network with “centroid connectors”, new route segments joining nodes outside the network with one or more nearby nodes on the network (Hollander 2016). The location of these connectors should be chosen carefully because they can lead to over-estimates of traffic volumes in their immediate surroundings (Jafari et al. 2015). The second type of nodes are nodes on the graph, each of which is connected by one or more straight ‘edges’ that represent individual segments on the network. We will see how transport networks can be represented as mathematical graphs in section 7.7. Public transport stops are particularly important nodes that can be represented as either type of node: a bus stop that is part of a road, or a large rail station that is represented by its pedestrian entry point hundreds of meters from railway tracks. We will use railway stations to illustrate public transport nodes, in relation to the research question of increasing cycling in Bristol. These stations are provided by spDataLarge in bristol_stations. A common barrier preventing people from switching away from cars for commuting to work is that the distance from home to work is too far to walk or cycle. Public transport can reduce this barrier by providing a fast and high-volume option for common routes into cities. From an active travel perspective public transport ‘legs’ of longer journeys divide trips into three: The origin leg, typically from residential areas to public transport stations. The public transport leg, which typically goes from the station nearest a trip’s origin to the station nearest its destination. The destination leg, from the station of alighting to the destination. Building on the analysis conducted in section 7.4, public transport nodes can be used to construct three-part desire lines for trips that can be taken by bus and (the mode used in this example) rail. The first stage is to identify the desire lines with most public transport travel, which in our case is easy because our previously created dataset desire_lines already contains a variable describing the number of trips by train (the public transport potential could also be estimated using public transport routing services such as OpenTripPlanner). To make our approach easy to follow we will select just the top three desire lines in terms of rails use: desire_rail = top_n(desire_lines, n = 3, wt = train) The challenge now is to ‘break-up’ each of these lines into three pieces, representing travel via public transport nodes. This can be done by converting a desire line into a multiline object consisting of three line geometries representing origin, public transport and destination legs of the trip. The first stage is to create matrices of coordinates that will subsequently be used to create matrices representing each leg: mat_orig = as.matrix(line2df(desire_rail)[c(&quot;fx&quot;, &quot;fy&quot;)]) mat_dest = as.matrix(line2df(desire_rail)[c(&quot;tx&quot;, &quot;ty&quot;)]) mat_rail = st_coordinates(bristol_stations) The outputs are three matrices representing the starting points of the trips, their destinations and possible intermediary points at public transport nodes (named orig, dest and rail respectively). But how to identify which intermediary points to use for each desire line? The knn() function from the nabor package (which is used internally by stplanr so it should already be installed) solves this problem by finding k nearest neighbors between two sets of coordinates. By setting the k parameter, one can define how many nearest neighbors should be returned. Of course, k cannot exceed the number of observations in the input (here: mat_rail). We are interested in just one nearest neighbor, namely, the closest railway station. : knn_orig = nabor::knn(mat_rail, query = mat_orig, k = 1)$nn.idx knn_dest = nabor::knn(mat_rail, query = mat_dest, k = 1)$nn.idx This results not in matrices of coordinates, but row indices that can subsequently be used to subset the mat_rail. It is worth taking a look at the results to ensure that the process has worked properly, and to explain what has happened: as.numeric(knn_orig) #&gt; [1] 27 3 2 as.numeric(knn_dest) #&gt; [1] 29 29 29 The output demonstrates that each object contains three whole numbers (the number of rows in desire_rail) representing the rail station closest to the origin and destination of each desire line. Note that while each ‘origin station’ is different, the destination (station 30) is the same for all desire lines. This is to be expected because rail travel in cities tends to converge on a single large station (in this case Bristol Temple Meads). The indices can now be used to create matrices representing the rail station of origin and destination: mat_rail_o = mat_rail[knn_orig, ] mat_rail_d = mat_rail[knn_dest, ] The final stage is to convert these matrices into meaningful geographic objects, in this case simple feature ‘multilinestrings’ that capture the fact that each stage is a separate line, but part of the same overall trip: mats2line = function(mat1, mat2) { lapply(1:nrow(mat1), function(i) { rbind(mat1[i, ], mat2[i, ]) %&gt;% st_linestring() }) %&gt;% st_sfc() } desire_rail$leg_orig = mats2line(mat_orig, mat_rail_o) desire_rail$leg_rail = mats2line(mat_rail_o, mat_rail_d) desire_rail$leg_dest = mats2line(mat_rail_d, mat_dest) Now we are in a position to visualize the results, in Figure 7.5: the three initial desire_rail lines now have three additional geometry list-columns representing travel from home to the origin station, from there to the destination, and finally from the destination station to the destination. In this case the destination leg is very short (walking distance) but the origin legs may be sufficiently far to justify investment in cycling infrastructure to encourage people to cycle to the stations on the outward leg of peoples’ journey to work in the residential areas surrounding the three origin stations in Figure 7.5. Figure 7.5: Station nodes (red dots) used as intermediary points that convert straight desire lines with high rail usage (black) into three legs: to the origin station (red) via public transport (grey) and to the destination (a very short blue line). 7.7 Route networks The data used in this section was downloaded using osmdata. To avoid having to request the data from OSM repeatedly, we will use the bristol_ways object, which contains point and line data for the case study area (see ?bristol_ways): summary(bristol_ways) #&gt; highway maxspeed ref geometry #&gt; cycleway:1262 30 mph : 834 A38 : 202 LINESTRING :4619 #&gt; rail : 813 20 mph : 456 M5 : 138 epsg:4326 : 0 #&gt; road :2544 40 mph : 332 A432 : 131 +proj=long...: 0 #&gt; 70 mph : 323 A4018 : 120 #&gt; 50 mph : 137 A420 : 114 #&gt; (Other): 470 (Other):1697 #&gt; NA&#39;s :2067 NA&#39;s :2217 The above code chunk loaded a simple feature object representing around 3,000 segments on the transport network. This an easily manageable dataset size (transport datasets can be large but it’s best to start small). As mentioned, route networks can usefully be represented as mathematical graphs, with nodes on the network connected by edges. A number of R packages have been developed for dealing with such graphs, notably igraph. One can manually convert a route network into an igraph object, but that process risks loosing the geographic attributes. To overcome this issue SpatialLinesNetwork() was developed in the stplanr package to represent route networks simultaneously as graphs and a set of geographic lines. This function is demonstrated below using a subset of the bristol_ways object used in previous sections. ways_freeway = bristol_ways %&gt;% filter(maxspeed == &quot;70 mph&quot;) ways_sln = SpatialLinesNetwork(ways_freeway) slotNames(ways_sln) #&gt; [1] &quot;sl&quot; &quot;g&quot; &quot;nb&quot; &quot;weightfield&quot; weightfield(ways_sln) #&gt; [1] &quot;length&quot; class(ways_sln@g) #&gt; [1] &quot;igraph&quot; The output of the previous code chunk shows that ways_sln is a composite object with various ‘slots’. These include: the spatial component of the network (named sl), the graph component (g) and the ‘weightfield’, the edge variable used for shortest path calculation (by default segment distance). ways_sln is of class sfNetwork, defined by the S4 class system. This means that each component can be accessed using the @ operator, which is used below to extract its graph component and process it using the igraph package, before plotting the results in geographic space. In the example below the ‘edge betweeness’, meaning the number of shortest paths passing through each edge, is calculated (see ?igraph::betweenness for further details). The results demonstrate that each graph edge represents a segment: the segments near the center of the road network have the greatest betweeness scores. g = ways_sln@g e = igraph::edge_betweenness(ways_sln@g) plot(ways_sln@sl$geometry, lwd = e / 500) Figure 7.6: Illustration of a small route network, with segment thickness proportional to its betweeness, generated using the igraph package and described in the text. One can also find the shortest route between origins and destinations using this graph representation of the route network. This is can be done using the sum_network_routes() function from stplanr, which uses local route network data instead of the online routing service described in section 7.5. This finds the shortest path between arbitrary nodes 1 and 20, on the network — ‘shortest’ with reference to the weightfield slot of ways_sln (route distance by default).50 The result is a spatial linestring object that can be plotted using sf plotting methods (result not shown — readers are encouraged to plot the result locally): path = sum_network_routes(ways_sln, 1, 20, &quot;length&quot;) plot(path$geometry, col = &quot;red&quot;, lwd = 10) plot(ways_sln@sl$geometry, add = TRUE) 7.8 Prioritizing new infrastructure This chapter’s final practical section demonstrates the policy-relevance of geocomputation for transport applications by identifying locations where new transport infrastructure may be needed. The next chapter (8) demonstrates another application: prioritising the location of new bike shops. Bike shops may benefit from new cycling infrastructure, demonstrating an important feature of transport systems: they are closely linked to broader social, economic and land-use patterns. This section ties-together the various strands that explored some geographic features of Bristol’s transport system, covered in sections 7.3 to 7.7. Clearly the types of analysis presented here would need to be extended and complimented by other methods to be used in real-world applications, as discussed in section 7.9. However each stage could be useful on its own, and feed-into wider analyses. To summarize, these were: identifying short but car-dependent commuting routes (generated from desire lines)in section 7.5; creating desire lines representing trips to rail stations in section 7.6; and analysis of transport systems at the route network using graph theory in section 7.7. The final code chunk of this chapter combines these strands of analysis. It adds the car-dependent routes in route_carshort with a newly-created object, route_rail and creates a new column representing the amount of travel along the centroid-to-centroid desire lines they represent: route_rail = st_set_geometry(desire_rail, &quot;leg_orig&quot;) %&gt;% line2route(route_fun = route_osrm) %&gt;% st_set_crs(4326) route_cycleway = rbind(route_rail, route_carshort) route_cycleway$all = c(desire_rail$all, desire_carshort$all) The result of this code, visualized in Figure 7.7, identifies routes of interest in terms of car dependency and key opportunities to invest in public transport-cycling integration. Although other routes between zones are likely to be used — in reality people do not travel to zone centroids or always use the shortest route algorithm for a particular mode — the results demonstrate routes along which cycle paths could be prioritized. Figure 7.7: Potential routes along which to prioritise cycle infrastructure in Bristol, based on access key rail stations (red dots) and routes with many short car journeys (north of Bristol surrounding Stoke Bradley). Line thickness is proportional to number of trips. The results may look more attractive in an interactive map, but what do they mean? The routes highlighted in Figure 7.7 suggest that transport systems are intimately linked to the wider economic and social context. The example of Stoke Bradley is a case in point: its location, lack of public transport services and active travel infrastructure help explain why it is so highly car-dependent. The wider point is that car dependency has a spatial distribution which has implications for sustainable transport policies (Hickman, Ashiru, and Banister 2011). 7.9 Future directions of travel This chapter provides a taster of the possibilities of using geocomputation for transport research. It has explored some key geographic elements that make-up a city’s transport system using open data and reproducible code. The results could help plan where investment is needed. Transport systems operate at multiple interacting levels, meaning that geocomputational methods have great potential to generate insights into how they work. There is much more that could be done in this area: it would be possible to build on the foundations presented in this chapter in many directions. Transport is the fastest growing source of greenhouse gas emissions in many countries, and is set to become “the largest GHG emitting sector, especially in developed countries” (see EURACTIV.com). Because of the highly unequal distribution of transport-related emissions across society, and the fact that transport (unlike food and heating) is not essential for well-being, there is great potential for the sector to rapidly decarbonize through demand reduction, electrification of the vehicle fleet and the uptake of active travel modes such as walking and cycling. Further exploration of such ‘transport futures’ at the local level represents promising direction of travel for transport-related geocomputational research. Methodologically the foundations presented in this chapter could be extended by including more variables in the analysis. Characteristics of the route such as speed limits, busyness and the provision of protected cycling and walking paths could be linked to ‘mode-split’ (the proportion of trips made by different modes of transport). By aggregating OpenStreetMap data using buffers and spatial data methods presented in Chapters 3 and 4, for example, it would be possible to detect the presence of green space in close proximity to transport routes. Using R’s statistical modelling capabilities this could then be used to predict current and future levels of cycling, for example. This type of analysis underlies the Propensity to Cycle Tool (PCT), a publicly accessible (see www.pct.bike) mapping tool developed in R that is being used to prioritize investment in cycling across England (Lovelace et al. 2017). Similar tools could be used to encourage evidence-based transport policies related to other topics such as air pollution and public transport access around the world. 7.10 Exercises What is the total distance of cycleways that would be constructed if all the routes presented in Figure 7.7 were to be constructed? Bonus: find two ways of arriving at the same answer. What proportion of trips represented in the desire_lines are accounted for in the route_cycleway object? Bonus: what proportion of trips cross the proposed routes? Advanced: write code that would increase this proportion. The analysis presented in this chapter is designed for teaching how geocomputation methods can be applied to transport research. If you were to do this ‘for real’ for local government or a transport consultancy what top 3 things would you do differently? Clearly the routes identified in Figure 7.7 only provide part of the picture. How would you extend the analysis to incorporate more trips that could potentially be cycled? Imagine that you want to extend the scenario by creating key areas (not routes) for investment in place-based cycling policies such as car-free zones, cycle parking points and reduced car parking strategy. How could raster data assist with this work? Bonus: develop a raster layer that divides the Bristol region into 100 cells (10 by 10) and provide a metric related to transport policy, such as number of people trips that pass through each cell by walking or the average speed limit of roads (from the bristol_ways dataset). References "],
["location.html", "8 Location analysis Prerequisites 8.1 Introduction 8.2 Case study: bike shops in Germany 8.3 Tidy the input data 8.4 Create census rasters 8.5 Define metropolitan areas 8.6 Points of interest 8.7 Identifying suitable locations 8.8 Discussion and next steps 8.9 Exercises", " 8 Location analysis Prerequisites This chapter requires the following packages (ggmap must also be installed): library(sf) library(raster) library(tidyverse) library(osmdata) Required data will be downloaded in due course. 8.1 Introduction This chapter demonstrates how the skills learned in Part I can be applied to a particular domain: location analysis (also called geomarketing). This is a broad field of research and commercial application, the aim of which is usually to decide the optimal location for new services. A typical example is where to locate a new shop. The aim here is to attract most visitors and, ultimately, make most profit. There are also many non-commercial applications that can use the technique for public benefit, for example where to locate new health services (Tomintz, Clarke, and Rigby 2008). People are fundamental to location analysis, in particular where they are likely to spend their time and other resources. Interestingly, ecological concepts and models are quite similar to those used for store location analysis. Animals and plants can best meet their needs in certain ‘optimal’ locations, based on variables that change over space (Muenchow et al. (2018); see also chapter ??) . This is one of the great strengths of geocomputation and GIScience in general. Concepts and methods are transferable to other fields. Polar bears, for example, prefer northern latitudes where temperatures are lower and food (seals and sea lions) is plentiful. Similarly, humans tend to congregate certain places, creating economic niches (and high land prices) analogous to the ecological niche of the Arctic. The main task of location analysis is to find out where such ‘optimal locations’ are for specific services, based on available data. Typical research questions include: Where do target groups live and which areas do they frequent? Where are competing stores or services located? How many people can easily reach specific stores? Do existing services over or under-exploit the market potential? What is the market share of a company in a specific area? This chapter demonstrates how geocomputation can answer such questions based on a hypothetical case study based on real data. 8.2 Case study: bike shops in Germany Imagine you are starting a chain of bike shops in Germany. The stores should be placed in urban areas with as many potential customers as possible. Additionally, a survey51 suggests that single young males (aged 20 to 40) are most likely to buy your products: this is the target audience. You are in the lucky position to have sufficient capital to open a number of shops. But where should they be placed? Consulting companies (employing location analysts) would happily charge high rates to answer such questions. Luckily, we can do so ourselves with the help of open data and open source software. The following sections will demonstrate how the techniques learned during the first chapters of the book can be applied to undertake the following steps: Tidy the input data from the German census (section 8.3). Convert the tabulated census data into raster objects (section 8.4). Identify metropolitan areas with high population densities (section 8.5). Download detailed geographic data (from OpenStreetMap, with osmdata) for these areas (section 8.6). Create rasters for scoring the relative desirability of different locations using map algebra (section 8.7). Although we have applied these steps to a specific case study, they could be generalized to many scenarios of store location or public service provision. 8.3 Tidy the input data The German government provides gridded census data at either 1 km or 100 m resolution. The following code chunk downloads, unzips and reads-in the 1 km data. download.file(&quot;https://tinyurl.com/ybtpkwxz&quot;, destfile = &quot;census.zip&quot;, mode = &quot;wb&quot;) unzip(&quot;census.zip&quot;) # unzip the files census_de = readr::read_csv2(list.files(pattern = &quot;Gitter.csv&quot;)) The census_de object is a data frame containing 13 variables for more than 300,000 grid cells across Germany. For our work we only need a subset of these: Easting and Northing, number of inhabitants (population), mean average age, proportion of women and average household size. These variables and selected and renamed in the code chunk below and summarized in Table 8.1. Further, mutate_all() is used to convert values -1 and -9 (meaning unknown) to NA. Table 8.1: Excerpt from the data description ‘Datensatzbeschreibung_klassierte_Werte_1km-Gitter.xlsx’ located in the downloaded file census.zip describing the classes of the retained variables. The classes -1 and -9 refer to uninhabited areas or areas which have to be kept secret, for example due to the need to preserve anonymity. class population (number of people) women (%) mean age (years) household size (number of people) 1 3-250 0-40 0-40 1-2 2 250-500 40-47 40-42 2-2.5 3 500-2000 47-53 42-44 2.5-3 4 2000-4000 53-60 44-47 3-3.5 5 4000-8000 &gt;60 &gt;47 &gt;3.5 6 &gt;8000 # pop = population, hh_size = household size input = dplyr::select(census_de, x = x_mp_1km, y = y_mp_1km, pop = Einwohner, women = Frauen_A, mean_age = Alter_D, hh_size = HHGroesse_D) # set -1 and -9 to NA input_tidy = mutate_all(input, funs(ifelse(. %in% c(-1, -9), NA, .))) 8.4 Create census rasters After the preprocessing, the data can be converted into a raster stack or brick (see sections 2.2.3 and 3.3.1). rasterFromXYZ() makes this really easy. It requires an input data frame where the first two columns represent coordinates on a regular grid. All the remaining columns (here: pop, women, mean_age, hh_size) will serve as input for the raster brick layers (Figure 8.1). input_ras = rasterFromXYZ(input_tidy, crs = st_crs(3035)$proj4string) # print the output to the console input_ras #&gt; class : RasterBrick #&gt; dimensions : 868, 642, 557256, 4 (nrow, ncol, ncell, nlayers) #&gt; resolution : 1000, 1000 (x, y) #&gt; extent : 4031000, 4673000, 2684000, 3552000 (xmin, xmax, ymin, ymax) #&gt; coord. ref. : +proj=laea +lat_0=52 +lon_0=10 +x_0=4321000 +y_0=3210000 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs #&gt; data source : in memory #&gt; names : pop, women, mean_age, hh_size #&gt; min values : 1, 1, 1, 1 #&gt; max values : 6, 5, 5, 5 Note that we are using an equal-area projection (EPSG:3035; Lambert Equal Area Europe), i.e. a projected CRS where each grid cell has the same area, here 1000 x 1000 square meters. Since we are using mainly densities such as the number of inhabitants or the portion of women per grid cell, it is of utmost importance that the area of each grid cell is the same to avoid ‘comparing apples and oranges’. Be careful with geographic CRS where grid cell areas constantly decrease in poleward directions (see also sections 2.3 and 5.2). Figure 8.1: Gridded German census data of 2011. See Table 8.1 for a description of the classes. The next stage is to reclassify the values of the rasters stored in input_ras in accordance with the survey mentioned in section 8.2, using the raster function reclassify(), which was introduced in section 4.3.3. In the case of the population data we convert the classes into a numeric data type using class means. Raster cells are assumed to have a population of 127 if they have a value of 1 (cells in ‘class 1’ contain between 3 and 250 inhabitants) and 375 if they have a value of 2 (containing 250 to 500 inhabitants), and so on (see Table 8.1). A cell value of 8000 inhabitants was chosen for ‘class 6’ because these cells contain more than 8000 people. Of course, these are approximations of the true population, not precise values.52 However, the level of detail is sufficient to delineate metropolitan areas (see next section). In contrast to the pop variable, representing absolute estimates of the total population, the remaining variables were re-classified as weights corresponding with weights used in the survey. Class 1 in the variable women, for instance, represents areas in which 0 to 40% of the population is female; these are reclassified with a comparatively high weight of 3 because the target demographic is predominantly male. Similarly, the classes containing the youngest people and highest proportion of single households are reclassified to have high weights. rcl_pop = matrix(c(1, 1, 127, 2, 2, 375, 3, 3, 1250, 4, 4, 3000, 5, 5, 6000, 6, 6, 8000), ncol = 3, byrow = TRUE) rcl_women = matrix(c(1, 1, 3, 2, 2, 2, 3, 3, 1, 4, 5, 0), ncol = 3, byrow = TRUE) rcl_age = matrix(c(1, 1, 3, 2, 2, 0, 3, 5, 0), ncol = 3, byrow = TRUE) rcl_hh = rcl_women rcl = list(rcl_pop, rcl_women, rcl_age, rcl_hh) We can loop with map2(), the purrr version of base R’s mapply(), in parallel over two vectors (here lists; for more information please refer to Wickham (2014) and Grolemund and Wickham (2016)). Note that we have to transform the raster brick into a list for the loop to work. Finally, we convert the output list back into a raster stack. reclass = map2(as.list(input_ras), rcl, function(x, y) { reclassify(x = x, rcl = y, right = NA) }) %&gt;% stack names(reclass) = names(input_ras) reclass #&gt; class : RasterStack #&gt; dimensions : 868, 642, 557256, 4 (nrow, ncol, ncell, nlayers) #&gt; resolution : 1000, 1000 (x, y) #&gt; extent : 4031000, 4673000, 2684000, 3552000 (xmin, xmax, ymin, ymax) #&gt; coord. ref. : +proj=laea +lat_0=52 +lon_0=10 +x_0=4321000 +y_0=3210000 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs #&gt; names : pop, women, mean_age, hh_size #&gt; min values : 127, 0, 0, 0 #&gt; max values : 8000, 3, 3, 3 8.5 Define metropolitan areas We define metropolitan areas as pixels of 20 km2 inhabited by more than 500,000 people. Pixels at this coarse resolution can rapidly be created using aggregate(), as introduced in section 5.4.2. The command below uses the argument fact = 20 to reduce the resolution of the result twenty-fold (recall the original raster resolution was 1 km2): pop_agg = aggregate(reclass$pop, fact = 20, fun = sum) The next stage is to keep only cells with more than half a million people, and convert the result cells into a vector object of class sf. polys = rasterToPolygons(pop_agg[pop_agg &gt; 500000, drop = FALSE]) %&gt;% st_as_sf(polys) Plotting these polygons reveals eight metropolitan regions (Fig. 8.2). Each region consists of one or more polygons (raster cells). It would be nice if we could join all polygons belonging to one region. One approach is to union the polygons (see section 5.3.5). polys = st_union(polys) This returns one multipolygon feature with its elements corresponding to the metropolitan regions. To extract these polygons from the multipolygon, we can use st_cast(). metros = st_cast(polys, &quot;POLYGON&quot;) However, visual inspection reveals eight metropolitan areas whereas the unioning-casting approach comes up with nine. This is because one polygon just touches the corner of another polygon (western Germany, Cologne/Düsseldorf area; Fig. 8.2). One could assign it to the neighboring region using a dissolving procedure, however, we leave this as an exercise to the reader, and simply delete the offending polygon. # find out about the offending polygon int = st_intersects(metros, metros) # polygons 5 and 9 share one border, delete polygon number 5 metros_2 = metros[-5] Figure 8.2: The aggregated population raster (resolution: 20 km) with the identified metropolitan areas (golden polygons) and the corresponding names. The defined metropolitan areas (Fig. 8.2) suitable for bike shops are still missing a name. A reverse geocoding approach can settle this problem. Given a coordinate, reverse geocoding finds the corresponding address. Consequently, extracting the centroid coordinate of each metropolitan area can serve as an input for a reverse geocoding API. The ggmap package makes use of the one provided by Google.53 ggmap::revgeocode() only accepts geographical coordinates (latitude/longitude), therefore, the first requirement is to bring the metropolitan polygons into an appropriate coordinate reference system (chapter 5). metros_wgs = st_transform(metros, 4326) coords = st_centroid(metros_wgs) %&gt;% st_coordinates() %&gt;% round(., 4) #&gt; Warning in st_centroid.sfc(metros_wgs): st_centroid does not give correct #&gt; centroids for longitude/latitude data Additionally, ggmap::revgeocode() only accepts one coordinate at a time, which is why we iterate over each coordinate of coords via a loop (map_dfr()). map_dfr() does exactly the same as lapply() except for returning a data.frame instead of a list.54 Sometimes, Google’s reverse geocoding API is unable to find an address returning NA. Usually, trying the same coordinate again returns an address at the second or third attempt (see while()-loop). However, if three attempts have already failed, this is a good indication that the requested information is indeed unavailable. Since we aim to be good cyberspace citizens, we try not to overburden the server with too many queries within a short amount of time by letting the loop sleep between one and four seconds after each iteration before accessing the reverse geocoding API again. # reverse geocoding to find out the names of the metropolitan areas metro_names = map_dfr(1:nrow(coords), function(i) { add = ggmap::revgeocode(coords[i, ], output = &quot;more&quot;) x = 2 while (is.na(add$address) &amp; x &gt; 0) { add = ggmap::revgeocode(coords[i, ], output = &quot;more&quot;) # just try three times x = x - 1 } # give the server a bit time Sys.sleep(sample(seq(1, 4, 0.1), 1)) # return the result add }) Choosing more as revgeocode()’s output option will give back a data.frame with several columns referring to the location including the address, locality and various administrative levels. Overall, we are satisfied with the locality column serving as metropolitan names (München, Nürnberg, Stuttgart, Frankfurt, Hamburg, Berlin, Leipzig) apart from one exception, namely Velbert. Hence, we replace Velbert with the corresponding name in the administrative_area_level_2 column, that is Düsseldorf (Fig. 8.2). Umlauts like ü might lead to trouble further on, for example when determining the bounding box of a metropolitan area with opq() (see further below), which is why we replace them. metro_names = dplyr::select(metro_names, locality, administrative_area_level_2) %&gt;% # replace Velbert and umlaut ü mutate(locality = ifelse(locality == &quot;Velbert&quot;, administrative_area_level_2, locality), locality = gsub(&quot;ü&quot;, &quot;ue&quot;, locality)) %&gt;% pull(locality) 8.6 Points of interest The osmdata package provides a fantastic and easy-to-use interface to download OSM data (see also section 6.2). Instead of downloading all shops for the whole of Germany, we restrict the download to the defined metropolitan areas. This relieves the OSM server resources, reduces download time and above all only gives back the shop locations we are interested in. The map() loop, the lapply() equivalent of purrr, runs through all eight metropolitan names which subsequently define the bounding box in the opq() function (see section 6.2). Alternatively, we could have provided the bounding box in the form of coordinates ourselves. Next, we indicate that we would only like to download shop features (see this page for a full list of OpenStreetMap map features). osmdata_sf() returns a list with several spatial objects (points, lines, polygons, etc.). Here, we will only keep the point objects. As with Google’s reverse geocode API, the OSM-download will sometimes fail at the first attempt. The while loop increases the number of download trials to three. If then still no features can be downloaded, it is likely that either there are none available or that another error has occurred before (e.g. due to erroneous output from opq()). shops = map(metro_names, function(x) { message(&quot;Downloading shops of: &quot;, x, &quot;\\n&quot;) # give the server a bit time Sys.sleep(sample(seq(5, 10, 0.1), 1)) query = opq(x) %&gt;% add_osm_feature(key = &quot;shop&quot;) points = osmdata_sf(query) # request the same data again if nothing has been downloaded iter = 2 while (nrow(points$osm_points) == 0 &amp; iter &gt; 0) { points = osmdata_sf(query) iter = iter - 1 } points = st_set_crs(points$osm_points, 4326) }) It is highly unlikely that there are no shops in any of our defined metropolitan areas. The following if condition simply checks if there is at least one shop for each region. If not, we would try to download the shops again for this/these specific region/s. # checking if we have downloaded shops for each metropolitan area ind = map(shops, nrow) == 0 if (any(ind)) { message(&quot;There are/is still (a) metropolitan area/s without any features:\\n&quot;, paste(metro_names[ind], collapse = &quot;, &quot;), &quot;\\nPlease fix it!&quot;) } To make sure that each list element (an sf data frame) comes with the same columns, we only keep the osm_id and the shop columns with the help of another map loop. This is not a given since OSM contributors are not equally meticulous when collecting data. Finally, we rbind all shops into one large sf object. # select only specific columns and rbind all list elements shops = map(shops, dplyr::select, osm_id, shop) %&gt;% reduce(rbind) It would have been easier to simply use map_dfr(). Unfortunately, so far it does not work in harmony with sf objects. The only thing left to do is to convert the spatial point object into a raster (see section 11.4). The sf object, shops, is converted into a raster having the same parameters (dimensions, resolution, CRS) as the reclass object. Importantly, the count() function is used here to calculate the number shops in each cell. If the shop column were used instead of the osm_id column, we would have retrieved fewer shops per grid cell. This is because the shop column contains NA values, which the count() function omits when rasterizing vector objects. The result of the subsequent code chunk is therefore an estimate of shop density (shops/km2). st_transform() is used before rasterize() to ensure the CRS of both inputs match. shops = st_transform(shops, proj4string(reclass)) # create poi raster poi = rasterize(x = shops, y = reclass, field = &quot;osm_id&quot;, fun = &quot;count&quot;) As with the other raster layers (population, women, mean age, household size) the poi raster is reclassified into four classes (see section 8.4). Defining class intervals is an arbitrary undertaking to a certain degree. One can use equal breaks, quantile breaks, fixed values or others. Here, we choose the Fisher-Jenks natural breaks approach which minimizes within-class variance, the result of which provides an input for the reclassification matrix. # construct reclassification matrix int = classInt::classIntervals(values(poi), n = 4, style = &quot;fisher&quot;) int = round(int$brks) rcl_poi = matrix(c(int[1], rep(int[-c(1, length(int))], each = 2), int[length(int)] + 1), ncol = 2, byrow = TRUE) rcl_poi = cbind(rcl_poi, 0:3) # reclassify poi = reclassify(poi, rcl = rcl_poi, right = NA) names(poi) = &quot;poi&quot; 8.7 Identifying suitable locations The only steps that remain before combining all the layers are to add POI and delete the population from the raster stack. The reasoning for the latter is twofold. First of all, we have already delineated metropolitan areas, that is areas where the population density is above average compared to the rest of Germany. Secondly, though it is advantageous to have many potential customers within a specific catchment area, the sheer number alone might not actually represent the desired target group. For instance, residential tower blocks are areas with a high population density but not necessarily with a high purchasing power for expensive cycle components. This is achieved with the complimentary functions addLayer() and dropLayer(): # add poi raster reclass = addLayer(reclass, poi) # delete population raster reclass = dropLayer(reclass, &quot;pop&quot;) In common with other data science projects, data retrieval and ‘tidying’ have consumed much of the overall workload so far. With clean data the final step, calculating a final score by summing up all raster layers, can be accomplished in a single line. # calculate the total score result = sum(reclass) For instance, a score greater than 9 might be a suitable threshold indicating raster cells where a bike shop could be placed (Figure 8.3). Figure 8.3: Suitable areas (i.e. raster cells with a score &gt; 9) in accordance with our hypothetical survey for bike stores in Berlin. 8.8 Discussion and next steps The presented approach is a typical example of the normative usage of a GIS (Longley 2015). We combined survey data with expert-based knowledge and assumptions (definition of metropolitan areas, defining class intervals, definition of a final score threshold). It should be clear that this approach is not suitable for scientific knowledge advancement but is a very applied way of information extraction. This is to say, we can only suspect based on common sense that we have identified areas suitable for bike shops. However, we have no proof that this is in fact the case. A few other things remained unconsidered but might improve the analysis: We used equal weights when calculating the final scores. But is, for example, the household size as important as the portion of women or the mean age? We used all points of interest. Maybe it would be wiser to use only those which might be interesting for bike shops such as do-it-yourself, hardware, bicycle, fishing, hunting, motorcycles, outdoor and sports shops (see the range of shop values available on the OSM Wiki). Data at a better resolution may change and improve the output. For example, there is also population data at a finer resolution (100 m; see exercises). We have used only a limited set of variables. For example, the INSPIRE geoportal might contain much more data of possible interest to our analysis (see also section 6.2. The bike paths density might be another interesting variable as well as the purchasing power or even better the retail purchasing power for bikes. Interactions remained unconsidered, such as a possible interaction between the portion of men and single households. However, to find out about such an interaction we would need customer data. In short, the presented analysis is far from perfect. Nevertheless, it should have given you a first impression and understanding of how to obtain, and deal with spatial data in R within a location analysis context. Finally, we have to point out that the presented analysis would be merely the first step of finding suitable locations. So far we have identified areas, 1 by 1 km in size, potentially suitable for a bike shop in accordance with our survey. We could continue the analysis as follows: Find an optimal location based on number of inhabitants within a specific catchment area. For example, the shop should be reachable for as many people as possible within 15 minutes of traveling bike distance (catchment area routing). Thereby, we should account for the fact that the further away the people are from the shop, the more unlikely it becomes that they actually visit it (distance decay function). Also it would be a good idea to take into account competitors. That is, if there already is a bike shop in the vicinity of the chosen location, one has to distribute possible customers (or sales potential) between the competitors (Huff 1963; Wieland 2017). We need to find suitable and affordable real estate (accessible, parking spots, frequency of passers-by, big windows, etc.). 8.9 Exercises We have used raster::rasterFromXYZ() to convert a input_tidy into a raster brick. Try to achieve the same with the help of the sp::gridded() function. In the text we have deleted one polygon of the metros object (polygon number 5) since it only touches the border of another polygon. Recreate the metros object and instead of deleting polygon number 5, make it part of the Cologne/Düsseldorf metropolitan region (hint: create a column named region_id, add polygon number 5 to the Cologne/Düsseldorf area and dissolve). Download the csv file containing inhabitant information for a 100 m cell resolution (https://www.zensus2011.de/SharedDocs/Downloads/DE/Pressemitteilung/DemografischeGrunddaten/csv_Bevoelkerung_100m_Gitter.zip?__blob=publicationFile&amp;v=3). Please note that the unzipped file has a size of 1.23 GB. To read it into R you can use readr::read_csv. This takes 30 seconds on my machine (16 GB RAM) data.table::fread() might be even faster, and returns an object of class data.table(). Use as.tibble() to convert it into a tibble. Build an inhabitant raster, aggregate it to a cell resolution of 1 km, and compare the difference with the inhabitant raster (inh) we have created using class mean values. Suppose our bike shop predominantly sold electric bikes to older people. Change the age raster accordingly, repeat the remaining analyses and compare the changes with our original result. References "],
["adv-map.html", "9 Making maps with R Prerequisites 9.1 Introduction 9.2 Static maps 9.3 Animated maps 9.4 Interactive maps 9.5 Web mapping applications with shiny 9.6 Other mapping packages 9.7 Exercises", " 9 Making maps with R Prerequisites This chapter requires the following packages that we have already been using: library(sf) library(spData) library(spDataLarge) library(tidyverse) In addition it uses the following visualization packages: library(leaflet) # for interactive maps library(mapview) # for interactive maps library(shiny) # for web applications library(tmap) # for static and interactive maps 9.1 Introduction A satisfying and important aspect of geographic research is producing and communicating the results in the form of maps. Map making — the art of Cartography — is an ancient skill that involves precision, consideration of the map-reader and often an element of creativity. Basic plotting of geographic data is straightforward in R with plot() (see section 2.1.3), and it is possible to create advanced maps using base R methods (Murrell 2016). This chapter focuses on dedicated map-making packages, especially tmap, for reasons that are explained in section 9.2. There are a multitude of options but when learning a new skill (in this case map making), it makes sense to gain depth-of-knowledge in one package before branching out, a concept that also applies to the choice of programming language as we saw in Chapter 1. It is worth developing advanced map making skills not only because it is a fun activity that can produce beautiful results. Map making also has important practical applications. A carefully crafted map can help ensure that time spent in the analysis phases of geocomputational projects — for example using methods covered in chapters 2 to 5 — are communicated effectively (???): Amateur-looking maps can undermine your audience’s ability to understand important information and weaken the presentation of a professional data investigation. Maps have been used for several thousand years for a wide variety of purposes. Historic examples include maps of buildings and land ownership in the Old Babylonian dynasty more than 3000 years ago and Ptolemy’s world map in his masterpiece Geography nearly 2000 years ago (Talbert 2014). However, maps have historically been out of reach for everyday people. Modern computing has the potential to change this. Map making skills can also help meet research and public engagement objectives. From a research perspective clear maps are often be the best way to present the results of geocomputational research. From policy and ‘citizen science’ perspectives, attractive and engaging maps can help change peoples’ minds, based on the evidence. Map making is therefore a critical part of geocomputation and its emphasis not only on describing, but also changing the world (see Chapter 1). 9.2 Static maps Static maps are the most common type of visual output from geocomputation. They are fixed images that can be included in printed outputs or published online. The majority of maps contained in this book, for example, are static maps saved as .png files (interactive maps are covered in section 9.4). The generic plot() function is often the fastest way to create static maps from vector and raster spatial objects, as demonstrated in sections 2.1.3 and 2.2.2. Sometimes the simplicity and speed of this approach is sufficient, especially during the development phase of a project: when using R interactively to understand a geographic dataset, you will likely be the only person who sees them. The base R approach is also extensible, with plot() offering dozens of arguments and the grid providing functions for low-level control of graphical outputs, — see R Graphics (Murrell 2016), especially Chapter 14. The focus of this section, however, is making static with tmap. Why tmap? It is a powerful and flexible map-making package with sensible defaults. It has a concise syntax that allows for the creation of attractive maps with minimal code, that will be familiar to ggplot2 users. Furthermore, tmap has a unique capability to generate static and interactive maps using the same code via tmap_mode(). It accepts a wider range of spatial classes (including raster objects) than alternatives such as ggplot2, as documented in vignettes tmap-nutshell and tmap-modes and an excellent academic paper on the subject (Tennekes 2018b). This section teaches how to make static maps with tmap, emphasizing the important aesthetic and layout options. 9.2.1 tmap basics tmap generates maps with sensible defaults for a wide range of spatial objects with tm_shape() (which accepts raster and vector objects), followed by one or more layer elements such as tm_fill() and tm_dots(). These functions are used singularly and in combination in the code chunk below, which generates the maps presented in Figure 9.1: # Add fill layer to nz shape tm_shape(nz) + tm_fill() # Add border layer to nz shape tm_shape(nz) + tm_borders() # Add fill and border layers to nz shape tm_shape(nz) + tm_fill() + tm_borders() Figure 9.1: New Zealand’s shape plotted with fill (left), border (middle) and fill and border (right) layers added using tmap functions. The object passed to tm_shape() in this case is nz, which represents the regions of New Zealand. Layers are added to represent nz visually, with tm_fill() and tm_borders() creating shaded areas (right panel) and border outlines (middle panel) in Figure 9.1, respectively. This is an intuitive approach to map making: the common task of adding new layers is undertaken by the addition operator +, followed by tm_*(). The asterisk (*) refers to a wide range of layer types which have self-explanatory names including fill, borders (demonstrated above), bubbles, text and raster (see `?tmap-element``` for a list of available elements). This ‘layering’ is illustrated in the right panel of Figure 9.1, which shows the result of adding a border on top of the fill layer. The order in which layers are added is the order in which they are rendered. qtm() is a handy function for quickly creating tmap maps (hence the snappy name). It is concise and provides a good default visualization in many cases: qtm(nz), for example, is equivalent to tm_shape(nz) + tm_fill() + tm_borders(). Further, layers can be added concisely using multiple qtm() calls, such as qtm(nz) + qtm(nz_height). The disadvantage is that it makes aesthetics of individual layers harder to control, explaining why we avoid teaching it in this chapter. 9.2.2 Map objects, shapes and layers A useful feature of tmap is its ability to store objects representing maps. The code chunk below demonstrates this by saving the last plot in Figure 9.1 as an object of class tmap (note the use of tm_polygons() which condenses tm_fill() + tm_borders() into a single function): map_nz = tm_shape(nz) + tm_polygons() class(map_nz) #&gt; [1] &quot;tmap&quot; map_nz can be plotted later, for example by adding additional layers (as shown below) or simply running map_nz in the console, which is equivalent to print(map_nz). New shapes can be added with + tm_shape(new_obj). In this case new_obj represents a new spatial object to be plotted on top of preceding layers. When a new new shape is added in this way all subsequent aesthetic functions refer to it, until another new shape is added. This syntax allows the creation of maps with multiple shapes and layers, as illustrated in the next code chunk which uses the function tm_raster() to plot a raster layer (with alpha set to make the layer semi-transparent): map_nz1 = map_nz + tm_shape(nz_elev) + tm_raster(alpha = 0.7) Building on the previously created map_nz object the preceding code creates a new map object map_nz1 which contains another shape (nz_eleve), representing average elevation across New Zealand (see Figure 9.2, left). More shapes and layers can be added, as illustrated in the code chunk below which creates nz_water, representing New Zealand’s territorial waters, and adds the resulting lines to an existing map object. nz_water = st_union(nz) %&gt;% st_buffer(22200) %&gt;% st_cast(to = &quot;LINESTRING&quot;) map_nz2 = map_nz1 + tm_shape(nz_water) + tm_lines() There is no limit to the number of layers or shapes that can be added to tmap objects. The same shape can even be used multiple times. The final map illustrated in Figure 9.2 is created by adding a layer representing high points (stored in the object nz_height) onto the previously created map_nz2 object with tm_dots() (see ?tm_dots and ?tm_bubbles for details on tmap’s point plotting functions). The resulting map, which has 5 layers, is illustrated in the right-hand panel of: map_nz3 = map_nz2 + tm_shape(nz_height) + tm_dots() A useful and little known feature of tmap is that multiple map objects can be arranged in a single ‘metaplot’ with tmap_arrange(). This is demonstrated in the code chunk below which plots map_nz1 to map_nz3, resulting in Figure 9.2. tmap_arrange(map_nz1, map_nz2, map_nz3) Figure 9.2: Maps with additional layers added to the final map of Figure 9.1. Additional elements such as north arrows (tm_compass()), scale bars (tm_scale_bar()) and layout options (tm_layout()) can also be added with the + operator. Aesthetic settings, however, are controlled by arguments to layer functions. 9.2.3 Aesthetics The plots in the previous section demonstrate tmap’s default aesthetic settings. Grey shades are used for tm_fill() and tm_bubbles() layers and a continuous red line is used to represent lines created with tm_lines(). Of course, these default values and other aesthetics can be overridden. The purpose this section is to show how. There are two main types of map aesthetics: those that change with the data and those that are constant. Unlike ggplot2 which uses the helper function aes() to represent the former, tmap layer functions accept aesthetic arguments that are either constant values or variable fields. The most commonly used aesthetics for fill and border layers include color, transparency, line width and line type, (set with col, alpha, lwd, and lty arguments respectively). The impact of setting these with fixed values is illustrated in Figure 9.3. ma1 = tm_shape(nz) + tm_fill(col = &quot;red&quot;) ma2 = tm_shape(nz) + tm_fill(col = &quot;red&quot;, alpha = 0.3) ma3 = tm_shape(nz) + tm_borders(col = &quot;blue&quot;) ma4 = tm_shape(nz) + tm_borders(lwd = 3) ma5 = tm_shape(nz) + tm_borders(lty = 2) ma6 = tm_shape(nz) + tm_fill(col = &quot;red&quot;, alpha = 0.3) + tm_borders(col = &quot;blue&quot;, lwd = 3, lty = 2) tmap_arrange(ma1, ma2, ma3, ma4, ma5, ma6) Figure 9.3: The impact of changing commonly used fill and border aesthetics to fixed values. Like base R plots, arguments defining aesthetics can also receive values that vary. Unlike the base R code below (which generates the left panel in Figure 9.4), tmap aesthetic arguments will not accept a numeric vector: plot(st_geometry(nz), col = 1:nrow(nz)) # works tm_shape(nz) + tm_fill(col = 1:nrow(nz)) # fails: #&gt; Error: Fill argument neither colors nor valid variable name(s) Instead col (and other aesthetics that can vary such as lwd for line layers and size for point layers) requires a character string naming an attribute associated with the geometry to be plotted. Thus, one would achieve the desired (plotted in the right-hand panel of Figure 9.4) result as follows: nz$col = 1:nrow(nz) tm_shape(nz) + tm_fill(col = &quot;col&quot;) Figure 9.4: Comparison of base (left) and tmap (right) handling of a numeric color field. An important argument in functions defining aesthetic layers such as tm_fill() is title, which sets the title of the associated legend. The following code chunk demonstrates this functionality by providing a more attractive name than the variable name Land_area (note the use of expression() for to create superscript text): legend_title = expression(&quot;Area (km&quot;^2*&quot;)&quot;) map_nza = tm_shape(nz) + tm_fill(col = &quot;Land_area&quot;, title = legend_title) + tm_borders() The resulting tmap object map_nza will be used, alongside map_nz, to illustrate different layout settings in the next section. 9.2.4 Color settings Color settings are an important part of map design. They can have a major impact on how spatial variability is portrayed as illustrated in Figure 9.5. This shows four ways of coloring regions in New Zealand depending on median income, from right to left (and demonstrated in the code chunk below): The default setting uses ‘pretty’ breaks, described in the next paragraph breaks which allows you to manually set the breaks n which sets the number of bins into which numeric variables are categorized and palette which defines the color scheme, for example Reds breaks = c(0, 3, 4, 5) * 10000 tm_shape(nz) + tm_polygons(col = &quot;Median_income&quot;) tm_shape(nz) + tm_polygons(col = &quot;Median_income&quot;, breaks = breaks) tm_shape(nz) + tm_polygons(col = &quot;Median_income&quot;, n = 10) tm_shape(nz) + tm_polygons(col = &quot;Median_income&quot;, palette = &quot;RdBu&quot;) Figure 9.5: Illustration of settings that affect color settings. The results show (from left to right): default settings, manual breaks, n breaks, and the impact of changing the palette. Another way to change color settings is by altering color break (or bin) settings. In addition to manually setting breaks tmap allows users to specify algorithms to automatically create breaks with the style argument. Six of the most useful break styles are illustrated in Figure 9.6 and described in the bullet points below: style = pretty, the default setting, rounds breaks into whole numbers where possible and spaces them evenly style = equal divides input values into bins of equal range, and is appropriate for variables with a uniform distribution (not recommended for variables with a skewed distribution as the resulting map may end-up having little color diversity) style = quantile ensures the same number of observations fall into each category (with the potential down side that bin ranges can vary widely) style = jenks identifies groups of similar values in the data and maximizes the differences between categories style = cont (and order) present a large number of colors over continuous color field, and are particularly suited for continuous rasters (order can help visualize skewed distributions) style = cat was designed to represent categorical values and assures that each category receives a unique color. Figure 9.6: Illustration of different binning methods set using the style argument in tmap. Although style is an argument of tmap functions it in facts originates as an argument in classInt::classIntervals() — see the help page of this function for details. Palettes define the color ranges associated the bins determined by the breaks, n, and style arguments described above. The default color palette is specified in tm_layout() (see section 9.2.5 to learn more), however, it could be quickly changed using the palette argument. It expects a vector of colors or a new color palette name, which can be selected interactively with tmaptools::palette_explorer(). You can add a - as prefix to reverse the palette order. There are three main groups of color palettes - categorical, sequential and diverging (Figure 9.7), and each of them serves a different purpose. Categorical palettes consist of easily distinguishable colors and are most appropriate for categorical data without any particular order such as state names or land cover classes. Colors should be intuitive: rivers should be blue, for example, and pastures green. Avoid too many categories: maps with large legends and many colors can be uninterpretable.55 The second group is sequential palettes. These follow a gradient, for example from light to dark colors (light colors tend to represent lower values), and appropriate for continuous (numeric) variables. Sequential palettes can be single (Blues go from light to dark blue for example) or multi-color/hue (YlOrBr is gradient from light yellow to brown via orange, for example), as demonstrated in the code chunk below — output not shown, run the code yourself to see the results! tm_shape(nz) + tm_fill(&quot;Population&quot;, palette = &quot;Blues&quot;) tm_shape(nz) + tm_fill(&quot;Population&quot;, palette = &quot;YlOrBr&quot;) The last group, diverging palettes, typically range between three distinct colors (purple-white-green in Figure 9.7) and are usually created by joining two single color sequential palettes with the darker colors at each end. Their main purpose is too visualize the difference from an important reference point, e.g. a certain temperature, the median household income or the mean probability for a drought event. Figure 9.7: Examples of categorical, sequential and diverging palettes. There are two important principles for consideration when working with colors - perceptibility and accessibility. Firstly, colors on maps should match our perception. This means that certain colors are viewed through our experience and also cultural lenses. For example, green color usually represent vegetation or lowlands and blue is connected with water or cool. Color palettes also should be easy to understand by effectively conveying information. It should be clear which values are lower and which are higher, and colors should change gradually. This property is not preserved in the rainbow color palette, therefore we suggest avoiding it in spatial data visualization (Borland and Ii 2007). Secondly, changes in colors should be accessible to the largest number of people. Therefore, it is important to use colorblind friendly palettes as often as possible.56 9.2.5 Layouts The map layout refers to the combination of all map elements into a cohesive map. Map elements include among others the objects to be mapped, the title, the scale bar, margins and aspect ratios, while the color settings covered in the previous section relate to the palette and break-points used affect how the map looks. Both may result in subtle changes that can have an equally large impact on the impression left by your maps. tmap allows a wide variety of layout settings to be changed, some of which are illustrated in Figure 9.8, produced using the following code (see args(tm_layout) or ?tm_layout for a full list): map_nz + tm_layout(title = &quot;New Zealand&quot;) map_nz + tm_layout(scale = 5) map_nz + tm_layout(bg.color = &quot;lightblue&quot;) map_nz + tm_layout(frame = FALSE) Figure 9.8: Layout options specified by (from left to right) title, scale, bg.color and frame arguments. The other arguments in tm_layout() provide control over many more aspects of the map in relation to the canvas on which it is placed. Some useful layout settings are listed below (see Figure 9.9 for illustrations of a selection of these): Frame width (frame.lwd) and an option to allow double lines (frame.double.line). Margin settings including outer.margin and inner.margin. Font settings, controlled by fontface and fontfamily. Legend settings including binary options such as legend.show (whether or not to show the legend) legend.only (omit the map) and legend.outside (should the legend go outside the map?), as well as multiple choice settings such as legend.position. Default colors of aesthetic layers (aes.color), map attributes such as the frame (attr.color). Color settings controlling sepia.intensity (how yellowy the map looks) and saturation (a color-greyscale). Figure 9.9: Illustration of selected layout options. The impact of changing the color settings listed above is illustrated in Figure 9.10 (see ?tm_layout for a full list). Figure 9.10: Illustration of selected color-related layout options. Beyond the low-level control over layouts and colors, tmap also offers high-level styles, using tm_style_ functions (representing the second meaning of ‘style’ in the package). Some styles such as tm_style_cobalt() result in stylized maps while others such as tm_style_grey() make more subtle changes, as illustrated in Figure 9.11, created using code below (see 09-tmstyles.R): map_nza + tm_style_bw() map_nza + tm_style_classic() map_nza + tm_style_cobalt() map_nza + tm_style_col_blind() #&gt; Warning in tm_style_bw(): tm_style_bw is deprecated as of tmap version 2.0. #&gt; Please use tm_style(&quot;bw&quot;, ...) instead #&gt; Warning in tm_style_classic(): tm_style_classic is deprecated as of tmap #&gt; version 2.0. Please use tm_style(&quot;classic&quot;, ...) instead #&gt; Warning in tm_style_cobalt(): tm_style_white is deprecated as of tmap #&gt; version 2.0. Please use tm_style(&quot;cobalt&quot;, ...) instead #&gt; Warning in tm_style_col_blind(): tm_style_col_blind is deprecated as of #&gt; tmap version 2.0. Please use tm_style(&quot;col_blind&quot;, ...) instead Figure 9.11: Selected tmap styles: bw, classic, cobalt and color blind (from left to right). A preview of predefined styles can be generated by executing tmap_style_catalogue(). This creates a folder called tmap_style_previews containing (in tmap 2.0) 10 images. Each image, from tm_style_albatross.png to tm_style_white.png shows a faceted map of Europe in the corresponding style. Note: tmap_style_catalogue() takes some time to run. 9.2.6 Faceted maps Faceted maps, also referred to as ‘small multiples’, are composed of many maps arranged side-by-side, and sometimes stacked vertically. Facets enable the visualization of how spatial relationships change with respect to another variable, such as time. The changing populations of settlements, for example, can be represented in a faceted map with each panel representing the population at a particular moment in time. The time dimension could be represented via another aesthetic such as color. However, this risks cluttering the map because it will involve multiple overlapping points (cities do not tend to move over time!). Typically all individual facets in a faceted map contain the same geometry data repeated multiple times, once for each column in the attribute data (this is the default plotting method for sf objects, see 2). However, facets can also represent shifting geometries such as as the evolution of a point pattern over time. This use case of faceted plot is illustrated in Figure 9.12. urb_1970_2030 = urban_agglomerations %&gt;% filter(year %in% c(1970, 1990, 2010, 2030)) tm_shape(world) + tm_polygons() + tm_shape(urb_1970_2030) + tm_dots(size = &quot;population_millions&quot;) + tm_facets(by = &quot;year&quot;, nrow = 2, free.coords = FALSE) Figure 9.12: Faceted map showing the top 30 largest ‘urban agglomerations’ from 1970 to 2030 based on population projects by the United Nations. The preceding code chunk demonstrates key features of faceted maps created with tmap: Shapes that do not have a facet variable are repeated (the countries in world in this case). The by argument which varies depending on a variable (year in this case). nrow/ncol setting specifying the number of rows and columns that facets should be arranged into. The free.coords-parameter specifying if each map has its own bounding box. In addition to their utility for showing changing spatial relationships, faceted maps are also useful as the foundation for animated maps (see 9.3). 9.2.7 Inset maps An inset map is a smaller map rendered within or next to the main map. Inset maps usually cover an area with densely located phenomena that cannot be clearly visible at the original map scale. They could be used to focus on a smaller area in more detail (Figure 9.13) or to bring some non-contiguous regions closer to ease their comparison (Figure 9.14). In the example below, we create an inset map of the central part of the New Zealand’s Southern Alps. The first step is to define the area of interest, which can be done by creating a new spatial object, nz_region. nz_region = st_bbox(c(xmin = 1340000, xmax = 1450000, ymin = 5130000, ymax = 5210000), crs = st_crs(nz_height)) %&gt;% st_as_sfc() In the second step, we create a base map showing a lager area. It gives a context and helps to locate the area of interest. Importantly, this map needs to clearly indicate the location of the inset map, for example by stating its borders. nz_map = tm_shape(nz) + tm_polygons() + tm_shape(nz_height) + tm_symbols(shape = 2, col = &quot;red&quot;, size = 0.1) + tm_shape(nz_region) + tm_borders(lwd = 3) + tm_layout(frame = FALSE) The third step consists of the inset map creation. This is a place where the most important message is stated. nz_height_map = tm_shape(nz_elev, bbox = tmaptools::bb(nz_region)) + tm_raster(style = &quot;cont&quot;, palette = &quot;-Spectral&quot;, auto.palette.mapping = FALSE, legend.show = FALSE) + tm_shape(nz_height) + tm_symbols(shape = 2, col = &quot;red&quot;, size = 0.1) Finally, we combine the two maps. A viewport from the grid package can be used by stating a center location (x and y) and a size (width and height) of the inset map. library(grid) nz_map print(nz_height_map, vp = grid::viewport(0.3, 0.7, width = 0.4, height = 0.4)) Figure 9.13: Inset map showing the central part of the Southern Alps in New Zealand. Inset map can be save to file either by using a graphic device (see section 6.6) or the tmap_save() function and its arguments - insets_tm and insets_vp. Inset maps are also used to create one map of non-contiguous areas. Probably, the most often use example is a map of United States, which consists of the contiguous United States, Hawaii and Alaska. It is very important to find the best projection for each individual inset in this type of cases (see section 5.2 to learn more). We can use US National Atlas Equal Area for the map of the contiguous United States by putting its EPSG code in the projection argument of tm_shape(). us_states_map = tm_shape(us_states, projection = 2163) + tm_polygons() + tm_layout(frame = FALSE) The rest of our objects, hawaii and alaska, already have proper projections, therefore we just need to create two separate maps: hawaii_map = tm_shape(hawaii) + tm_polygons() + tm_layout(title = &quot;Hawaii&quot;, frame = FALSE, bg.color = NA, title.position = c(&quot;left&quot;, &quot;bottom&quot;)) alaska_map = tm_shape(alaska) + tm_polygons() + tm_layout(title = &quot;Alaska&quot;, frame = FALSE, bg.color = NA) The final map is created by combining and arranging these three maps: us_states_map print(hawaii_map, vp = viewport(x = 0.4, y = 0.1, width = 0.2, height = 0.1)) print(alaska_map, vp = viewport(x = 0.15, y = 0.15, width = 0.3, height = 0.3)) Figure 9.14: Map of the United States. The code presented above is very compact and allows for creation of many similar maps, however the map do not represent sizes and locations of Hawaii and Alaska well. You can see an alternative approach in the vignettes/us-map.Rmd file in the book’s GitHub repo, which tries to mitigate this issues. The main goal of this section is to present how to generate and arrange inset maps. The next step is to use the knowledge from the previous sections to improve the map style or to add another data layers. Moreover, the same skills can be applied to combine maps and plots. 9.3 Animated maps Faceted maps, described in 9.2.6, provide a way of showing how spatial relationships vary but the approach has disadvantages. Facets become tiny when many of them are squeezed into a single plot, potentially hiding any spatial relationships. Furthermore the fact that each facet is separated by space on the screen or page means that subtle differences between facets can be hard to detect. With an increasing proportion of communication happening via digital screens, animated maps are becoming more popular. Animated maps can even be useful for paper reports: you can always link readers to a web-page containing an animated (or interactive) version of a printed map to help make it come alive. Figure 9.15 is a simple example of an animated map. Unlike the faceted plot it does not squeeze multiple maps into a single screen and allows the reader to see how the spatial distribution of the worlds most populous agglomerations evolve over time (see the book’s website for the animated version). Figure 9.15: Animated map showing the top 30 largest ‘urban agglomerations’ from 1950 to 2030 based on population projects by the United Nations. The animated map illustrated in Figure 9.15 can be created using the same tmap techniques that generates faceted maps, demonstrated in section 9.2.6. There are two differences, however, related to arguments in tm_facets(): free.coords = FALSE, which maintains the map extent for each map iteration nrow = 1 and ncol = 1 ensure only one facet is created per year These additional arguments are demonstrated in the subsequent code chunk: urb_anim = tm_shape(world) + tm_polygons() + tm_shape(urban_agglomerations) + tm_dots(size = &quot;population_millions&quot;) + tm_facets(by = &quot;year&quot;, free.coords = FALSE, nrow = 1, ncol = 1) The resulting urb_anim represents a set of separate maps for each year. The final stage is to combine them and save the result as a .gif file with tmap_animation(). The following command creates the animation illustrated in Figure 9.15, with a few elements missing, that we will add-in during the exercises: tmap_animation(urb_anim, filename = &quot;urb_anim.gif&quot;, delay = 25) Another illustration of the power of animated maps is provided in Figure 9.16. This shows the development of states in United States, which first formed in the East and then incrementally to the West and finally into the interior. Code to reproduce this map can be found in the script 09-usboundaries.R. Figure 9.16: Animated map showing population growth and state formation and boundary changes in the United States, 1790-2010. 9.4 Interactive maps The easiest way to create an interactive map to view spatial data in R is with the mapview package, as illustrated in the following ‘one liner’: mapview::mapview(nz) Figure 9.17: Illustration of mapview in action. 9.5 Web mapping applications with shiny The interactive web maps demonstrated in section 9.4 can go far. Careful selection of layers to display, base-maps and pop-ups can be used to communicate the main results of many projects involving geocomputation. But the web mapping approach to interactivity has limitations: Although the map is interactive in terms of panning, zooming and clicking, the code is static, meaning the user interface is fixed. All map content is generally static in a web map, meaning that web maps cannot scale to handle large datasets easily. Additional layers of interactivity, such a graphs showing relationships between variables and ‘dashboards’ are difficult to create using the web-mapping approach. Overcoming these limitations involves going beyond static web mapping and towards geospatial frameworks and map servers. Products in this field include GeoDjango (which extends the Django web framework and is written in Python), MapGuide (a framework for developing web applications, largely written in C++) and GeoServer (a mature and powerful map server written in Java). Each of these (particularly GeoServer) is scalable, enabling maps to be served to thousands of people daily — assuming there is sufficient public interest in your maps! The bad news is that such server-side solutions require much skilled developer time to set-up and maintain, often involving teams of people with roles such as a dedicated geospatial database administrator (DBA). The good news is that web mapping applications can now be rapidly created using shiny, a package for converting R code into interactive web applications. This is thanks to its support for interactive maps via functions such as renderLeaflet(), documented on the Shiny integration section of RStudio’s leaflet website. This section some context, teaches the basics of shiny from a web mapping perspective and culminates in a full-screen mapping application in less than 100 lines of code. The way shiny works is well documented at shiny.rstudio.com. The two key elements of a shiny app reflect the duality common to most web application development: ‘front end’ (the bit the user sees) and ‘back end’ code. In shiny apps these elements are typically created in objects named ui and server within an R script named app.R, that lives in an ‘app folder’. This allows web mapping applications to be represented in a single file, such as the coffeeApp/app.R file in the book’s GitHub repo. In shiny apps these are often split into ui.R (short for user interface) and server.R files, naming conventions used by shiny-server, a server-side Linux application for serving shiny apps on public-facing websites shiny-server also serves apps defined by a single app.R file in an ‘app folder’. Before considering large apps it is worth seeing a minimal example, named ‘lifeApp’, in action.57 The code below defines and launches — with the command shinyApp() — a lifeApp, which provides an interactive slider allowing users to make countries appear with progressively lower levels of life expectancy (see Figure 9.18): ui = fluidPage( sliderInput(inputId = &quot;life&quot;, &quot;Life expectancy&quot;, 0, 80, value = 80), leafletOutput(outputId = &quot;map&quot;) ) server = function(input, output) { output$map = renderLeaflet({ leaflet() %&gt;% addProviderTiles(&quot;OpenStreetMap.BlackAndWhite&quot;) %&gt;% addPolygons(data = world[world$lifeExp &gt; input$life, ])}) } shinyApp(ui, server) Figure 9.18: Minimal example of a web mapping application created with shiny. The user interface (ui) of lifeApp is created by fluidPage(). This contains input ‘widgets’ — a sliderInput() in this case (many other *Input() functions are available) — and outputs, a leafletOutput() in this case. Elements added to a fluidPage() are arranged row-wise by default, explaining why the slider interface is placed directly above the map in Figure 9.18 (?column explains how to add content column-wise). The server side (server) is a function with input and output arguments. output is a list of objects containing elements generated by render*() function — renderLeaflet() which generates output$map in this case. Inputs elements such as input$life referred to in the server must relate to elements that exist in the ui — defined by inputId = &quot;life&quot; in the code above. The function shinyApp() combines both the ui and server elements and serves the results interactively via a new R process. When you move the slider in in Figure 9.18, you are actually causing R code to re-run, although this is hidden from view in the user interface. Building on this basic example and knowing where to find help (see ?shiny), the best way forward may be to stop reading and start programming! The recommended next step is to open the previously mentioned coffeeApp/app.R script in an IDE of choice, modify it and re-run it repeatedly. The example contains some of the components of a web mapping application implemented in shiny and should ‘shine’ a light on how they behave (pun intended). The coffeeApp/app.R script contains shiny functions that go beyond those demonstrated in the simple ‘lifeApp’ example. These include reactive() and observe() (for creating outputs that respond to the user interface — see ?reactive) and leafletProxy() (for modifying a leaflet object that has already been created). Such elements are critical to the creation of web mapping applications implemented in shiny. There are a number of ways to run a shiny app. For RStudio users the simplest way is probably to click on the ‘Run App’ button located in the top right of the source pane when an app.R, ui.R or server.R script is open. shiny apps can also be initiated by using runApp() with the first argument being the folder containing the app code and data: runApp(“coffeeApp”) in this case (which assumes a folder named coffeeApp containing the app.R script is in your working directory). You can also launch apps from a Unix command line with the command Rscript -e ‘shiny::runApp(“coffeeApp”)’. Experimenting with apps such as coffeeApp will build not only your knowledge of web mapping applications in R but your practical skills. Changing the contents of setView(), for example, will change the starting bounding box that the user sees when the app is initiated. Such experimentation should not be done at random, but with reference to relevant documentation, starting with ?shiny, and motivated by a desire to solve problems such as those posed in the exercises. shiny used in this way can make prototyping mapping applications faster and more accessible than ever before (deploying shiny apps is a separate topic beyond the scope of this chapter). Even if your applications are eventually deployed using different technologies, shiny undoubtedly allows web mapping applications to be developed in relatively few lines of code (60 in the case of coffeeApp). That does not stop shiny apps getting rather large. The Propensity to Cycle Tool (PCT) hosted at pct.bike, for example, is a national mapping tool funded by the UK’s Department for Transport. The PCT is used by dozens of people each day and has multiple interactive elements based on more than 1000 lines of code (Lovelace et al. 2017). While such apps undoubtedly take time and effort to develop, shiny provides a framework for reproducible prototyping that should aid the development process. One potential problem with the ease of developing prototypes with shiny is the temptation to start programming too early, before the purpose of the mapping application has been envisioned in detail. For that reason, despite advocating shiny, we recommend starting with the longer established technology of a pen and paper as the first stage for interactive mapping projects. This way your prototype web applications should be limited not by technical considerations but by your motivations and imagination. Figure 9.19: coffeeApp, a simple web mapping application for exploring global coffee production in 2016 and 2017. 9.6 Other mapping packages tmap provides a powerful interface for creating a wide range of static maps (section 9.2) and also supports interactive maps (section 9.4). But there are many other options for creating maps in R. The aim of this section is to provide a taster of some of these and pointers for additional resources: map making is a surprisingly active area of R package development so there is more to learn than can be covered here. The most mature option is to use plot() methods provided by core spatial packages sf and raster, covered in sections 2.1.3 and 2.2.2 respectively. What we have not mentioned in those sections was that plot methods for raster and vector objects can be combined, as illustrated in the subsequent code chunk which generates Figure 9.20. plot() has many options which can be explored by following links in the ?plot help page and the sf vignette sf5. g = st_graticule(nz, lon = c(170, 175), lat = c(-45, -40, -35)) plot(nz_water, graticule = g, axes = TRUE, col = &quot;blue&quot;) raster::plot(nz_elev / 1000, add = TRUE) plot(st_geometry(nz), add = TRUE) Figure 9.20: Map of New Zealand created with plot(). The legend to the left refers to elevation (1000 m above sea level). Since version 2.2.2, the tidyverse plotting package ggplot2 has supported sf objects with geom_sf(). The syntax is similar to that used by tmap: an initial ggplot() call is followed by one or more layers, that are added with + geom_*(), where * represents a layer type such as geom_sf() (for sf objects) or geom_points() (for points). ggplot2 plots graticules by default. The default settings for the graticules can be overridden using scale_x_continuous() and scale_y_continuous(). Other notable features include the use of unquoted variable names encapsulated in aes() to indicate which aesthetics vary and switching data sources using the data argument, as demonstrated in the code chunk below which creates Figure 9.21: library(ggplot2) g1 = ggplot() + geom_sf(data = nz, aes(fill = col)) + geom_sf(data = nz_height) + scale_x_continuous(breaks = c(170, 175)) g1 Figure 9.21: Map of New Zealand created with ggplot2. An advantage of ggplot2 is that it has a strong user-community and many add-on packages. Good additional resources can be found in the open source ggplot2-book and in the descriptions of the multitude of ‘ggpackages’ such as ggrepel and tidygraph. Another benefit of maps based on ggplot2 is that they can easily be given a level of interactivity when printed using the function ggplotly() from the plotly package. Try plotly::ggplotly(g1) for example, and compare the result with other plotly mapping functions described at blog.cpsievert.me. We have covered mapping with sf, raster and ggplot2 packages first because these packages are highly flexible, allowing for the creation of a wide range of static maps. Many other static mapping packages are more specific. Before we cover mapping packages for plotting a specific type of map (in the next paragraph), it is worth considering alternatives to the packages already covered for general-purpose static mapping (Table 9.1). Table 9.1: Selected mapping packages, with associated metrics. package title cartography Thematic Cartography ggplot2 Create Elegant Data Visualisations Using the Grammar of Graphics leaflet Create Interactive Web Maps with Leaflet mapview Interactive Viewing of Spatial Data in R plotly Create Interactive Web Graphics via ‘plotly.js’ rasterVis Visualization Methods for Raster Data tmap Thematic Maps Table 9.1 shows a range of mapping packages are available, and there are many others not listed in this table. Of note is cartography, which generates a range of unusual maps including choropleth, ‘proportional symbol’ and ‘flow’ maps, each of which is documented in the vignette cartography. 9.7 Exercises For these exercises we will create a new object, africa, using the world and worldbank_df datasets from the spData package (see chapter 3 to learn more about attribute operations): africa = world %&gt;% filter(continent == &quot;Africa&quot;) %&gt;% left_join(worldbank_df, by = &quot;iso_a2&quot;) %&gt;% select(name, subregion, gdpPercap, HDI, pop_growth) We will also use zion and nlcd datasets from spDataLarge: zion = st_read((system.file(&quot;vector/zion.gpkg&quot;, package = &quot;spDataLarge&quot;))) %&gt;% st_transform(4326) #&gt; Reading layer `zion&#39; from data source `/home/travis/R/Library/spDataLarge/vector/zion.gpkg&#39; using driver `GPKG&#39; #&gt; Simple feature collection with 1 feature and 11 fields #&gt; geometry type: POLYGON #&gt; dimension: XY #&gt; bbox: xmin: 303000 ymin: 4110000 xmax: 335000 ymax: 4150000 #&gt; epsg (SRID): NA #&gt; proj4string: +proj=utm +zone=12 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs tm_shape(africa) + tm_polygons(&quot;pop_growth&quot;) + tm_facets(by = &quot;name&quot;) 1. Sketch out on paper ideas for a web mapping app that could be used to make transport or land-use policies more evidence based: - In the city you live in, for a couple of users per day - In the country you live in, for dozens of users per day - Worldwide for hundreds of users per day and large data serving requirements 1. How would app design, deployment and project management decisions change as the scale of map deployment increases? 1. Update the code in coffeeApp/app.R so that instead of centering on Brazil the user can select which country to focus on: - Using textInput() - Using selectInput() References "],
["gis.html", "10 Bridges to GIS software 10.1 (R)QGIS 10.2 (R)SAGA 10.3 GRASS through rgrass7 10.4 When to use what? 10.5 Exercises", " 10 Bridges to GIS software An important feature of R is that users must use the command-line interface (CLI) to interact with the computer: you type commands and hit Enter to execute them interactively. The most popular open-source GIS software packages, by contrast, feature a prominent graphical user interface (GUI): you can interact with QGIS, SAGA and gvSIG by typing into (dockable) command-lines, but most users interact with such programs by ‘pointing and clicking their way through life’ as Gary Sherman puts it below (Sherman 2008):58 With the advent of ‘modern’ GIS software, most people want to point and click their way through life. That’s good, but there is a tremendous amount of flexibility and power waiting for you with the command line. Many times you can do something on the command line in a fraction of the time you can do it with a GUI. Gary Sherman is well-qualified to comment on this matter as the creator of QGIS, the world’s premier open source GUI-based GIS! The ‘CLI vs GUI’ debate is often adversarial and polarized but it does not have to be: both options are great if well chosen in accordance with your needs and tasks. The advantages of a good CLI such as that provided by R are numerous. Among others, a CLI: Facilitates the automation of repetitive tasks. We strongly dislike the manual execution of iterations, and would recommend to always think about how to solve such tasks programmatically. This way, you most likely discover new programming features and concepts, i.e., you learn and enhance your skill set. By contrast, what are you learning from executing the same tasks a 1000 times? As a nice side-effect, automation is surely more effective and less error-prone. Ensures transparency and reproducibility (which also is the backbone of good scientific practice). In short, it is easier to share your R script than explain a sequence of 10+ ‘points and clicks’. Encourages extending existing software by making it easy to modify and extend functions. If you are missing an algorithm you need, the CLI gives you the freedom to write your own! Is the way to (geo-)data science. Professional and advanced technical skills will certainly enhance your career prospects, and are in dire need across a wide range of disciplines. Is fun, but admittedly that is a subjective argument. … On the other hand, GUI-based GIS systems (particularly QGIS) are also advantageous. For instance, think of: The GUI. The really user-friendly graphical interface spares the user from programming. Though you probably wouldn’t read the book if this were your main objective. Digitizing and all related tools (trace, snap, topological rules, etc.). Note that there is also the new mapedit package but its intention is to allow the quick editing of a few spatial features, and not professional, large-scale cartographic digitizing. Georeferencing Stereoscopic mapping (e.g., LiDAR and structure from motion) The built-in geodatabase management system often integrated in Desktop GIS (ArcGIS, GRASS GIS) and all related advantages such as object relational modeling, topology, fast (spatial) querying, etc. Map production, in case you only want to create a beautiful map once. If you have to produce it over and over again, then maybe the CLI approach is better. Zooming and dragging on WMS (though this is also possible with mapview and leaflet) … This general overview already points out the differences between R’s CLI and desktop GIS. However, there is more: dedicated GIS software provides hundreds of geoalgorithms that are simply missing in R. The good news is that ‘GIS bridges’ enable the access to these with the comfort of the R command line.59 The R language was originally designed as an interface to and extension of other languages, especially C and FORTRAN, to enable access to statistical algorithms in a user-friendly and intuitive read-evaluate-print loop (REPL) (Chambers 2016). R was not originally intended to be a GIS. This makes the breadth of R’s geospatial capabilities astonishing to many who are unaware of its ability to replicate established GIS software for many operations on spatial data. There are some domains where R can now outperform desktop GIS including spatial statistical modeling, online interactive visualization and the generation of animated or faceted maps. Instead of implementing existing GIS algorithms in R, it makes sense to avoid ‘reinventing the wheel’ by taking advantage of R’s ability to interface with other languages (especially C++, which is used for much low-level and high-performance GIS work). Using compiled code for new geoprocessing functionality (particularly with the help of the excellent Rcpp package) could form the basis of new R packages, building on the success of sf. However, there is already a wide range of algorithms that can be accessed via R’s interfaces to dedicated GIS software. It makes sense to understand these before moving to develop your own optimized algorithms. For this reason this chapter focuses on ‘bridges’ to the mature GIS products QGIS (via the package RQGIS), SAGA (RSAGA) and GRASS (rgrass7) from within R. Obviously, we here focus on open-source software solutions, however, there is also a bridge to the commercial GIS leader ArcGIS through the RPyGeo package. And the so-called R-ArcGIS bridge allows to use R from within ArcGIS. As a final note, we would like to point out that aside from interfaces to desktop GIS there are also interfaces to geospatial libraries such as GDAL (gdalUtils, rgdal, sf) and GEOS (rgeos, sf). By the end of the chapter you should have a working knowledge of the functionality such packages open up, and a more nuanced understanding of the ‘CLI vs GUI’ debate. As mentioned in chapter 1, doing GIS at the command-line makes it more reproducible, in-line with the principles of Geographic Data Science. 10.1 (R)QGIS QGIS is one of the most popular open-source GIS (Table 10.1; Graser and Olaya 2015). Its main advantage lies in the fact that it provides a unified interface to several other open-source GIS. Table 10.1: Comparison between three open-source GIS. Hybrid refers to the support of vector and raster operations. GIS first release no. functions support GRASS 1984 &gt;500 hybrid QGIS 2002 &gt;1000 hybrid SAGA 2004 &gt;600 hybrid Note: a Comparing downloads of different providers is rather difficult (see http://spatialgalaxy.net/2011/12/19/qgis-users-around-the-world/), and here also useless since every Windows QGIS download automatically also downloads SAGA and GRASS. First and foremost, this means that you have access to GDAL/OGR, GRASS and SAGA through QGIS but also to other third-party providers such as TauDEM, Orfeo Toolbox and Lastools (tools for LiDAR data) (Graser and Olaya 2015). To run all these geoalgorithms (frequently more than 1000 depending on your set up) outside of the QGIS GUI, QGIS provides a Python API. RQGIS establishes a tunnel to this Python API through the reticulate package. Basically, functions set_env and open_app are doing this. Note that it is optional to run set_env and open_app since all functions depending on their output will run them automatically if needed. Before running RQGIS you have to make sure to have installed QGIS and all its (third-party) dependencies such as SAGA and GRASS. To help you with the installation process, please follow the steps as detailed in vignette(&quot;install_guide&quot;, package = &quot;RQGIS&quot;) for several platforms (Windows, Linux, MacOS). library(RQGIS) set_env() open_app() Leaving the path-argument of set_env unspecified will search the computer for a QGIS installation. Hence, it is faster to specify explicitly the path to your QGIS installation. Subsequently, open_app sets all paths necessary to run QGIS from within R, and finally creates a so-called QGIS custom application http://docs.qgis.org/testing/en/docs/pyqgis_developer_cookbook/intro.html#using-pyqgis-in-custom-applications. We are now ready for some QGIS geoprocessing from within R! First of all, we load some data from the spData-package, namely the boroughs of London (lnd) and cycle hire points in London (cycle_hire). library(spData) In chapter 2, we already learned how to do a spatial overlay using the sf-package. Of course, any GIS is also able to perform spatial overlays. Here, we would like to know how many cycle points we can find per borough. First of all, we need to come up with the name of the function in QGIS. find_algorithms lets you search all QGIS geoalgorithms with the help of regular expressions. Here, we assume that the short description of the function contains first the word “point” and secondly somewhere later also the word “poly”. If you have no clue at all what to look for you can leave the search_term-argument empty which will return a list of all available QGIS geoalgorithms. If you also want to have a short description for each geoalgorithm, set the name_only-parameter to FALSE. find_algorithms(&quot;points.*poly&quot;, name_only = TRUE) Now that we know the name of the function (qgis:countpointsinpolygon), we wonder how we can use it. get_usage returns all function parameters and default values. open_help lets you access the corresponding online help. alg = &quot;qgis:countpointsinpolygon&quot; get_usage(alg) open_help(alg) Finally, we can let QGIS do the work. Note that the workhorse function run_qgis accepts R named arguments, i.e., you can specify the parameter names as returned by get_usage as you would do in any other regular R function. Note also that run_qgis accepts spatial objects residing in R’s global environment as input (here: lnd and cycle_hire). But of course, you could also specify paths to shapefiles stored on disk. Setting the load_output to TRUE automatically loads the QGIS output into R. Since we only indicated the name of the output (“cycle.shp”), run_qgis saves the output to a temporary folder as returned by tempdir(), and loads it into R as an sf-object. bike_points = run_qgis(alg, POLYGONS = lnd, POINTS = cycle_hire, FIELD = &quot;no_bikes&quot;, OUTPUT = &quot;cycle.shp&quot;, load_output = TRUE) summary(bike_points$no_bikes) sum(bike_points$no_bikes &gt; 0) In case you leave some parameters of a geoalgorithm unspecified, run_qgis will automatically use the default values as arguments if available. To find out about the default values, run get_args_man. get_args_man(alg) In this case the output tells us, had we left the FIELDS-parameter unspecified, our output (attribute) field would have been named “NUMPOINTS” (instead of “no_bikes”). Other notes: Leaving the output parameter(s) unspecified, saves the resulting QGIS output to a temporary folder created by QGIS. run_qgis prints these paths to the console after successfully running the QGIS engine. If the output consists of multiple files and you have set load_output to TRUE, run_qgis will return a list with each element corresponding to one output file. To learn more about RQGIS please refer to the (hopefully) forthcoming paper (cite). 10.2 (R)SAGA Similar to QGIS, the System for Automated Geoscientific Analyses (SAGA; Table 10.1) provides the possibility to run SAGA modules from Python (SAGA Python API). In addition, there is also a command line interface (saga_cmd.exe) to execute SAGA modules (see also https://sourceforge.net/p/saga-gis/wiki/Executing%20Modules%20with%20SAGA%20CMD/). RSAGA uses the latter to run SAGA from within R. Though SAGA is a hybrid GIS, its main focus has been on raster processing, and here particularly on digital elevation models (soil properties, terrain attributes, climate parameters). Hence, SAGA is especially good at the fast processing of large (high-resolution) rasters datasets (Conrad et al. 2015). Therefore, we will introduce RSAGA with a raster and use case from Muenchow, Brenning, and Richter (2012). Specifically, we would like to compute the SAGA wetness index from a digital elevation model. First of all, we need to make sure that RSAGA will find SAGA on the computer when called. For this, all RSAGA functions using SAGA in the background make use of rsaga.env(). Usually, rsaga.env() will detect SAGA automatically by searching several likely directories (see its help for more information). However, we have ‘hidden’ SAGA in the OSGEO4W-installation, a location rsaga.env() does not search automatically. linkSAGA searches your computer for a valid SAGA installation. If it finds one, it adds the newest version to the PATH environment variable thereby making sure that rsaga.env runs successfully. library(RSAGA) library(link2GI) saga = linkSAGA() rsaga.env() Secondly, we need to write the digital elevation model to a SAGA-format. Note that calling data(landslides) attaches two object to the global environment - dem, a digital elevation model in the form of a list, and landslides, a data.frame containing observations representing the presence or absence of a landslide: data(landslides) write.sgrd(data = dem, file = file.path(tempdir(), &quot;dem&quot;), header = dem$header) The organization of SAGA is modular. Libraries contain so-called modules, i.e., geoalgorithms. To find out which libraries are available, run: tail(rsaga.get.libraries(), 10) Instead of presenting all available libraries we have shown only the last ten. We choose the library ta_hydrology (ta is the abbreviation for terrain analysis). Subsequently, we can access the available modules of a specific library (here: ta_hydrology) as follows: rsaga.get.modules(libs = &quot;ta_hydrology&quot;) Similarly to RQGIS::get_usage(), rsaga.get.usage() prints the function parameters of a specific geoalgorithm, e.g., the SAGA Wetness Index, to the console. rsaga.get.usage(lib = &quot;ta_hydrology&quot;, module = &quot;SAGA Wetness Index&quot;) Finally, you can run SAGA from within R using RSAGA’s geoprocessing workhorse function rsaga.geoprocessor(). The function expects a parameter-argument list in which you have specified all necessary parameters. params = list(DEM = file.path(tempdir(), &quot;dem.sgrd&quot;), TWI = file.path(tempdir(), &quot;twi.sdat&quot;)) rsaga.geoprocessor(lib = &quot;ta_hydrology&quot;, module = &quot;SAGA Wetness Index&quot;, param = params) To facilitate the access to the SAGA interface, RSAGA frequently provides user-friendly wrapper-functions with meaningful default values (see RSAGA documentation for examples, e.g., ?rsaga.wetness.index). So the function call for calculating the ‘SAGA Wetness Index’ becomes as simple as: rsaga.wetness.index(in.dem = file.path(tempdir(), &quot;dem&quot;), out.wetness.index = file.path(tempdir(), &quot;twi&quot;)) Of course, we would like to inspect our result visually (Figure ??). To load and plot the SAGA output file, we use the raster package. library(raster) twi = raster::raster(file.path(tempdir(), &quot;twi.sdat&quot;)) plot(twi, col = RColorBrewer::brewer.pal(n = 9, name = &quot;Blues&quot;)) # or using mapview # proj4string(twi) = paste0(&quot;+proj=utm +zone=17 +south +ellps=WGS84 +towgs84=&quot;, # &quot;0,0,0,0,0,0,0 +units=m +no_defs&quot;) # mapview(twi, col.regions = RColorBrewer::brewer.pal(n = 9, &quot;Blues&quot;), # at = seq(cellStats(twi, &quot;min&quot;) - 0.01, cellStats(twi, &quot;max&quot;) + 0.01, # length.out = 9)) You can find a much more extended version of the here presented example in the RSAGA vignette vignette(&quot;RSAGA-landslides&quot;). This example includes statistical geocomputing, i.e., it uses a GIS to derive terrain attributes as predictors for a non-linear Generalized Linear Model (GAM) to predict spatially landslide susceptibility (Muenchow, Brenning, and Richter 2012). The term statistical geocomputation emphasizes the strength of combining R’s data science power with the geoprocessing power of a GIS which is at the very heart of building a bridge from R to GIS. 10.3 GRASS through rgrass7 The U.S. Army - Construction Engineering Research Laboratory (USA-CERL) created the core of the hybrid raster-vector Geographical Resources Analysis Support System (GRASS) (Table 10.1; Neteler and Mitasova 2008) from 1982 to 1995. Academia continued this work since 1997. Similar to SAGA, GRASS focussed on raster processing in the beginning while only later, since GRASS 6.0, adding advanced vector functionality (Bivand, Pebesma, and Gómez-Rubio 2013). We will introduce rgrass7 with one of the most interesting problems in GIScience - the traveling salesman problem. Suppose a traveling salesman would like to visit 24 customers while covering the shortest distance possible. Additionally, the salesman would like to set out his journey at home, and come back to it after having finished all customer visits. There is a single best solution to this problem, however, to find it, is even for modern computers (mostly) impossible (Longley 2015). In our case, the number of possible solution correspond to (25 - 1)! / 2 possible solutions, i.e. the factorial of 24 divided by 2 (since we do not differentiate between forward or backward direction). Even if one iteration can be done in a nanosecond this still corresponds to 9.83710^{6} years. Luckily, there are clever, almost optimal solutions which run in a tiny fraction of this inconceibable amount of time. GRASS GIS provides one of these solutions (for more details see v.net.salesman). In our use case, we would like to find the shortest path between the first 25 bicycle stations (instead of customers) on London’s streets. library(spData) data(cycle_hire) points = cycle_hire[1:25, ] Aside from the cycle hire points data, we will need the OpenStreetMap data of London. We download it with the help of the osmdata package (see also section 6.2). We constrain the download of the street network (in OSM language called highway) to the bounding box of the cycle hire data, and attach the corresponding data as an sf-object. osmdata_sf returns a list with several spatial objects (points, lines, polygons, etc.). Here, we will only keep the line objects. To learn more about how to use the osmdata-package, please refer to https://ropensci.github.io/osmdata/. library(sf) library(osmdata) b_box = sf::st_bbox(cycle_hire) streets = opq(b_box) %&gt;% add_osm_feature(key = &quot;highway&quot;) %&gt;% osmdata_sf() %&gt;% `[[`(., &quot;osm_lines&quot;) Now that we have the data, we can go on and initiate a GRASS session, i.e., we have to create a GRASS geodatabase. The GRASS geodatabase system is based on SQLite. Consequently, different users can easily work on the same project, possibly with different read/write permissions. However, one has to set up this geodatabase (also from within R), and users used to a GIS GUI popping up by one click might find this process a bit intimidating in the beginning. First of all, the GRASS database requires its own directory, and contains a location (see the GRASS GIS Quickstart document and GRASS GIS Database help pages at grass.osgeo.org for further information). The location in turn simply contains the geodata for one project. Within one location several mapsets can exist, and typically refer to different users. PERMANENT is a mandatory mapset, and created automatically. It stores the projection, the spatial extent and the default resolution for raster data. In order to share spatial data with all users of a project, the database owner can add spatial data to the PERMANENT mapset. Please refer to Neteler and Mitasova (2008) and the GRASS GIS quick start for more information on the GRASS geodatabase system. You have to set up a location and a mapset if you want to use GRASS from within R. First of all, we need to find out if and where GRASS7 is installed on the computer. library(link2GI) link = findGRASS() #&gt; Warning in link2GI::searchGRASSX(MP = searchLocation): Did not find any #&gt; valid GRASS installation at mount point /usr link is a data.frame which contains in its rows the GRASS 7 installations on your computer. Here, we will use a GRASS7 installation. If you have not installed GRASS7 on your computer, we recommend that you do so now. Assuming that we have found a working installation on your computer, we use the corresponding path in initGRASS. Additionally, we specify where to store the geodatabase (gisDbase), name the location london, and use the PERMANENT mapset. library(rgrass7) # find a GRASS7 installation, and use the first one ind = grep(&quot;7&quot;, link$version)[1] # next line of code only necessary if we want to use GRASS as installed by # OSGeo4W. Among others, this adds some paths to PATH, which are also needed # for running GRASS. link2GI::paramGRASSw(link[ind, ]) grass_path = ifelse(!is.null(link$installation_type) &amp;&amp; link$installation_type[ind] == &quot;osgeo4W&quot;, file.path(link$instDir[ind], &quot;apps/grass&quot;, link$version[ind]), link$instDir) initGRASS(gisBase = grass_path, # home parameter necessary under UNIX-based systems home = tempdir(), gisDbase = tempdir(), location = &quot;london&quot;, mapset = &quot;PERMANENT&quot;, override = TRUE) Subsequently, we define the projection, the extent and the resolution. execGRASS(&quot;g.proj&quot;, flags = c(&quot;c&quot;, &quot;quiet&quot;), proj4 = sf::st_crs(streets)$proj4string) b_box = sf::st_bbox(streets) execGRASS(&quot;g.region&quot;, flags = c(&quot;quiet&quot;), n = as.character(b_box[&quot;ymax&quot;]), s = as.character(b_box[&quot;ymin&quot;]), e = as.character(b_box[&quot;xmax&quot;]), w = as.character(b_box[&quot;xmin&quot;]), res = &quot;1&quot;) Once you are familiar how to set up the GRASS environment, it becomes tedious to do so over and over again. Luckily, linkGRASS7 of the link2GI packages lets you do it with one line of code. The only thing you need to provide is a spatial object which determines the projection and the extent of the geodatabase. First, linkGRASS7 finds all GRASS installations on your computer. Since we have set ver_select to TRUE, we can interactively choose one of the found GRASS-installations. If there is just one installation, the linkGRASS7 automatically chooses this one. Secondly, linkGRASS7 establishes a connection to GRASS7. link2GI::linkGRASS7(streets, ver_select = TRUE) Before we can use GRASS geoalgorithms, we need to add data to GRASS’s spatial database. Luckily, the convenience function writeVECT does this for us. (Use writeRast in the case of raster data.) In our case we add the street and cycle hire point data while using only the first attribute column, and name them also streets and points. writeVECT(as(streets[, 1], &quot;Spatial&quot;), vname = &quot;streets&quot;) writeVECT(SDF = as(points[, 1], &quot;Spatial&quot;), vname = &quot;points&quot;) To perform our network analysis, we need a topological clean street network. GRASS’s v.clean takes care of the removal of duplicates, small angles and dangles, among others. Here, we break lines at each intersection to ensure that the subsequent routing algorithm can actually turn right or left at an intersection, and save the output in a GRASS object named streets_clean. Probably a few of our cycling station points do not exactly lie on a street segment. However, to find the shortest route between them, we need to connect them to the nearest streets segment. v.net’s connect-operator does exactly this. We save its output in streets_points_con. execGRASS(cmd = &quot;v.clean&quot;, input = &quot;streets&quot;, output = &quot;streets_clean&quot;, tool = &quot;break&quot;, flags = &quot;overwrite&quot;) execGRASS(cmd = &quot;v.net&quot;, input = &quot;streets_clean&quot;, output = &quot;streets_points_con&quot;, points = &quot;points&quot;, operation = &quot;connect&quot;, threshold = 0.001, flags = c(&quot;overwrite&quot;, &quot;c&quot;)) The resulting clean dataset serves as input for the v.net.salesman-algorithm, which finally finds the shortest route between all cycle hire stations. center_cats requires a numeric range as input. This range represents the points for which a shortest route should be calculated. Since we would like to calculate the route for all cycle stations, we set it to 1-25. To access the GRASS help page of the traveling salesman algorithm, run execGRASS(&quot;g.manual&quot;, entry = &quot;v.net.salesman&quot;). execGRASS(cmd = &quot;v.net.salesman&quot;, input = &quot;streets_points_con&quot;, output = &quot;shortest_route&quot;, center_cats = paste0(&quot;1-&quot;, nrow(points)), flags = c(&quot;overwrite&quot;)) To visualize our result, we import the output layer into R, and visualize it with the help of the mapview package (Figure ??). library(mapview) route = readVECT(&quot;shortest_route&quot;) mapview(route) + mapview(points) Further notes: Please note that we have used GRASS’s geodatabase (based on SQLite) which allows faster processing. That means we have only exported spatial data in the beginning. Then we created new objects but only imported the final result back into R. To find out which datasets are currently available, run execGRASS(&quot;g.list&quot;, type = &quot;vector,raster&quot;, flags = &quot;p&quot;). Of course, we could have also accessed an already existing GRASS geodatabase from within R. Prior to importing data into R, you might want to perform some (spatial) subsetting. Use v.select and v.extract for vector data. db.select lets you select subsets of the attribute table of a vector layer without returning the correponding geometry. You can start R also from within a running GRASS session (for more information please refer to Bivand, Pebesma, and Gómez-Rubio 2013 and this wiki). Refer to the excellent GRASS online help or execGRASS(&quot;g.manual&quot;, flags = &quot;i&quot;) for more information on each available GRASS geoalgorithm. If you would like to use GRASS 6 from within R, use the R package spgrass6. 10.4 When to use what? To recommend a single R-GIS interface is hard since the usage depends on personal preferences, the tasks at hand and your familiarity with different GIS. The latter means if you have already preferred the GUI of a certain GIS, you are quite likely to use the corresponding interface. Certainly, RQGIS is an appropriate choice for most use cases. Its main advantages are: An unified access to several GIS, and therefore the provision of &gt;1000 geoalgorithms. Of course, this includes duplicated functionality, e.g., you can perform overlay-operations using QGIS-, SAGA- or GRASS-geoalgorithms. The automatic data format conversions. For instance, SAGA uses .sdat grid files and GRASS uses its own database format but QGIS will handle the corresponding conversions for you on the fly. RQGIS can also handle spatial objects residing in R as input for geoalgorithms, and loads QGIS output automatically back into R if desired. Its convenience functions to support the access of the online help, R named arguments and automatic default value retrieval. Please note that rgrass7 inspired the latter two features. Currently (but this might change), RQGIS supports newer SAGA (2.3.1) versions than RSAGA (2.2.3). However, there are use cases when you certainly should use one of the other R-GIS bridges. QGIS only provides access to a subset of GRASS and SAGA functionality. Therefore, to use the complete set of SAGA and GRASS functions, stick with RSAGA and rgrass7. When doing so, make advantage of RSAGA’s numerous user-friendly functions. Note also, that RSAGA offers native R functions for geocomputation such as multi.local.function, pick.from.grid and many more. Finally, if you need topological correct data and/or geodatabase-management functionality, we recommend the usage of GRASS. In addition, if you would like to run simulations with the help of a geodatabase (Krug, Roura-Pascual, and Richardson 2010), use rgrass7 directly since RQGIS always starts a new GRASS session for each call. 10.5 Exercises Create two overlapping polygons (poly_1 and poly_2) with the help of the sf-package (see chapter 2). Calculate the intersection using: RQGIS, RSAGA and rgrass7 sf Run data(dem, package = &quot;RQGIS&quot;) and data(random_points, package = &quot;RQGIS&quot;). Select randomly a point from random_points and find all dem pixels that can be seen from this point (hint: viewshed). Visualize your result. For example, plot a hillshade, and on top of it the digital elevation model, your viewshed output and the point. Additionally, give mapview a try. References "],
["raster-vector.html", "11 Raster-vector interactions Prerequisites 11.1 Introduction 11.2 Raster cropping 11.3 Raster extraction 11.4 Rasterization 11.5 Spatial vectorization 11.6 Exercises", " 11 Raster-vector interactions Prerequisites This chapter requires the following packages: library(sf) library(raster) library(tidyverse) library(spData) library(spDataLarge) 11.1 Introduction In this section we will focus on the interactions between a raster and vector model. It includes four main techniques. Raster cropping and masking using vector objects is covered in section 11.2. In section 11.3, we will describe how raster values can be extract using different types of vector data - points, lines and polygons. Finally, sections 11.4 and 11.5 show how to convert vectors into rasters and raster images into vectors. We will illustrate the above concepts and suggest when they can be useful. 11.2 Raster cropping Many spatial analysis involve integrating data from many different sources, e.g. remote sensing images (rasters) and administrative boundaries (vectors). This often means that the extent of a raster is larger than the actual area of interest. Two techniques, raster cropping and raster masking, are used to unify the analyzed area. They could be vital for many projects as they reduce an object size and therefore decrease computational times needed for the following calculations. Additionally, they are often used to prepare the data before creating maps. We are going to illustrate raster cropping and masking using the srtm object, a elevation raster of the Southwestern Utah, and the zion object, a vector representing the area of the Zion National Park (Figure 11.1:A). Both objects should have the same projection, therefore we need to transform zion to fit the projection of srtm before raster cropping or masking (see section 5.2 to learn more about spatial data reprojection): srtm = raster((system.file(&quot;raster/srtm.tif&quot;, package = &quot;spDataLarge&quot;))) zion = st_read((system.file(&quot;vector/zion.gpkg&quot;, package = &quot;spDataLarge&quot;))) %&gt;% st_transform(4326) The role of the crop function is to decrease the extent of a raster based on the extent of another spatial object. It this example, we crop the elevation data to the extent of the Zion National Park area (Figure 11.1:B): srtm_cropped = crop(srtm, as(zion, &quot;Spatial&quot;)) The goal of the mask function is a little bit different - it keeps the raster values only in the area of interest, while the values outside of the analyzed area are set to NA. The code below masks every cell outside of the the Zion National Park boundaries (Figure 11.1:C): srtm_masked = mask(srtm_cropped, zion) The mask function also can be modified to give different results. For example a value of mask (NA) can be changed with the maskvalue argument and the inverse argument set to TRUE masks an area of interest keeping everything else intact. You can learn more in the function’s help file - ?mask. Figure 11.1: Illustration of raster cropping (center) and raster masking (right). 11.3 Raster extraction Raster extraction is a process of pulling out values from rasters based on the locations from vector data. It behaves differently depending on the type of secondary data (points, lines or polygons) and selected arguments. We will present some of the most often use cases below using the raster::extract() function. The reverse process of transferring vector data values into rasters is usually done by rasterization (see section 11.4). The simplest example of raster extraction is when values of raster cells are extracted based on points coordinates. The zion_points dataset consists of 30 points located in the Zion National Park (Figure 11.2). They could represent places where soils properties were measured and we want to know what is the elevation of each point. In this case, we just need to add a new column (elevation) to the point dataset that would store values extracted from the srtm object: zion_points$elevation = raster::extract(srtm, zion_points) In extraction by points it is also possible to provide the radius of a buffer (in meters) around each point (the buffer argument). This allows for similar operations to the extraction by polygon (see examples below). Figure 11.2: Locations of points used for raster extraction. The second example shows raster extraction along a line or lines. For this purpose, we will create a simple line going from northwest to southeast of the Zion National Park (11.3:A): zion_transect = st_sfc(st_linestring(rbind(c(-113.2, 37.45), c(-112.9, 37.2)))) %&gt;% st_sf() Importantly, it does not need to be a straight line. Try to imagine that you are planing to go on a hike - you can extract elevation along your proposed path. To extract a raster by line, the along argument needs to be set to TRUE. We also add identifiers (cellnumbers = TRUE) and convert a new object to data.frame: transect_df = raster::extract(srtm, zion_transect, along = TRUE, cellnumbers = TRUE) %&gt;% data.frame() This would allow us to estimate the distances along measurements on the line. However, firstly we need to extract the coordinates of measurements and based on them we can calculate the distances using the geosphere::distm() function. transect_coords = xyFromCell(srtm, transect_df$cell) transect_df$dist = geosphere::distm(transect_coords)[, 1] The final data.frame can be used to create a plot in Figure 11.3:B. Figure 11.3: Location of a line used for raster extraction (left) and the elevation along this line (right). The last group of examples is an extraction of raster values that are covered by polygons. Polygons are usually larger than a single cell, therefore this kind of extraction would return many raster values for each polygon. In the below output, ID represents the row number in the polygon and srtm is the elevation value extracted from the raster: zion_srtm_values = raster::extract(x = srtm, y = zion, df = TRUE) Extract by polygon also allows for more complex analyses, such as calculating statistics of continuous raster’s values (11.4:A) or counting occurrences of categorical raster’s classes (11.4:B) for each polygon. These outputs are used to characterize a single region or to compare many regions. Continuous raster’s values can be described by summary statistics. In the code below, we create a new vector, our_stats, that contains three statistics functions, and name them: our_stats = c(min, mean, max) names(our_stats) = c(&quot;minimum&quot;, &quot;mean&quot;, &quot;maximum&quot;) We use the map_dfr() function to loop through our statistics functions and extract their values from the raster. Next, the spread() function reorganizes the data. In the final step, we join our polygon dataset and the values of statistics. zion_srtm_df = our_stats %&gt;% map_dfr(raster::extract, x = srtm, y = zion, df = TRUE, .id = &quot;stat&quot;) %&gt;% spread(stat, srtm) zion_srtm_new = bind_cols(zion, zion_srtm_df) Description of categorical rasters requires a different approach, as we cannot calculate the mean between category of “Forest” and “Water”. However, it is possible to count the number of occurrences of each class. To illustrate this, we will use a land cover data nlcd(11.4:B). Firstly, we need to extract all of the values in our polygon to a new data.frame: data(nlcd) zion_nlcd = raster::extract(nlcd, zion, df = TRUE) The new output, zion_nlcd, can then be used to count the number of occurrences of each class. Finally, we need to reshape the output to have one row per polygon and join the extracted information with the polygon dataset: zion_count = count(zion_nlcd, ID, layer) zion_nlcd_df = spread(zion_count, layer, n, fill = 0) zion_nlcd_new = bind_cols(zion, zion_nlcd_df) Figure 11.4: Area used for continuous (left) and categorical (right) raster extraction. The extract function is well suited for a small to medium-sized data, however it is not very efficient for large datasets. There are several alternatives to consider in those cases. Firstly, raster extraction could be parallelized when many vector objects are used. Instead of using just one CPU thread for the whole operation, vector objects could be split into several groups. Next, extraction would be performed independently for each group and the results would be combined. See the ?raster::clusterR() for more information. Secondly, the velox package (Hunziker 2017) provides a fast method for extracting raster data that fits in the RAM memory. This process is described in detail at https://hunzikp.github.io/velox/extract.html. Finally, it could be worthwhile to consider using R-GIS bridges. For example, efficient calculation of polygon in raster statistics exist in SAGA and can be called using RQGIS (saga:gridstatisticsforpolygons). To learn more visit chapter 10. 11.4 Rasterization Rasterization is a conversion from vector objects into rasters. Usually, the output raster is used for quantitative analysis (e.g. analysis of terrain) or modeling. The rasterize() function takes a vector object and converts it into a raster with extent, resolution and CRS determined by another raster object. Parameters of a template raster have big impact on rasterization output – coarse resolution could not capture all of important spatial objects, while high resolution could increase computation times. However, there is no simple rules for parameters selection as it depends on the input data and rasterization purpose. For the first group of examples, we will use a template raster having the same extent and CRS as cycle_hire_osm_projected and spatial resolution of 1000 meters: cycle_hire_osm_projected = st_transform(cycle_hire_osm, 27700) raster_template = raster(extent(cycle_hire_osm_projected), resolution = 1000, crs = st_crs(cycle_hire_osm_projected)$proj4string) Rasterization is a very flexible operation and gives different results based not only on a template raster, but also on the type of input vector (e.g. points, polygons) and given arguments. Let’s try three different approaches to rasterize points - cycle hire locations across London (Figure 11.5:A). The simplest case is when we want to create a raster containing areas with cycle hire points (also known as a presence/absence raster). In this situation, rasterize() expects only three arguments - an input vector data, a raster template, and a value to be transferred to all non-empty cells (Figure 11.5:B). ch_raster1 = rasterize(cycle_hire_osm_projected, raster_template, field = 1) rasterize() also could take a fun argument which specifies how attributes are transferred to the raster object. For example, the fun = &quot;count&quot; argument counts the number of points in each grid cell (Figure 11.5:C). ch_raster2 = rasterize(cycle_hire_osm_projected, raster_template, field = 1, fun = &quot;count&quot;) The new output, ch_raster2, shows the number of cycle hire points in each grid cell. However, the cycle hire locations have different numbers of bicycles, which is described by the capacity variable. We need to select a field (&quot;capacity&quot;) and a function (sum) to determine a cycle hire capacity in each grid cell (Figure 11.5:D). In the same way, another statistics could be calculated such as an average capacity for each grid cell, etc. ch_raster3 = rasterize(cycle_hire_osm_projected, raster_template, field = &quot;capacity&quot;, fun = sum) Figure 11.5: Examples of point’s rasterization. Additionally, we will illustrate polygons and lines rasterizations using California’s polygons (california) and borders (california_borders). A template raster here will have the resolution of a 0.5 degree: california = dplyr::filter(us_states, NAME == &quot;California&quot;) california_borders = st_cast(california, &quot;MULTILINESTRING&quot;) raster_template2 = raster(extent(california), resolution = 0.5, crs = st_crs(california)$proj4string) All cells that are touched by a line get a value in a line rasterization (Figure 11.6:A). california_raster1 = rasterize(california_borders, raster_template2) On the other hand, polygon rasterization is based on the positions of cells’ centers (points on Figure 11.6:B). Values are only given when the center of the cell lies inside of the input polygon (Figure 11.6:B). california_raster2 = rasterize(california, raster_template2) It is also possible to use the field or fun arguments for lines and polygons rasterizations. Figure 11.6: Examples of line and polygon rasterizations. While rasterize works well for most cases, it is not performance optimized. Fortunately, there are several alternatives, including the fasterize::fasterize() and gdalUtils::gdal_rasterize(). The former is much (100 times+) faster than rasterize() but is currently limited to polygon rasterization. The latter is part of GDAL and therefore requires a vector file (instead of an sf object) and rasterization parameters (instead of a Raster* template object) as inputs.60 11.5 Spatial vectorization Spatial vectorization is the counterpart of rasterization 11.4, and hence the process of converting continuous raster data into discrete vector data such as points, lines or polygons. Be careful with the wording! In R vectorization refers to the possibility of replacing for-loops and alike by doing things like 1:10 / 2 (see also Wickham (2014)). The simplest form of vectorization is to convert a raster into points by keeping the cell values and replacing the grid cells by its centroids. The rasterToPoints() does exactly this for all non-NA raster grid cells (Figure 11.7). Setting the spatial parameter to TRUE makes sure that the output is a spatial object, otherwise a matrix is returned. elev_point = rasterToPoints(elev, spatial = TRUE) %&gt;% st_as_sf() Figure 11.7: Raster and point representation of elev. Another common application is the representation of a digital elevation model as contour lines, hence, converting raster data into spatial lines. Here, we will us a real-world DEM since our artificial raster elev produces parallel lines (give it a try yourself) because when creating it we made the upper left corner the lowest and the lower right corner the highest value while increasing cell values by one from left to right. rasterToContour() is a wrapper around contourLines(). # not shown data(dem, package = &quot;RQGIS&quot;) plot(dem, axes = FALSE) plot(rasterToContour(dem), add = TRUE) Use contour(), rasterVis::contourplot() or tmap::tm_iso() if you want to add contour lines to a plot with isoline labels (Fig. 11.8). Figure 11.8: DEM hillshade of the southern flank of Mt. Mongón overlaid with contour lines. Finally, rasterToPolygons() converts each raster cell into one polygon consisting of five coordinates all of which need to be explicitly stored. Be careful with this approach when using large raster datasets since you might run into memory problems. Here, we convert grain into polygons and subsequently dissolve the output in accordance with the grain size categories which rasterToPolygons() stored in an attribute named layer (see section 5.3.6 and Figure 11.9). A convenient alternative for converting rasters into polygons is spex::polygonize() which by default returns an sf object. grain_poly = rasterToPolygons(grain) %&gt;% st_as_sf() grain_poly2 = grain_poly %&gt;% group_by(layer) %&gt;% summarize() Figure 11.9: Illustration of vectorization of raster (left) into polygon (center) and polygon aggregation (right). 11.6 Exercises The next two exercises will use a vector (random_points) and raster dataset (ndvi) from RQGIS package. We will also create a convex hull of the vector dataset (ch), which will represent an area of interest: library(RQGIS) data(random_points) data(ndvi) ch = st_combine(random_points) %&gt;% st_convex_hull() Crop the ndvi raster using (1) the random_points dataset and (2) the ch dataset. Are there any difference in the output maps? Next, mask ndvi using these two datasets. Can you see any difference now? How can you explain that? Firstly, extract values from ndvi using random_points. Next, extract average values of ndvi using 90 meters buffers around each point from random_points. Compare these two sets of values. When extract by buffer could be more suitable than an extract by point? Subset points higher than 3100 meters in New Zealand (the nz_height object). Using the new object: Count numbers of the highest points in grid cells with a resolution of 3 km. Find maximum elevation value for grid cells with a resolution of 3 km. Polygonize the grain dataset and filter all squares representing clay. Name two advantages and disadvantages of vector data over raster data. At which points would it be useful to convert rasters to vectors in your work? References "],
["algorithms-and-functions-for-geocomputation.html", "12 Algorithms and functions for geocomputation Prerequisites 12.1 Geographic algorithms 12.2 Functions 12.3 Implementation 12.4 Case study 12.5 Exercises", " 12 Algorithms and functions for geocomputation Prerequisites 12.1 Geographic algorithms 12.2 Functions 12.3 Implementation 12.4 Case study 12.5 Exercises "],
["spatial-cv.html", "13 Statistical learning for geographic data Prerequisites 13.1 Introduction 13.2 Case study: Landslide susceptibility 13.3 Conventional modeling approach in R 13.4 Introduction to (spatial) cross-validation 13.5 Spatial CV with mlr 13.6 Conclusions 13.7 Exercises", " 13 Statistical learning for geographic data Prerequisites This chapter assumes proficiency with spatial data, for example gained by studying the contents and working-through the exercises in chapters 2 to 5. A familiarity with generalized linear regression and machine learning is highly recommended (for example from Zuur et al. 2009; James et al. 2013). The chapter uses the following packages:61 library(sf) library(mlr) library(raster) library(tidyverse) library(parallelMap) Required data will be attached in due course. 13.1 Introduction Statistical learning is concerned with the use of statistical and computational models for identifying patterns in data and predicting from these patterns. Due to its origins, statistical learning is one of R’s great strengths (see section 1.3).62 Statistical learning combines and blends methods from both statistics and machine learning that learn from data. Roughly, one can distinguish statistical learning into supervised and unsupervised techniques, both of which are used throughout a vast range of disciplines including economics, physics, medicine, biology, ecology and geography (James et al. 2013). This chapter focuses on supervised techniques, as opposed to unsupervised techniques such as clustering. Response variables can be binary (such as landslide occurrence), categorical (land use), integer (species richness count) or numeric (soil acidity measured in pH). Supervised techniques model the relationship between such responses — which are known for a sample of observations — and one or more predictors. The primary aim of machine learning is to make good predictions. It is increasingly appealing in the age of ‘big data’ because it makes few assumptions about input variables and can scale to handle problems that involve large datasets. Machine learning is conducive to tasks such as the prediction of future customer behavior, recommendation services (music, movies, what to buy next), face recognition, autonomous driving, text classification and predictive maintenance (infrastructure, industry). This chapter is based on a case study: the (spatial) prediction of landslides. This application links to the applied nature of geocomputation, defined in Chapter 1, and illustrates how machine learning borrows from the field of statistics when the sole aim is prediction. Therefore, this chapter first introduces modeling and cross-validation concepts with the help of a Generalized Linear Model (GLM; Zuur et al. 2009). Building on this the chapter implements a more typical machine learning algorithm, namely a Support Vector Machine (SVM). The models’ predictive performance will be assessed using spatial cross-validation (CV), which accounts for the fact that geographic data is special. CV determines a model’s ability to generalize to new data, by splitting a dataset (repeatedly) into training and test sets. It uses the training data to fit the model, and checks its performance when predicting to the test data. CV helps to detect overfitting since models that predict the training data too closely (noise) will tend to perform poorly on the test data. Randomly splitting spatial data can lead to training points that are neighbors in space with test points. Due to spatial autocorrelation, test and training datasets would not be independent in this scenario, with the consequence that CV fails to detect a possible overfitting. Spatial CV alleviates this problem and is the central theme in this chapter. 13.2 Case study: Landslide susceptibility The case study is based on a dataset of landslide locations Southern Ecuador, illustrated in Figure 13.1 and described in detail in Muenchow, Brenning, and Richter (2012). A subset of the dataset used in that paper is provided in the RSAGA package, which can be loaded as follows: data(&quot;landslides&quot;, package = &quot;RSAGA&quot;) This should load three objects: a data.frame named landslides, a list named dem, and an sf object named study_area. landslides contains a factor column lslpts where TRUE corresponds to an observed landslide ‘initiation point’, with the coordinates stored in columns x and y.63 The coordinates for the non-landslide points were sampled randomly from the study area, with the restriction that they must fall outside a small buffer around the landslide polygons. There are 175 landslide points and 1360 non-landslide, as shown by summary(landslides). To make number of landslide and non-landslide points balanced, let us sample 175 from the 1360 non-landslide points. # select non-landslide points non_pts = filter(landslides, lslpts == FALSE) # select landslide points lsl_pts = filter(landslides, lslpts == TRUE) # randomly select 175 non-landslide points set.seed(11042018) non_pts_sub = sample_n(non_pts, size = nrow(lsl_pts)) # create smaller landslide dataset (lsl) lsl = bind_rows(non_pts_sub, lsl_pts) dem is a digital elevation model consisting of two elements: dem$header, a list which represents a raster ‘header’ (see section 2.2), and dem$data, a matrix with the altitude of each pixel. dem can be converted into a raster object with: dem = raster( dem$data, crs = dem$header$proj4string, xmn = dem$header$xllcorner, xmx = dem$header$xllcorner + dem$header$ncols * dem$header$cellsize, ymn = dem$header$yllcorner, ymx = dem$header$yllcorner + dem$header$nrows * dem$header$cellsize ) To model landslide susceptibility, we need some predictors. Terrain attributes are frequently associated with landsliding (Muenchow, Brenning, and Richter 2012), and these can be computed from the digital elevation model (dem) using R-GIS bridges (see Chapter 10). We leave it as an exercise to the reader to compute the following terrain attribute rasters and extract the corresponding values to our landslide/non-landslide data frame (see exercises): slope: slope angle (°). cplan: plan curvature (rad m−1) expressing the convergence or divergence of a slope and thus water flow. cprof: profile curvature (rad m-1) as a measure of flow acceleration, also known as downslope change in slope angle. elev: elevation (m a.s.l.) as the representation of different altitudinal zones of vegetation and precipitation in the study area. log10_carea: the decadic logarithm of the catchment area (log10 m2) representing the amount of water flowing towards a location. The first three rows of the resulting data frame, still named lsl look like this (rounded to two significant digits): #&gt; x y lslpts slope cplan cprof elev log10_carea #&gt; 1 715078 9558647 FALSE 37 0.021 0.0087 2500 2.6 #&gt; 2 713748 9558047 FALSE 42 -0.024 0.0068 2500 3.1 #&gt; 3 712508 9558887 FALSE 20 0.039 0.0150 2100 2.3 As a convenience to the reader, lsl is also available in the spDataLarge package along with the corresponding terrain attributes stored in a raster stack (data(&quot;ta&quot;, package = &quot;spDataLarge&quot;)). Figure 13.1: Landslide initiation points (red) and points unaffected by landsliding (blue) in Southern Ecuador. 13.3 Conventional modeling approach in R Before introducing the mlr package, an umbrella-package providing a unified interface to dozens of learning algorithms (section 13.5), it is worth taking a look at the conventional modeling interface in R. This introduction to supervised statistical learning provides the basis for doing spatial CV, and contributes to a better grasp on the mlr approach presented subsequently. Supervised learning involves predicting a response variable as a function of predictors (section 13.1). In R, modeling functions are usually specified using formulas (see ?formula and the detailed Formulas in R Tutorial for details of R formulas). The following command specifies and runs a generalized linear model: fit = glm(lslpts ~ slope + cplan + cprof + elev + log10_carea, family = binomial(), data = lsl) It is worth understanding each of the three input arguments: A formula, which specifies landslide occurrence (lslpts) as a function of the predictors. A family, which specifies the type of model, in this case binomial because the response is binary (see ?family). The dataframe which contains the response and the predictors. The results of this model can be printed as follows (summary(fit) provides a more detailed account of the results): class(fit) #&gt; [1] &quot;glm&quot; &quot;lm&quot; fit #&gt; #&gt; Call: glm(formula = lslpts ~ slope + cplan + cprof + elev + log10_carea, #&gt; family = binomial(), data = lsl) #&gt; #&gt; Coefficients: #&gt; (Intercept) slope cplan cprof elev #&gt; 1.97e+00 9.30e-02 -2.57e+01 -1.43e+01 2.41e-05 #&gt; log10_carea #&gt; -2.12e+00 #&gt; #&gt; Degrees of Freedom: 349 Total (i.e. Null); 344 Residual #&gt; Null Deviance: 485 #&gt; Residual Deviance: 361 AIC: 373 The model object fit, of class glm, contains the coefficients defining the fitted relationship between response and predictors. It can also be used for prediction. This is done with the generic predict() method, which in this case calls the function predict.glm(). Setting type to response returns the predicted probabilities (of landslide occurrence) for each observation in lsl, as illustrated below (see ?predict.glm): pred_glm = predict(object = fit, type = &quot;response&quot;) head(pred_glm) #&gt; 1 2 3 4 5 6 #&gt; 0.3327 0.4755 0.0995 0.1480 0.3486 0.6766 Spatial predictions can be made by applying the coefficients to the predictor rasters. This can be done manually or with raster::predict(). In addition to a model object (fit), this function also expects a raster stack with the predictors named as in the model’s input dataframe (Figure 13.2). # attaching ta, a raster brick containing the predictors data(&quot;ta&quot;, package = &quot;spDataLarge&quot;) # making the prediction pred = raster::predict(object = ta, model = fit, type = &quot;response&quot;) Figure 13.2: Spatial prediction of landslide susceptibility using a GLM. Here, when making predictions we neglect spatial autocorrelation since we assume that on average the predictive accuracy remains the same with or without spatial autocorrelation structures. However, it is possible to include spatial autocorrelation structures into models (Zuur et al. 2009; Blangiardo and Cameletti 2015; Zuur et al. 2017) as well as into predictions (kriging approaches, see e.g., Goovaerts 1997; Hengl 2007; Bivand, Pebesma, and Gómez-Rubio 2013). This is, however, beyond the scope of this book. Spatial prediction maps are one very important outcome of a model. Even more important is how good the underlying model is at making them since a prediction map is useless if the model’s predictive performance is bad. The most popular measure to assess the predictive performance of a binomial model is the Area Under the Receiver Operator Characteristic Curve (AUROC). This is a value between 0.5 and 1.0 with 0.5 indicating no and 1.0 indicating a perfect discrimination of the two classes. Thus, the higher the AUROC the better is our model at making predictions. In the following we compute the receiver operator characteristic with the help of roc() by providing it with the response variable and the predicted values. auc() returns the area under the curve. pROC::auc(pROC::roc(lsl$lslpts, fitted(fit))) #&gt; Area under the curve: 0.826 An AUROC of 0.83 represents a good fit. However, this is an overoptimistic estimation since we have computed it on the complete dataset. To derive a biased-reduced assessment we have to use cross-validation and in the case of spatial data should make use of spatial CV. 13.4 Introduction to (spatial) cross-validation Cross-validation belongs to the family of resampling methods (James et al. 2013). The basic idea is to split (repeatedly) a dataset into training and test sets whereby the training data is used to fit a model which then is applied to the test set. Comparing the predicted values with the known response values from the test set (using a performance measure such as the AUROC in the binomial case) gives a bias-reduced assessment of the model’s capability to generalize the learned relationship to independent data. For example, a 100-repeated 5-fold cross-validation means to randomly split the data into five partitions (folds) with each fold being used once as a test set (see upper row of Figure 13.3). This guarantees that each observation is used once in one of the test sets, and requires the fitting of five models. Subsequently, this procedure is repeated 100 times. Of course, the data splitting will differ in each repetition. Overall, this sums up to 500 models whereas the mean performance measure (AUROC) of all models is the model’s overall predictive power. However, geographic data is special. As we saw in Chapter 7, the ‘first law’ of geography states that points close to each other are, generally, more similar than points further away (Miller 2004). This means these points are not statistically independent because training and test points in conventional CV are often too close to each other (see first row of 13.3). ‘Training’ observations near the ‘test’ observations can provide a kind of ‘sneak preview’: information that should be unavailable to the training dataset. To alleviate this problem ‘spatial partitioning’ is used to split the observations into spatially disjointed subsets (using the observations’ coordinates in a k-means clustering; A. Brenning (2012b); second row of Figure 13.3). This partitioning strategy is the only difference between spatial and conventional CV. As a result spatial CV leads to a bias-reduced assessment of a model’s predictive performance, and hence helps to avoid overfitting. Figure 13.3: Spatial visualization of selected test and training observations for cross-validation of one repetition. Random (upper row) and spatial partitioning (lower row). 13.5 Spatial CV with mlr There are dozens of packages for statistical learning, as described for example in the CRAN machine learning task view. Getting acquainted with each of these packages, including how to undertake cross-validation and hyperparameter tuning, can be a time-consuming process. Comparing model results from different packages can be even more laborious. The mlr package was developed to address these issues. It acts as a ‘meta-package’, providing a unified interface to the most popular statistical learning techniques including classification, regression, survival analysis and clustering (Bischl et al. 2016).64 The standardized mlr interface is based on eight ‘building blocks’. As illustrated in Figure 13.4, these have a clear order. Figure 13.4: Basic building blocks of the mlr package. Source: openml.github.io. Permission to reuse this figure was kindly granted. The mlr modelling process consists of three main stages. First, a task specifies the data (including response and predictor variables) and the model type (such as regression or classification). Second, a learner defines the specific learning algorithm that is applied to the created task. Third, the resampling approach assesses the predictive performance of the model, i.e. its ability to generalize to new data (see also section 13.1). 13.5.1 Generalized linear model To implement a GLM in mlr we must create a task containing the landslide data. Since the response is binary (two-category variable) we create a classification task with makeClassifTask() (for regression tasks use makeRegrTask(), see ?makeClassifTask for other task types). The first essential argument of these make*() functions is data. The target argument expects the name of a response variable and positive determines which of the two factor levels of the response variable indicate the landslide initiation point (in our case this is TRUE). All other variables of the lsl dataset will serve as predictors except for the coordinates (see the result of getTaskFormula(task) for the model formula). For spatial CV the coordinates parameter is used (see section 13.1 and Figure 13.3) which expects the coordinates as a xy-dataframe. library(mlr) # coordinates needed for the spatial partitioning coords = lsl[, c(&quot;x&quot;, &quot;y&quot;)] # select response and predictors to use in the modeling data = dplyr::select(lsl, -x, -y) coords = lsl[, c(&quot;x&quot;, &quot;y&quot;)] # create task task = makeClassifTask(data = data, target = &quot;lslpts&quot;, positive = &quot;TRUE&quot;, coordinates = coords) makeLearner() determines the statistical learning method to use. All classification learners start with classif. and all regression learners with regr. (see ?makeLearners for details). listLearners() helps to find out about all available learners and from which package mlr imports them (Table 13.1). For a specific task, we can run: listLearners(task, warn.missing.packages = FALSE) %&gt;% dplyr::select(class, name, short.name, package) %&gt;% head Table 13.1: Sample of available learners for binomial tasks in the mlr package. class name short.name package classif.binomial Binomial Regression binomial stats classif.featureless Featureless classifier featureless mlr classif.fnn Fast k-Nearest Neighbour fnn FNN classif.knn k-Nearest Neighbor knn class classif.lda Linear Discriminant Analysis lda MASS classif.logreg Logistic Regression logreg stats This yields all learners able to model two-class problems (landslide yes or no). We opt for the binomial classification method used in section 13.3 and implemented as classif.binomial in mlr. Additionally, we must specify the link-function, logit in this case, which is also the default of the binomial() function. predict.type determines the type of the prediction with prob resulting in the predicted probability for landslide occurrence between 0 and 1 (this corresponds to type = response in predict.glm). lrn = makeLearner(cl = &quot;classif.binomial&quot;, link = &quot;logit&quot;, predict.type = &quot;prob&quot;, fix.factors.prediction = TRUE) To find out from which package the specified learner is taken and how to access the corresponding help pages, we can run: getLearnerPackages(lrn) helpLearner(lrn) The set-up steps for modeling with mlr may seem tedious. But remember this single interface provides access to the 150+ learners shown by listLearners(); it would be far more tedious to learn the interface for each learner! Further advantages are simple parallelization of resampling techniques and the ability to tune machine learning hyperparameters (see section 13.5.2). Most importantly, (spatial) resampling in mlr is straightforward, requiring only two more steps: specifying a resampling method and running it. We will use a 100-repeated 5-fold spatial CV: five partitions will be chosen based on the provided coordinates in our task and the partitioning will be repeated 100 times:65 resampling = makeResampleDesc(method = &quot;SpRepCV&quot;, folds = 5, reps = 100) To execute the spatial resampling, we run resample() using the specified learner, task, resampling strategy and of course the performance measure, here the AUROC. This takes some time (around 10 seconds on a modern laptop) because it computes the AUROC for 500 models. Setting a seed ensures the reprocubility of the obtained result and will ensure the same spatial partitioning when re-running the code. set.seed(012348) sp_cv = mlr::resample(learner = lrn, task = task, resampling = resampling, measures = mlr::auc) The output of the preceding code chunk is a bias-reduced assessment of the model’s predictive performance, as illustrated in the following code chunk (required input data is saved in the file spatialcv.Rdata in the book’s GitHub repo): # summary statistics of the 500 models summary(sp_cv$measures.test$auc) #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; 0.686 0.757 0.789 0.780 0.795 0.861 # mean AUROC of the 500 models mean(sp_cv$measures.test$auc) #&gt; [1] 0.78 To put these results in perspective let us compare them with AUROC values from a 100-repeated 5-fold non-spatial cross-validation (Figure 13.5; the code for the non-spatial cross-validation is not shown here but will be explored in the exercise section). As expected, the spatially cross-validated result yields lower AUROC values on average than the conventional cross-validation approach, underlining the over-optimistic predictive performance due to spatial autocorrelation of the latter. Figure 13.5: Boxplot showing the difference in AUROC values between spatial and conventional 100-repeated 5-fold cross-validation. 13.5.2 Spatial tuning of machine-learning hyperparameters Section 13.1 introduced machine learning as part of statistical learning. To recap, we adhere to the following definition of machine learning by Jason Brownlee: Machine learning, more specifically the field of predictive modeling is primarily concerned with minimizing the error of a model or making the most accurate predictions possible, at the expense of explainability. In applied machine learning we will borrow, reuse and steal algorithms from many different fields, including statistics and use them towards these ends. In section 13.5.1 a GLM was used to predict landslide susceptibility. This section introduces support vector machines (SVM) for the same purpose. In short, SVMs search for the best possible ‘hyperplanes’ to separate classes (in a classification case) and estimate ‘kernels’ with specific hyperparameters to allow for non-linear boundaries between classes (James et al. 2013). Hyperparameters should not be confused with coefficients of parametric models, which are sometimes also referred to as parameters.66 Coefficients can be estimated from the data while hyperparameters are set before the learning begins. Optimal hyperparameters are usually determined within a defined range with the help of cross-validation methods. This is called hyperparameter tuning. Some SVM implementations such as that provided by kernlab allow hyperparameters to be tuned automatically, usually based on random sampling (see upper row of Figure 13.3). This works for non-spatial data but is of less use for spatial data where ‘spatial tuning’ should be undertaken. Before defining spatial tuning we will set-up the mlr building blocks, introduced in section 13.5.1, for the SVM. The task remains the same as the task object created in section 13.5.1. Learners implementing SVM can be found using listLearners() as follows: lrns = listLearners(task, warn.missing.packages = FALSE) filter(lrns, grepl(&quot;svm&quot;, class)) %&gt;% dplyr::select(class, name, short.name, package) #&gt; class name short.name package #&gt; 6 classif.ksvm Support Vector Machines ksvm kernlab #&gt; 9 classif.lssvm Least Squares Support Vector Machine lssvm kernlab #&gt; 17 classif.svm Support Vector Machines (libsvm) svm e1071 Of the options illustrated above, we will use ksvm() from the kernlab package (Karatzoglou et al. 2004). To allow for non-linear relationships we use the popular radial basis function (or Gaussian) kernel which is also the default of ksvm(). lrn_ksvm = makeLearner(&quot;classif.ksvm&quot;, predict.type = &quot;prob&quot;, kernel = &quot;rbfdot&quot;) The next stage is to specify a resampling strategy. Again we will use a 100-repeated 5-fold spatial CV: # performance estimation level perf_level = makeResampleDesc(&quot;SpRepCV&quot;, folds = 5, reps = 100) So far, the process is identical to that described in section 13.5.1. The next step is new, however: to tune the hyperparameters. Using the same data for the performance assessment and the tuning would potentially lead to overoptimistic results (Cawley and Talbot 2010). This can be avoided using nested spatial CV. Figure 13.6: Visual representation of the hyperparameter tuning and performance estimation levels in spatial and non-spatial cross-validation. Permission for reusing the figure was kindly granted by Patrick Schratz (Schratz et al. 2018). This means that we split each fold again into five spatially disjoint subfolds which are used to determine the optimal hyperparameters (tune_level object in the code chunk below; see Figure 13.6 for a visual representation). To find the optimal hyperparameter combination we here fit 50 models in each of these subfolds with randomly selected hyperparameter values (ctrl object in the code chunk below). Additionally, we restrict the randomly chosen values to a predefined tuning space (ps object). The latter was chosen with values recommended in the literature (Schratz et al. 2018). # five spatially disjoint partitions tune_level = makeResampleDesc(&quot;SpCV&quot;, iters = 5) # use 50 randomly selected hyperparameters ctrl = makeTuneControlRandom(maxit = 50) # define the outer limits of the randomly selected hyperparameters ps = makeParamSet( makeNumericParam(&quot;C&quot;, lower = -12, upper = 15, trafo = function(x) 2^x), makeNumericParam(&quot;sigma&quot;, lower = -15, upper = 6, trafo = function(x) 2^x) ) The next stage is to modify the learner lrn_ksvm in accordance with all the characteristics defining the hyperparameter tuning with makeTuneWrapper(). wrapped_lrn_ksvm = makeTuneWrapper(learner = lrn_ksvm, resampling = tune_level, par.set = ps, control = ctrl, show.info = TRUE, measures = mlr::auc) The mlr is now set-up to fit 250 models to determine optimal hyperparameters for one fold. Repeating this for each fold, we end up with 1250 (250 * 5) models for each repetition. Repeated 100 times means fitting a total of 125,000 models to identify optimal hyperparameters (Figure 13.3). These are used in the performance estimation, which requires the fitting of another 500 models (5 folds * 100 repetitions; see Figure 13.3). The process of hyperparameter tuning and performance estimation is computationally intensive. Model runtime can be reduced with parallelization, which can be done in a number of ways, depending on the operating system. Before starting the parallelization, we ensure that the processing continues even if one of the models throws an error by setting on.learner.error to warn. This avoids the process stopping just because of one failed model, which is desirable on large model runs. To inspect the failed models once the processing is completed, we dump them: configureMlr(on.learner.error = &quot;warn&quot;, on.error.dump = TRUE) To start the parallelization, we set the mode to multicore which will use mclapply() in the background on a single machine in the case of a Unix-based operating system67 Equivalenty, parallelStartSocket() enables parallelization under Windows. level defines the level at which to enable parallelization, with mlr.tuneParams determining that the hyperparameter tuning level should be parallelized (see lower left part of Figure 13.6, ?parallelGetRegisteredLevels, and the mlr parallelization tutorial for details). We will use half of the available cores (set with the cpus parameter), a setting that allows possible other users to work on the same high performance computing cluster in case one is used (which was the case when we ran the code). Setting mc.set.seed to TRUE ensures that the randomly chosen hyperparameters during the tuning can be reproduced when running the code again. Unfortunately, mc.set.seed is only available under Unix-based systems. library(parallelMap) if (Sys.info()[&quot;sysname&quot;] %in% c(&quot;Linux, Darwin&quot;)) { parallelStart(mode = &quot;multicore&quot;, # parallelize the hyperparameter tuning level level = &quot;mlr.tuneParams&quot;, # just use half of the available cores cpus = round(parallel::detectCores() / 2), mc.set.seed = TRUE) } if (Sys.info()[&quot;sysname&quot;] == &quot;Windows&quot;) { parallelStartSocket(level = &quot;mlr.tuneParams&quot;, cpus = round(parallel::detectCores() / 2)) } Now we are set-up for computing the nested spatial CV. Using a seed allows to recreate the exact same spatial partitions when re-running the code. Specifying the resample() parameters follows the exact same procedure as presented when using a GLM, the only difference being the extract argument. This allows the extraction of the hyperparameter tuning results which is important if we plan follow-up analyses on the tuning. After the processing, it is good practice to explicitly stop the parallelization with parallelStop(). Finally, we save the output object (result) to disk in case we would like to use it another R session. Before running the subsequent code, be aware that it is time-consuming: the 125,500 models took ~1/2hr on a server using 24 cores (see below). set.seed(12345) result = mlr::resample(learner = wrapped_lrn_ksvm, task = task, resampling = perf_level, extract = getTuneResult, measures = mlr::auc) # stop parallelization parallelStop() # save your result, e.g.: # saveRDS(result, &quot;svm_sp_sp_rbf_50it.rds&quot;) In case you do not want to run the entire model locally, we have saved a subset of the results in the book’s GitHub repo. They can be loaded as follows: result = readRDS(&quot;extdata/spatial_cv_result.rds&quot;) Note that runtime depends on many aspects: CPU speed, the selected algorithm, the selected number of cores and the dataset. # Exploring the results # runtime in minutes round(result$runtime / 60, 2) #&gt; [1] 37.4 Even more important than the runtime is the final aggregated AUROC: the model’s ability to discriminate the two classes. # final aggregated AUROC result$aggr #&gt; auc.test.mean #&gt; 0.758 # same as mean(result$measures.test$auc) #&gt; [1] 0.758 It appears that the GLM (aggregated AUROC was 0.78) is slightly better than the SVM in this specific case. However, using more than 50 iterations in the random search would probably yield hyperparameters that result in models with a better AUROC (Schratz et al. 2018). On the other hand, increasing the number of random search iterations would also increase the total number of models and thus runtime The estimated optimal hyperparameters for each fold at the performance estimation level can also be viewed. The following command shows the best hyperparameter combination of the first fold of the first iteration (recall this results from the first 5 * 50 model runs): # winning hyperparameters of tuning step, i.e. the best combination out of 50 * # 5 models result$extract[[1]]$x #&gt; $C #&gt; [1] 0.458 #&gt; #&gt; $sigma #&gt; [1] 0.023 The estimated hyperparameters have been used for the first fold in the first iteration of the performance estimation level which resulted in the following AUROC value: result$measures.test[1, ] #&gt; iter auc #&gt; 1 1 0.799 So far spatial CV has been used to assess the ability of learning algorithms to generalize to unseen data. For spatial prediction, one would tune the hyperparameters on the complete dataset (see Chapter ??). 13.6 Conclusions Resampling methods are an important part of a data scientist’s toolbox (James et al. 2013). This chapter used cross-validation to assess predictive performance of various models. As described in section 13.1, observations with spatial coordinates may not be statistically independent due to spatial autocorrelation, violating a fundamental assumption of cross-validation. Spatial CV addresses this issue by reducing bias introduced by spatial autocorrelation. The mlr package facilitates (spatial) resampling techniques in combination with the most popular statistical learning techniques including linear regression, semi-parametric models such as generalized additive models and machine learning techniques such as random forests, SVMs, and boosted regression trees (Bischl et al. 2016; Schratz et al. 2018). Machine learning algorithms often require hyperparameter inputs, the optimal ‘tuning’ of which can require thousands of model runs which require large computational resources, consuming much time, RAM and/or cores. mlr tackles this issue by enabling parallelization. Machine learning overall, and its use to understand spatial data, is a large field and this chapter has provided the basics, but there is more to learn. We recommend the following resources in this direction: The mlr tutorials on Machine Learning in R and Handling of spatial Data. An academic paper on hyperparameter tuning (Schratz et al. 2018). In case of spatio-temporal data, one should account for spatial and temporal autocorrelation when doing CV (Meyer et al. 2018). 13.7 Exercises Compute the following terrain attributes from the dem datasets loaded with data(&quot;landslides&quot;, package = &quot;RSAGA&quot;) with the help of R-GIS bridges (see Chapter 10): slope plan curvature profile curvature catchment area Extract the values from the corresponding output rasters to the landslides data frame (data(landslides, package = &quot;RSAGA&quot;) by adding new variables called slope, cplan, cprof, elev and log_carea. Keep all landslide initiation points and 175 randomly selected non-landslide points (see section 13.2 for details). Use the derived terrain attribute rasters in combination with a GLM to make a spatial prediction map similar to that shown in Figure 13.2. Running data(&quot;study_mask&quot;, package = &quot;spDataLarge&quot;) attaches a mask of the study area. Compute a 100-repeated 5-fold non-spatial cross-validation and spatial CV based on the GLM learner and compare the AUROC values from both resampling strategies with the help of boxplots (see Figure 13.5). Hint: You need to specify a non-spatial task and a non-spatial resampling strategy. Model landslide susceptibility using a quadratic discriminant analysis (QDA, James et al. 2013). Assess the predictive performance (AUROC) of the QDA. What is the difference between the spatially cross-validated mean AUROC value of the QDA and the GLM? Hint: Before running the spatial cross-validation for both learners set a seed to make sure that both use the same spatial partitions which in turn guarantees comparability. Run the SVM without tuning the hyperparameters. Use the rbfdot kernel with \\(\\sigma\\) = 1 and C = 1. Leaving the hyperparameters unspecified in kernlab’s ksvm() would otherwise initialize an automatic non-spatial hyperparameter tuning. For a discussion on the need for (spatial) tuning of hyperparameters please refer to Schratz et al. (2018). Model landslide susceptibility with the help of mlr using a random forest model as implemented by the ranger package. Apply a nested spatial CV. Parallelize the tuning level. Use a random search with 50 iterations to find the optimal hyperparameter combination (here: mtry and num.trees). The tuning space limits are 1 and 4 for mtry, and 1 and 10,000 for num.trees. (warning: this might take a long time). References "],
["references.html", "References", " References "]
]
